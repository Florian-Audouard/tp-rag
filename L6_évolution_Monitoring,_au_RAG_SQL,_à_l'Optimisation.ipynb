{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydlAa2C-rSeh"
      },
      "source": [
        "# Lesson 6: Improve Agent's GPA\n",
        "\n",
        "In this lesson, you'll make two targeted changes to the agent:\n",
        "\n",
        "1. Adjust the planning prompt to include explicit goals, pre-conditions, and post-conditions for each step. This helps the executor understand the sub-goals it needs to reach.\n",
        "\n",
        "2. You will add inline evals so the agent receives feedback on when to do additional research. This provides the executor feedback on whether it's reaching its sub-goals.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtqgaXFU4_iv"
      },
      "source": [
        "```+---------------------------------------------------------------------------------------------------------+\n",
        "|                                    ARCHITECTURE DU SYSTEME MULTI-AGENTS                                 |\n",
        "+---------------------------------------------------------------------------------------------------------+\n",
        "                                                |\n",
        "                                          [User Query]\n",
        "                                                |\n",
        "                                                v\n",
        "+---------------------------------------------------------------------------------------------------------+\n",
        "|  1. PLANNER NODE (Le Cerveau)                                                                           |\n",
        "|  -----------------------------------------------------------------------------------------------------  |\n",
        "|  * Fonction : `planner_node(state)`                                                                     |\n",
        "|  * Prompt   : `patched_plan_prompt` (qui utilise RECURSION_LIMIT pour gÃ©rer le budget)                  |\n",
        "|  * Sortie   : GÃ©nÃ¨re un plan JSON (Step 1, Step 2, ...) ajoutÃ© au `state['plan']`                       |\n",
        "+---------------------------------------------------------------------------------------------------------+\n",
        "                                                |\n",
        "                                                v\n",
        "          +---------------------------------------------------------------------------+\n",
        "          |                                                                           |\n",
        "          |   2. EXECUTOR NODE (Le Chef d'Orchestre)    <-------------------------+   |\n",
        "          |   -----------------------------------------------------------------   |   |\n",
        "          |   * Fonction : `executor_node(state)`                                 |   |\n",
        "          |   * Logique  : Lit le `state['plan']`, vÃ©rifie `current_step`.        |   |\n",
        "          |                DÃ©cide quel agent appeler (Routing).                   |   |\n",
        "          |                GÃ¨re les drapeaux `replan` et `previous_step_failed`.  |   |\n",
        "          |                                                                           |\n",
        "          +----------------------+----------------------+-------------------------+---+\n",
        "                                 |                      |                         |\n",
        "        +------------------------+                      |                         |\n",
        "        | (Route: \"cortex_researcher\")                  | (Route: \"web_...\")      | (Route: \"chart_...\")\n",
        "        v                                               v                         v\n",
        "+------------------------------------+    +-----------------------------+    +-----------------------------+\n",
        "| 3a. CORTEX RESEARCH NODE           |    | 3b. WEB RESEARCH NODE       |    | 3c. CHART GENERATOR NODE    |\n",
        "| ---------------------------------- |    | --------------------------- |    | --------------------------- |\n",
        "| * Func: `cortex_agents_research_...|    | * Func: `web_research_node` |    | * Func: `chart_node`        |\n",
        "| * Outils :                         |    | * Agent : ReAct Agent       |    | * Agent : ReAct Agent       |\n",
        "|   - `wikipedia_rag_tool`           |    | * Outil : `tavily_tool`     |    | * Outil : `PythonREPL`      |\n",
        "|   - `wikidata_sparql_tool`         |    |                             |    |                             |\n",
        "| * Note: Utilise `_cortex_llm_...`  |    |                             |    |                             |\n",
        "+-----------------+------------------+    +--------------+--------------+    +--------------+--------------+\n",
        "                  |                                      |                                  |\n",
        "                  |                                      |                                  |\n",
        "                  +-------------------+------------------+----------------------------------+\n",
        "                                      |\n",
        "                                      | (RÃ©sultat de l'Ã©tape / Demande de Replanification)\n",
        "                                      | Retour vers Executor\n",
        "                                      |\n",
        "+---------------------------------------------------------------------------------------------------------+\n",
        "|                                     CONDITION DE FIN                                                    |\n",
        "| Si (toutes les Ã©tapes finies) OU (replanification impossible) OU (remaining_steps <= 0)                 |\n",
        "+---------------------------------------------------------------------------------------------------------+\n",
        "                                                |\n",
        "                                                v\n",
        "+---------------------------------------------------------------------------------------------------------+\n",
        "|  4. SYNTHESIZER NODE (Le RÃ©dacteur)                                                                     |\n",
        "|  -----------------------------------------------------------------------------------------------------  |\n",
        "|  * Fonction : `synthesizer_node(state)`                                                                 |\n",
        "|  * Prompt   : `final_answer_prompt`                                                                     |\n",
        "|  * EntrÃ©e   : Prend tout l'historique des messages et des rÃ©sultats d'outils.                           |\n",
        "|  * Sortie   : RÃ©ponse finale structurÃ©e pour l'utilisateur.                                             |\n",
        "+---------------------------------------------------------------------------------------------------------+\n",
        "                                                |\n",
        "                                                v\n",
        "                                          [Final Answer]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unstructured[pdf] in /usr/local/lib/python3.12/dist-packages (0.18.27)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (3.4.4)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (6.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (2.32.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (4.13.5)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (2.15.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (2025.11.16)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (2.0.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (3.14.3)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (4.15.0)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (0.42.8)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (1.17.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (5.9.5)\n",
            "Requirement already satisfied: python-oxmsg in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (0.0.2)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (1.1)\n",
            "Requirement already satisfied: onnx>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (1.20.1)\n",
            "Requirement already satisfied: onnxruntime>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (1.23.2)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (1.17.0)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (20260107)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (10.2.0)\n",
            "Requirement already satisfied: pi_heif in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (1.1.1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (6.6.0)\n",
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (3.12.0)\n",
            "Requirement already satisfied: effdet in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (0.4.1)\n",
            "Requirement already satisfied: unstructured-inference>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (1.1.4)\n",
            "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in /usr/local/lib/python3.12/dist-packages (from unstructured[pdf]) (0.3.15)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.17.0->unstructured[pdf]) (5.29.5)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.17.0->unstructured[pdf]) (0.5.4)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.19.0->unstructured[pdf]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.19.0->unstructured[pdf]) (25.12.19)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.19.0->unstructured[pdf]) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.19.0->unstructured[pdf]) (1.14.0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[pdf]) (0.0.21)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[pdf]) (0.36.0)\n",
            "Requirement already satisfied: opencv-python>=4.12 in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[pdf]) (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[pdf]) (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[pdf]) (2.9.0+cu126)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[pdf]) (1.0.24)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[pdf]) (4.57.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[pdf]) (1.12.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[pdf]) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[pdf]) (1.16.3)\n",
            "Requirement already satisfied: pypdfium2 in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[pdf]) (5.3.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[pdf]) (11.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->unstructured[pdf]) (2.8.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->unstructured[pdf]) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->unstructured[pdf]) (0.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from effdet->unstructured[pdf]) (0.24.0+cu126)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from effdet->unstructured[pdf]) (2.0.11)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.12/dist-packages (from effdet->unstructured[pdf]) (2.3.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf]) (2.29.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-vision->unstructured[pdf]) (2.43.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-vision->unstructured[pdf]) (1.76.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-vision->unstructured[pdf]) (1.27.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.12/dist-packages (from html5lib->unstructured[pdf]) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from html5lib->unstructured[pdf]) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured[pdf]) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured[pdf]) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured[pdf]) (2025.11.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six->unstructured[pdf]) (43.0.3)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.12/dist-packages (from pikepdf->unstructured[pdf]) (1.3.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.12/dist-packages (from python-oxmsg->unstructured[pdf]) (0.47)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->unstructured[pdf]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->unstructured[pdf]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->unstructured[pdf]) (2026.1.4)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[pdf]) (24.1.0)\n",
            "Requirement already satisfied: httpcore>=1.0.9 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[pdf]) (1.0.9)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[pdf]) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.11.2 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[pdf]) (2.12.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[pdf]) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (2.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.72.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.71.2)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf]) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf]) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf]) (4.9.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore>=1.0.9->unstructured-client->unstructured[pdf]) (0.16.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pdf]) (4.12.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf>=2.0->effdet->unstructured[pdf]) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf>=2.0->effdet->unstructured[pdf]) (6.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured[pdf]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured[pdf]) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured[pdf]) (0.4.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm->unstructured-inference>=1.1.1->unstructured[pdf]) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (3.20.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (75.2.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[pdf]) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.19.0->unstructured[pdf]) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.25.1->unstructured-inference>=1.1.1->unstructured[pdf]) (0.22.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->unstructured-inference>=1.1.1->unstructured[pdf]) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[pdf]) (1.1.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.19.0->unstructured[pdf]) (10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.1.1->unstructured[pdf]) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.1.1->unstructured[pdf]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.1.1->unstructured[pdf]) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.1.1->unstructured[pdf]) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.1.1->unstructured[pdf]) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.1.1->unstructured[pdf]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->unstructured-inference>=1.1.1->unstructured[pdf]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->unstructured-inference>=1.1.1->unstructured[pdf]) (2025.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (2.23)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf]) (0.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->unstructured-inference>=1.1.1->unstructured[pdf]) (3.0.3)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZU50-durbyM",
        "outputId": "70ccf6ce-d226-4d05-e419-ce8e3cde8ea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m210.1/210.1 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m309.2/309.2 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.8/161.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m130.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m137.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m139.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m587.2/587.2 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m125.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m160.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m538.2/538.2 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m432.2/432.2 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m154.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m124.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.8/167.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m126.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m152.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m129.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCloning into 'tp-rag'...\n",
            "remote: Enumerating objects: 185, done.\u001b[K\n",
            "remote: Counting objects: 100% (96/96), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 185 (delta 57), reused 59 (delta 23), pack-reused 89 (from 1)\u001b[K\n",
            "Receiving objects: 100% (185/185), 94.78 MiB | 41.14 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n",
            "/content/tp-rag\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Install Dependencies & Setup (will kill 1st time then, re-launch)\n",
        "import os, sys, time\n",
        "if os.path.exists(\".lib_installed\"):\n",
        "    print(\"Dependencies installed.\")\n",
        "else:\n",
        "  !pip install -q \\\n",
        "    langchain \\\n",
        "    langchain-core \\\n",
        "    langchain-community \\\n",
        "    langchain-openai \\\n",
        "    langchain-experimental \\\n",
        "    langchain-tavily \\\n",
        "    langgraph \\\n",
        "    trulens-core trulens-providers-openai trulens-apps-langgraph trulens-dashboard \\\n",
        "    opentelemetry-sdk nest-asyncio2 openinference-instrumentation-langchain arize-phoenix uvicorn \\\n",
        "    python-dotenv \\\n",
        "    wikipedia \\\n",
        "    SPARQLWrapper \\\n",
        "    nest_asyncio \\\n",
        "    langchain-chroma \\\n",
        "    langchain-huggingface \\\n",
        "    unstructured \\\n",
        "    unstructured[pdf]\n",
        "\n",
        "  with open(\".lib_installed\", \"w\") as f: f.write(\"Installation OK\")\n",
        "\n",
        "  # # Si on est dans Colab, on tue le processus pour forcer le rechargement des nouvelles librairies\n",
        "  # if \"google.colab\" in sys.modules:\n",
        "  #     print(\"ğŸ”„ RedÃ©marrage automatique de la session pour appliquer les mises Ã  jour... âš ï¸ (Vous verrez peut-Ãªtre une notification 'Session Ã©crasÃ©e', c'est normal !)\")\n",
        "  #     time.sleep(1)\n",
        "  #     os.kill(os.getpid(), 9)\n",
        "\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    # Commandes Ã  exÃ©cuter uniquement sur Google Colab\n",
        "    if os.path.isdir('tp-rag'):\n",
        "        %cd tp-rag\n",
        "        print(\"ChangÃ© de rÃ©pertoire vers tp-rag\")\n",
        "    if not os.path.isdir('.git'):\n",
        "        !git clone https://github.com/Florian-Audouard/tp-rag\n",
        "        %cd tp-rag\n",
        "\n",
        "import nest_asyncio2 as nest_asyncio\n",
        "nest_asyncio.apply()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33440ab4a3d84bccaea827c04f3e8230",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0268680571f74cec88e2dadf7a8caea9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e07fd8d6b2fd4e49b51d00a318c12879",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef5318fad9554eedb36dc21bfea5a071",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac0d7048146b456686e112fe41872ecb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aeee5412180540458e75d8b08f7f72c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a059a97b1fa34deabae4a78e4ce1bfcd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aba2b8025dbc499ea313f83463068735",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2b475537-418d-47cd-8d44-245030682797)')' thrown while requesting HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/special_tokens_map.json\n",
            "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2b475537-418d-47cd-8d44-245030682797)')' thrown while requesting HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/special_tokens_map.json\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d60ac0a7f67a4f4c9e916e6838efb9f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef9b0ae2325c48cba4e1b73db9912364",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Warning: No languages specified, defaulting to English.\n",
            "Number of documents loaded: 63\n",
            "Number of paragraphs created: 8847\n",
            "Added batch 1: 5000 documents\n",
            "Added batch 2: 3847 documents\n",
            "All 8847 documents added to the vector store.\n"
          ]
        }
      ],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# LOCAL RAG TOOL\n",
        "EMBESSINGS_MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
        "DATA_FOLDER = \"data\"\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = CHUNK_SIZE // 5\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=EMBESSINGS_MODEL_NAME)\n",
        "vector_store_splits = Chroma(\n",
        "    collection_name=\"split_data_collection\",\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=\"./chroma_langchain_split_db\",  # Where to save data locally, remove if not necessary\n",
        ")\n",
        "loader = DirectoryLoader(DATA_FOLDER)\n",
        "documents = loader.load()\n",
        "print(f\"Number of documents loaded: {len(documents)}\")\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, add_start_index=True\n",
        ")\n",
        "all_splits = text_splitter.split_documents(documents)\n",
        "print(f\"Number of paragraphs created: {len(all_splits)}\")\n",
        "\n",
        "# Add documents in batches to avoid exceeding max batch size\n",
        "BATCH_SIZE = 5000\n",
        "for i in range(0, len(all_splits), BATCH_SIZE):\n",
        "    batch = all_splits[i : i + BATCH_SIZE]\n",
        "    vector_store_splits.add_documents(documents=batch)\n",
        "    print(f\"Added batch {i//BATCH_SIZE + 1}: {len(batch)} documents\")\n",
        "print(f\"All {len(all_splits)} documents added to the vector store.\")\n",
        "\n",
        "\n",
        "def search_rag(vector_store, query, k=3, score=False):\n",
        "    if score:\n",
        "        return vector_store.similarity_search_with_score(query, k=k)\n",
        "    return vector_store.similarity_search(query, k=k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qenKYEtHrq0V"
      },
      "outputs": [],
      "source": [
        "# @title 2. Central Configuration & Secrets\n",
        "import os\n",
        "\n",
        "groq = True\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"XXX\"  # PASTE TAVILY KEY\n",
        "\n",
        "os.environ[\"OPENAI_BASE_URL\"] = (\n",
        "    \"https://api.groq.com/openai/v1\"  # OPENROUTER: \"https://openrouter.ai/api/v1\", GROQ: \"https://api.groq.com/openai/v1\"\n",
        ")\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"XXX\"  # OPENROUTER: \"sk-XXX\", GROQ:\n",
        "# --- 2. MODELS DEFINITION\n",
        "if \"groq\" in os.environ[\"OPENAI_BASE_URL\"]:\n",
        "    os.environ[\"MODEL_EXECUTOR\"] = (\n",
        "        \"llama-3.1-8b-instant\"  # \"llama-3.3-70b-versatile\" # Llama 3.3 70B est le plus polyvalent (\"Versatile\") pour la rÃ©daction et la synthÃ¨se.\n",
        "    )\n",
        "    os.environ[\"MODEL_REASONING\"] = (\n",
        "        \"llama-3.1-8b-instant\"  # \"llama-3.3-70b-versatile\" # On utilise DeepSeek R1 (version distillÃ©e sur Llama 70B) - C'est ACTUELLEMENT le meilleur modÃ¨le de raisonnement gratuit sur Groq (\"Thinking Model\").\n",
        "    )\n",
        "    os.environ[\"MODEL_EVAL\"] = (\n",
        "        \"llama-3.1-8b-instant\"  # \"llama-3.3-70b-versatile\" # On rÃ©utilise Llama 3.3 pour avoir une critique de qualitÃ©. - Si vous avez trop d'erreurs 429 (quota), remplacez celui-ci par \"llama-3.1-8b-instant\"\n",
        "    )\n",
        "else:\n",
        "    os.environ[\"MODEL_EXECUTOR\"] = (\n",
        "        \"openai/gpt-5-nano\"  # \"google/gemini-2.0-flash-lite-001\" # \"openai/gpt-5-nano\"\n",
        "    )\n",
        "    os.environ[\"MODEL_REASONING\"] = (\n",
        "        \"openai/gpt-5-nano\"  # \"openai/gpt-oss-120b\" # or \"openai/o3-mini\"\n",
        "    )\n",
        "    os.environ[\"MODEL_EVAL\"] = (\n",
        "        \"openai/gpt-5-nano\"  # \"google/gemini-2.0-flash-lite-001\" # \"deepseek/deepseek-r1-distill-qwen-14b\" # \"openai/gpt-5-nano\"\n",
        "    )\n",
        "\n",
        "# --- 3. TRULENS SETUP ---\n",
        "os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ttQS-dwMOcj"
      },
      "source": [
        "# Setup a rate limiter to ensure to enjoy free Groq API :-)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1blwT4ZL-mq",
        "outputId": "edcb8739-93a5-4ee4-9f13-d5f72d39ce12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ›¡ï¸ Groq Armor activÃ© : 5 RPM + Auto-Retry sur 429 (LangChain & TruLens protÃ©gÃ©s)\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import openai\n",
        "from collections import deque\n",
        "\n",
        "# --- CONFIGURATION (Safe Mode) ---\n",
        "RPM_LIMIT = 20  # On vise 20 pour rester sous les 30 (marge de sÃ©cu)\n",
        "MAX_RETRIES = 5  # Nombre d'essais en cas d'erreur 429\n",
        "BASE_SLEEP = 2  # Temps d'attente initial (backoff exponentiel)\n",
        "\n",
        "# Stockage des timestamps pour le Rate Limiter\n",
        "_timestamps = deque(maxlen=RPM_LIMIT)\n",
        "\n",
        "# On patch le niveau le plus bas : la mÃ©thode `request` du client HTTP interne\n",
        "# Cela couvre TOUT : LangChain, TruLens, appels directs, nouveaux imports.\n",
        "if not hasattr(openai._base_client.SyncHttpxClientWrapper, \"_original_request\"):\n",
        "    openai._base_client.SyncHttpxClientWrapper._original_request = (\n",
        "        openai._base_client.SyncHttpxClientWrapper.request\n",
        "    )\n",
        "\n",
        "\n",
        "def protected_request(self, *args, **kwargs):\n",
        "    # 1. Rate Limiter PrÃ©ventif (Sliding Window)\n",
        "    if len(_timestamps) == RPM_LIMIT:\n",
        "        elapsed = time.time() - _timestamps[0]\n",
        "        if elapsed < 60:\n",
        "            time.sleep(60 - elapsed + 0.5)\n",
        "\n",
        "    # 2. Retry Logic pour erreur 429 (Tokens/TPM)\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            # Appel rÃ©el\n",
        "            response = self._original_request(*args, **kwargs)\n",
        "            _timestamps.append(time.time())  # SuccÃ¨s -> on note l'heure\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            # On dÃ©tecte l'erreur 429 (Too Many Requests)\n",
        "            if \"429\" in str(e) and attempt < MAX_RETRIES - 1:\n",
        "                wait = BASE_SLEEP * (2**attempt)  # 2s, 4s, 8s, 16s...\n",
        "                print(f\"âš ï¸ Quota Groq atteint (429). Pause de {wait}s...\")\n",
        "                time.sleep(wait)\n",
        "            else:\n",
        "                raise e  # Autre erreur ou max retries -> on plante\n",
        "\n",
        "\n",
        "# Application du patch\n",
        "openai._base_client.SyncHttpxClientWrapper.request = protected_request\n",
        "print(\n",
        "    f\"ğŸ›¡ï¸ Groq Armor activÃ© : {RPM_LIMIT} RPM + Auto-Retry sur 429 (LangChain & TruLens protÃ©gÃ©s)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UGjO3Nh3OTWH",
        "outputId": "e1835730-44e8-4f22-ab1b-b1a28e6db553"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:phoenix.session.session:âš ï¸ PHOENIX_COLLECTOR_ENDPOINT is set to http://localhost:6002/v1/traces.\n",
            "âš ï¸ This means that traces will be sent to the collector endpoint and not this app.\n",
            "âš ï¸ If you would like to use this app to view traces, please unset this environmentvariable via e.g. `del os.environ['PHOENIX_COLLECTOR_ENDPOINT']` \n",
            "âš ï¸ You will need to restart your notebook to apply this change.\n",
            "/usr/lib/python3.12/contextlib.py:144: SAWarning: Skipped unsupported reflection of expression-based index ix_cumulative_llm_token_count_total\n",
            "  next(self.gen)\n",
            "/usr/lib/python3.12/contextlib.py:144: SAWarning: Skipped unsupported reflection of expression-based index ix_latency\n",
            "  next(self.gen)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4077952863.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mphoenix_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch_app\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 1. Lancer l'UI locale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Give it time to spin up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ğŸš€ Phoenix UI is ready at: {phoenix_session.url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/phoenix/session/session.py\u001b[0m in \u001b[0;36mlaunch_app\u001b[0;34m(primary, reference, corpus, trace, default_umap_parameters, host, port, root_path, run_in_thread, notebook_environment, use_temp_dir)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ğŸŒ To view the Phoenix app in your browser, visit {_session.url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_temp_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ğŸ’½ Your data is being persisted to {get_printable_db_url(database_url)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/phoenix/session/session.py\u001b[0m in \u001b[0;36murl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;34m\"\"\"Returns the url for the phoenix app\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_get_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/phoenix/session/session.py\u001b[0m in \u001b[0;36m_get_url\u001b[0;34m(host, port, notebook_env, root_path)\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"google.colab.kernel.proxyPort({port}, {{'cache': true}})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnotebook_env\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mNotebookEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAGEMAKER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;31m# NB: Sagemaker notebooks only work with port 6006 - which is used by tensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title Initiate ğŸš€ Phoenix monitoring of Langchain / LangGraph\n",
        "import phoenix as px\n",
        "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
        "import os, time\n",
        "from google.colab import output\n",
        "\n",
        "os.environ[\"PHOENIX_PORT\"] = (\n",
        "    \"6002\"  # Petite sÃ©curitÃ© pour Ã©viter le conflit de ports si vous relancez plusieurs fois\n",
        ")\n",
        "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = (\n",
        "    \"http://localhost:\" + os.environ[\"PHOENIX_PORT\"] + \"/v1/traces\"\n",
        ")\n",
        "os.environ[\"PHOENIX_PROJECT_NAME\"] = \"langgraph-data-toulon\"\n",
        "\n",
        "try:\n",
        "    phoenix_session = px.launch_app()  # 1. Lancer l'UI locale\n",
        "    time.sleep(5)  # Give it time to spin up\n",
        "    print(f\"ğŸš€ Phoenix UI is ready at: {phoenix_session.url}\")\n",
        "    # try: # L'instumentation est dÃ©placÃ©e aprÃ¨s l'initialisation de TruGraph pour se brancher dessus pour permettre de partager le flux OTEL\n",
        "    #     LangChainInstrumentor().instrument() # 2. Activer l'instrumentation\n",
        "    #     print(\"âœ… Instrumentation activÃ©e.\") # LangChainInstrumentor capture aussi les noeuds LangGraph de base\n",
        "    # except Exception as e:\n",
        "    #     print(f\"âš ï¸ Erreur d'instrumentation (peut-Ãªtre dÃ©jÃ  active): {e}\")\n",
        "    output.serve_kernel_port_as_iframe(\n",
        "        os.environ[\"PHOENIX_PORT\"], height=1000\n",
        "    )  # Cela ouvre une fenÃªtre directement dans le notebook\n",
        "except Exception as e:\n",
        "    print(f\"Erreur au lancement: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV6A4xnNrweW"
      },
      "source": [
        "CrÃ©ation de prompts.py, helper.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IylUwZSkrsRf"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# %%writefile prompts.py\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# from langchain.schema import HumanMessage  # type: ignore[import-not-found]\n",
        "from langchain_core.messages import HumanMessage\n",
        "import json\n",
        "from typing import Optional\n",
        "from langgraph.graph import MessagesState\n",
        "from langgraph.types import Command\n",
        "from typing import Literal, Optional, List, Dict, Any, Type\n",
        "\n",
        "MAX_REPLANS = 2\n",
        "\n",
        "\n",
        "# Custom State class with specific keys\n",
        "class State(MessagesState):\n",
        "    enabled_agents: Optional[List[str]]\n",
        "    # Current plan only: mapping from step number (as string) to step definition\n",
        "    plan: Optional[Dict[str, Dict[str, Any]]]\n",
        "    user_query: Optional[str]\n",
        "    current_step: int\n",
        "    replan_flag: Optional[bool]\n",
        "    last_reason: Optional[str]\n",
        "    # Replan attempts tracked per step number\n",
        "    replan_attempts: Optional[Dict[int, int]]\n",
        "    agent_query: Optional[str]\n",
        "\n",
        "\n",
        "def get_agent_descriptions() -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Return structured agent descriptions with capabilities and guidelines.\n",
        "    Edit this function to change how the planner/executor reason about agents.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"web_researcher\": {\n",
        "            \"name\": \"Web Researcher\",\n",
        "            \"capability\": \"Fetch public data via Tavily web search\",\n",
        "            \"use_when\": \"Public information, news, current events, or external facts are needed\",\n",
        "            \"limitations\": \"Cannot access private/internal company data\",\n",
        "            \"output_format\": \"Raw research data and findings from public sources\",\n",
        "        },\n",
        "        \"cortex_researcher\": {\n",
        "            \"name\": \"Cortex Researcher\",\n",
        "            # \"capability\": \"Query private/company data in Snowflake, including structured deal records (company name, deal value, sales rep, close date, deal status, product line) and unstructured sales meeting notes, via Snowflake Cortex Agents.\",\n",
        "            # \"use_when\": \"Internal documents, company databases, or private data access is required\",\n",
        "            # \"limitations\": \"Cannot access public web data\",\n",
        "            # \"output_format\": \"For structured requests, return the exact fields and include SQL when applicable; for unstructured, return concise relevant excerpts with citations.\",\n",
        "            \"capability\": \"Query general knowledge. Use interal company data.\",\n",
        "            \"use_when\": \"Questions about internal company information or data.\",\n",
        "            \"limitations\": \"Cannot access data from internet\",\n",
        "            \"output_format\": \"documents or text excerpts from internal data sources.\",\n",
        "        },\n",
        "        \"chart_generator\": {\n",
        "            \"name\": \"Chart Generator\",\n",
        "            \"capability\": \"Build visualizations from structured data\",\n",
        "            \"use_when\": \"User explicitly requests charts, graphs, plots, visualizations (keywords: chart, graph, plot, visualise, bar-chart, line-chart, histogram, etc.)\",\n",
        "            \"limitations\": \"Requires structured data input from previous steps\",\n",
        "            \"output_format\": \"Visual charts and graphs\",\n",
        "            \"position_requirement\": \"Must be used as final step after data gathering is complete\",\n",
        "        },\n",
        "        \"chart_summarizer\": {\n",
        "            \"name\": \"Chart Summarizer\",\n",
        "            \"capability\": \"Summarize and explain chart visualizations\",\n",
        "            \"use_when\": \"After chart_generator has created a visualization\",\n",
        "            \"limitations\": \"Requires a chart as input\",\n",
        "            \"output_format\": \"Written summary and analysis of chart content\",\n",
        "        },\n",
        "        \"synthesizer\": {\n",
        "            \"name\": \"Synthesizer\",\n",
        "            \"capability\": \"Write comprehensive prose summaries of findings\",\n",
        "            \"use_when\": \"Final step when no visualization is requested - combines all previous research\",\n",
        "            \"limitations\": \"Requires research data from previous steps\",\n",
        "            \"output_format\": \"Coherent written summary incorporating all findings\",\n",
        "            \"position_requirement\": \"Should be used as final step when no chart is needed\",\n",
        "        },\n",
        "    }\n",
        "\n",
        "\n",
        "def _get_enabled_agents(state: State | None = None) -> List[str]:\n",
        "    \"\"\"Return enabled agents; if absent, use baseline/default.\n",
        "\n",
        "    Supports both dict-style and attribute-style state objects.\n",
        "    \"\"\"\n",
        "    baseline = [\"web_researcher\", \"chart_generator\", \"chart_summarizer\", \"synthesizer\"]\n",
        "    if not state:\n",
        "        return baseline\n",
        "    val = (\n",
        "        state.get(\"enabled_agents\")\n",
        "        if hasattr(state, \"get\")\n",
        "        else getattr(state, \"enabled_agents\", None)\n",
        "    )\n",
        "\n",
        "    if isinstance(val, list) and val:\n",
        "        allowed = {\n",
        "            \"web_researcher\",\n",
        "            \"cortex_researcher\",\n",
        "            \"chart_generator\",\n",
        "            \"chart_summarizer\",\n",
        "            \"synthesizer\",\n",
        "        }\n",
        "        filtered = [a for a in val if a in allowed]\n",
        "        return filtered\n",
        "    return baseline\n",
        "\n",
        "\n",
        "def format_agent_list_for_planning(state: State | None = None) -> str:\n",
        "    \"\"\"\n",
        "    Format agent descriptions for the planning prompt.\n",
        "    \"\"\"\n",
        "    descriptions = get_agent_descriptions()\n",
        "    enabled_list = _get_enabled_agents(state)\n",
        "    agent_list = []\n",
        "\n",
        "    for agent_key, details in descriptions.items():\n",
        "        if agent_key not in enabled_list:\n",
        "            continue\n",
        "        agent_list.append(f\"  â€¢ `{agent_key}` â€“ {details['capability']}\")\n",
        "\n",
        "    return \"\\n\".join(agent_list)\n",
        "\n",
        "\n",
        "def format_agent_guidelines_for_planning(state: State | None = None) -> str:\n",
        "    \"\"\"\n",
        "    Format agent usage guidelines for the planning prompt.\n",
        "    \"\"\"\n",
        "    descriptions = get_agent_descriptions()\n",
        "    enabled = set(_get_enabled_agents(state))\n",
        "    guidelines = []\n",
        "\n",
        "    # Cortex vs Web researcher (only include guidance for enabled agents)\n",
        "    if \"cortex_researcher\" in enabled:\n",
        "        guidelines.append(\n",
        "            f\"- Use `cortex_researcher` when {descriptions['cortex_researcher']['use_when'].lower()}.\"\n",
        "        )\n",
        "    if \"web_researcher\" in enabled:\n",
        "        guidelines.append(\n",
        "            f\"- Use `web_researcher` for {descriptions['web_researcher']['use_when'].lower()}.\"\n",
        "        )\n",
        "\n",
        "    # Chart generator specific rules\n",
        "    if \"chart_generator\" in enabled:\n",
        "        chart_desc = descriptions[\"chart_generator\"]\n",
        "        cs_hint = (\n",
        "            \" A `chart_summarizer` should be used to summarize the chart.\"\n",
        "            if \"chart_summarizer\" in enabled\n",
        "            else \"\"\n",
        "        )\n",
        "        guidelines.append(\n",
        "            f\"- **Include `chart_generator` _only_ if {chart_desc['use_when'].lower()}**. Do NOT use it for text summaries, news articles, or lists of topics. If included, `chart_generator` must be {chart_desc['position_requirement'].lower()}.\"\n",
        "        )\n",
        "\n",
        "    # Synthesizer default\n",
        "    if \"synthesizer\" in enabled:\n",
        "        synth_desc = descriptions[\"synthesizer\"]\n",
        "        guidelines.append(\n",
        "            f\"  â€“ Otherwise use `synthesizer` as {synth_desc['position_requirement'].lower()}, and be sure to include all of the data from the previous steps.\"\n",
        "        )\n",
        "\n",
        "    return \"\\n\".join(guidelines)\n",
        "\n",
        "\n",
        "def format_agent_guidelines_for_executor(state: State | None = None) -> str:\n",
        "    \"\"\"\n",
        "    Format agent usage guidelines for the executor prompt.\n",
        "    \"\"\"\n",
        "    descriptions = get_agent_descriptions()\n",
        "    enabled = _get_enabled_agents(state)\n",
        "    guidelines = []\n",
        "\n",
        "    if \"web_researcher\" in enabled:\n",
        "        web_desc = descriptions[\"web_researcher\"]\n",
        "        guidelines.append(\n",
        "            f\"- Use `\\\"web_researcher\\\"` when {web_desc['use_when'].lower()}.\"\n",
        "        )\n",
        "    if \"cortex_researcher\" in enabled:\n",
        "        cortex_desc = descriptions[\"cortex_researcher\"]\n",
        "        guidelines.append(\n",
        "            f\"- Use `\\\"cortex_researcher\\\"` for {cortex_desc['use_when'].lower()}.\"\n",
        "        )\n",
        "\n",
        "    return \"\\n\".join(guidelines)\n",
        "\n",
        "\n",
        "def plan_prompt(state: State) -> HumanMessage:\n",
        "    \"\"\"\n",
        "    Build the prompt that instructs the LLM to return a highâ€‘level plan.\n",
        "    \"\"\"\n",
        "    replan_flag = state.get(\"replan_flag\", False)\n",
        "    user_query = state.get(\"user_query\", state[\"messages\"][0].content)\n",
        "    prior_plan = state.get(\"plan\") or {}\n",
        "    replan_reason = state.get(\"last_reason\", \"\")\n",
        "\n",
        "    # Get agent descriptions dynamically\n",
        "\n",
        "    agent_list = format_agent_list_for_planning(state)\n",
        "    agent_guidelines = format_agent_guidelines_for_planning(state)\n",
        "\n",
        "    enabled_list = _get_enabled_agents(state)\n",
        "\n",
        "    # Build planner agent enum based on enabled agents\n",
        "    enabled_for_planner = [\n",
        "        a\n",
        "        for a in enabled_list\n",
        "        if a\n",
        "        in (\"web_researcher\", \"cortex_researcher\", \"chart_generator\", \"synthesizer\")\n",
        "    ]\n",
        "    planner_agent_enum = (\n",
        "        \" | \".join(enabled_for_planner)\n",
        "        or \"web_researcher | chart_generator | synthesizer\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "        You are the **Planner** in a multiâ€‘agent system.  Break the user's request\n",
        "        into a sequence of numbered steps (1,â€¯2,â€¯3, â€¦).  **There is no hard limit on\n",
        "        step count** as long as the plan is concise and each step has a clear goal.\n",
        "\n",
        "        You may decompose the user's query into sub-queries, but **prioritize grouping related information retrieval**.\n",
        "        Avoid creating unnecessary granular steps in order to save execution budget while maintaining quality.\n",
        "\n",
        "        For example, \"Find the top 5 cities AND their populations\" should be a SINGLE step, not two.\n",
        "\n",
        "        However, if the user's query is \"What were the key\n",
        "        action items in the last quarter, and what was a recent news story for\n",
        "        each of them?\", you may break it into steps:\n",
        "\n",
        "        1. Fetch the key action items in the last quarter.\n",
        "        2. Fetch a recent news story for the first action item.\n",
        "        3. Fetch a recent news story for the second action item.\n",
        "        4. Fetch a recent news story for the last action item\n",
        "\n",
        "        Here is a list of available agents you can call upon to execute the tasks in your plan. You may call only one agent per step.\n",
        "\n",
        "        {agent_list}\n",
        "\n",
        "        Return **ONLY** valid JSON (no markdown, no explanations) in this form:\n",
        "\n",
        "        {{\n",
        "        \"1\": {{\n",
        "            \"agent\": \"{planner_agent_enum}\",\n",
        "            \"action\": \"string\",\n",
        "        }},\n",
        "        \"2\": {{ ... }},\n",
        "        \"3\": {{ ... }}\n",
        "        }}\n",
        "\n",
        "        Guidelines:\n",
        "        {agent_guidelines}\n",
        "        \"\"\"\n",
        "\n",
        "    if replan_flag:\n",
        "        prompt += f\"\"\"\n",
        "        The current plan needs revision because: {replan_reason}\n",
        "\n",
        "        Current plan:\n",
        "        {json.dumps(prior_plan, indent=2)}\n",
        "\n",
        "        When replanning:\n",
        "        - Focus on UNBLOCKING the workflow rather than perfecting it.\n",
        "        - Only modify steps that are truly preventing progress.\n",
        "        - Prefer simpler, more achievable alternatives over complex rewrites.\n",
        "        \"\"\"\n",
        "\n",
        "    else:\n",
        "        prompt += \"\\nGenerate a new plan from scratch.\"\n",
        "\n",
        "    prompt += f'\\nUser query: \"{user_query}\"'\n",
        "\n",
        "    return HumanMessage(content=prompt)\n",
        "\n",
        "\n",
        "# @instrument(attributes=lambda ret, exception, *args, **kwargs: {\"retrieved_execution\": ret.update.get(\"messages\", [HumanMessage(content=\"\")])[-1].content})\n",
        "def executor_prompt(state: State) -> HumanMessage:\n",
        "    \"\"\"\n",
        "    Build the singleâ€‘turn JSON prompt that drives the executor LLM.\n",
        "    \"\"\"\n",
        "    step = int(state.get(\"current_step\", 0))\n",
        "    latest_plan: Dict[str, Any] = state.get(\"plan\") or {}\n",
        "    plan_block: Dict[str, Any] = latest_plan.get(str(step), {})\n",
        "    max_replans = MAX_REPLANS\n",
        "    attempts = (state.get(\"replan_attempts\", {}) or {}).get(step, 0)\n",
        "\n",
        "    # Get agent guidelines dynamically\n",
        "    executor_guidelines = format_agent_guidelines_for_executor(state)\n",
        "    plan_agent = plan_block.get(\"agent\", \"web_researcher\")\n",
        "\n",
        "    messages_tail = (state.get(\"messages\") or [])[-4:]\n",
        "\n",
        "    executor_prompt = f\"\"\"\n",
        "        **IMPORTANT:** Respond **ONLY** with a valid JSON object. Do NOT include any additional text, explanation, or conversational phrases, such as \"FINAL ANSWER\".\n",
        "\n",
        "        {{\n",
        "        \"replan\": <true|false>,\n",
        "        \"goto\": \"<{ '|'.join([a for a in _get_enabled_agents(state) if a in ['web_researcher','cortex_researcher','chart_generator','chart_summarizer','synthesizer']] + ['planner']) }>\",\n",
        "        \"reason\": \"<1 sentence>\",\n",
        "        \"query\": \"<text>\"\n",
        "        }}\n",
        "\n",
        "        You are the **executor** in a multiâ€‘agent system with these agents:\n",
        "        `{ '`, `'.join(sorted(set([a for a in _get_enabled_agents(state) if a in ['web_researcher','cortex_researcher','chart_generator','chart_summarizer','synthesizer']] + ['planner']))) }`.\n",
        "\n",
        "        **Tasks**\n",
        "        1. Decide if the current plan needs revision.  â†’ `\"replan_flag\": true|false`\n",
        "        2. Decide which agent to run next.             â†’ `\"goto\": \"<agent_name>\"`\n",
        "        3. Give oneâ€‘sentence justification.            â†’ `\"reason\": \"<text>\"`\n",
        "        4. Write the exact question that the chosen agent should answer\n",
        "                                                    â†’ \"query\": \"<text>\"\n",
        "\n",
        "        **Guidelines**\n",
        "        {executor_guidelines}\n",
        "        - After **{MAX_REPLANS}** failed replans for the same step, move on.\n",
        "        - If you *just replanned* (replan_flag is true) let the assigned agent try before\n",
        "        requesting another replan.\n",
        "\n",
        "        **PRIORITIZE FORWARD PROGRESS:** Only replan if the current step is completely blocked.\n",
        "        1. If any reasonable data was obtained that addresses the step's core goal, set `\"replan\": false` and proceed.\n",
        "        2. Set `\"replan\": true` **only if** ALL of these conditions are met:\n",
        "        â€¢ The step has produced zero useful information\n",
        "        â€¢ The missing information cannot be approximated or obtained by remaining steps\n",
        "        â€¢ `attempts < {max_replans}`\n",
        "        3. When `attempts == {max_replans}`, always move forward (`\"replan\": false`).\n",
        "\n",
        "        ### Decide `\"goto\"`\n",
        "        - If `\"replan\": true` â†’ `\"goto\": \"planner\"`.\n",
        "        - If current step has made reasonable progress â†’ move to next step's agent.\n",
        "        - Otherwise execute the current step's assigned agent (`{plan_agent}`).\n",
        "\n",
        "        ### Build `\"query\"`\n",
        "        Write a clear, standalone instruction for the chosen agent. If the chosen agent\n",
        "        is `web_researcher` or `cortex_researcher`, the query should be a standalone question,\n",
        "        written in plain english, and answerable by the agent.\n",
        "\n",
        "        Ensure that the query uses consistent language as the user's query.\n",
        "\n",
        "        Context you can rely on\n",
        "        - User query ..............: {state.get(\"user_query\")}\n",
        "        - Current step index ......: {step}\n",
        "        - Current plan step .......: {plan_block}\n",
        "        - Justâ€‘replanned flag .....: {state.get(\"replan_flag\")}\n",
        "        - Previous messages .......: {messages_tail}\n",
        "        \"\"\"\n",
        "\n",
        "    return HumanMessage(content=executor_prompt)\n",
        "\n",
        "\n",
        "def agent_system_prompt(suffix: str) -> str:\n",
        "    return (\n",
        "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
        "        \" Use the provided tools to progress towards answering the question.\"\n",
        "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
        "        \" will help where you left off. Execute what you can to make progress.\"\n",
        "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
        "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
        "        f\"\\n{suffix}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH_8GCeQBOWb"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# %%writefile helper.py\n",
        "from __future__ import annotations\n",
        "\n",
        "# pyright: reportMissingImports=false, reportMissingTypeStubs=false, reportIncompatibleMethodOverride=false\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\", message=r\"Valid config keys have changed in V2\", category=UserWarning\n",
        ")\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=r\"WARNING! response_format is not default parameter\",\n",
        "    category=UserWarning,\n",
        ")\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=r\"pkg_resources is deprecated as an API.*\",\n",
        "    category=UserWarning,\n",
        "    module=r\"^munch$\",\n",
        ")\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import wikipedia  # Addition\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# from snowflake.snowpark import Session\n",
        "from langchain_core.tools import tool\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "from typing import Annotated, Literal, Optional, List, Dict, Any, Type\n",
        "from trulens.otel.semconv.trace import SpanAttributes\n",
        "from trulens.core.otel.instrument import instrument\n",
        "\n",
        "# from snowflake.core import Root\n",
        "# from snowflake.core.cortex.lite_agent_service import AgentRunRequest\n",
        "from pydantic import BaseModel, PrivateAttr\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_tavily import TavilySearch\n",
        "\n",
        "# from langchain.schema import HumanMessage\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langgraph.graph import MessagesState, START, StateGraph, END\n",
        "from langgraph.types import Command\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from trulens.core import Feedback, Select\n",
        "from trulens.core.feedback.selector import Selector\n",
        "\n",
        "# from trulens.core.feedback.selector import Selector\n",
        "from trulens.providers.openai import OpenAI\n",
        "import numpy as np\n",
        "\n",
        "# from prompts import plan_prompt, executor_prompt, agent_system_prompt\n",
        "\n",
        "from langgraph.managed.is_last_step import RemainingSteps\n",
        "\n",
        "# load full dotenv\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "# --- HELPERS POUR SELECTION JSON (MODE SANS OTEL) ---\n",
        "def select_context(output):\n",
        "    return [\n",
        "        m.content\n",
        "        for m in output.get(\"messages\", [])\n",
        "        if getattr(m, \"name\", \"\") in [\"web_researcher\", \"cortex_researcher\"]\n",
        "    ]\n",
        "\n",
        "\n",
        "def select_plan_text(output):\n",
        "    for m in output.get(\"messages\", []):\n",
        "        if getattr(m, \"name\", \"\") in [\"initial_plan\", \"replan\"]:\n",
        "            return m.content\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def select_user_query(output):\n",
        "    return output.get(\"user_query\", \"\")\n",
        "\n",
        "\n",
        "def select_final_answer(output):\n",
        "    return output.get(\"final_answer\", \"\")\n",
        "\n",
        "\n",
        "def select_all(data):\n",
        "    return data\n",
        "\n",
        "\n",
        "# Custom State class with specific keys\n",
        "class State(MessagesState):\n",
        "    enabled_agents: Optional[List[str]]\n",
        "    # Current plan only: mapping from step number (as string) to step definition\n",
        "    plan: Optional[Dict[str, Dict[str, Any]]]\n",
        "    user_query: Optional[str]\n",
        "    current_step: int\n",
        "    replan_flag: Optional[bool]\n",
        "    last_reason: Optional[str]\n",
        "    # Replan attempts tracked per step number\n",
        "    replan_attempts: Optional[Dict[int, int]]\n",
        "    agent_query: Optional[str]\n",
        "    remaining_steps: RemainingSteps\n",
        "\n",
        "\n",
        "MAX_REPLANS = 2\n",
        "\n",
        "# # Create a Snowflake session\n",
        "# snowflake_connection_parameters = {\n",
        "#     \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
        "#     \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n",
        "#     \"password\": os.getenv(\"SNOWFLAKE_PAT\"),\n",
        "#     \"database\": os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
        "#     \"schema\": os.getenv(\"SNOWFLAKE_SCHEMA\"),\n",
        "#     \"role\": os.getenv(\"SNOWFLAKE_ROLE\"),\n",
        "#     \"warehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
        "# }\n",
        "\n",
        "# snowpark_session = Session.builder.configs(\n",
        "#     snowflake_connection_parameters\n",
        "# ).create()\n",
        "\n",
        "# create a python repl tool for importing in the lessons\n",
        "repl = PythonREPL()\n",
        "\n",
        "\n",
        "@tool\n",
        "def python_repl_tool(\n",
        "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
        "):\n",
        "    \"\"\"Use this to execute python code. You will be used to execute python code\n",
        "    that generates charts. Only print the chart once.\n",
        "    This is visible to the user.\"\"\"\n",
        "    try:\n",
        "        result = repl.run(code)\n",
        "    except BaseException as e:\n",
        "        return f\"Failed to execute. Error: {repr(e)}\"\n",
        "    result_str = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
        "    return (\n",
        "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
        "    )\n",
        "\n",
        "\n",
        "reasoning_llm = ChatOpenAI(\n",
        "    model=os.environ[\"MODEL_REASONING\"],\n",
        "    model_kwargs={\"response_format\": {\"type\": \"json_object\"}},\n",
        ")\n",
        "\n",
        "\n",
        "@instrument(\n",
        "    attributes=lambda ret, exception, *args, **kwargs: {\n",
        "        \"retrieved_plan\": json.dumps(\n",
        "            ret.update.get(\"plan\", {})\n",
        "        ),  # 1. On capture le Plan (Output)\n",
        "        \"retrieved_query\": args[0].get(\"user_query\")\n",
        "        or args[0].get(\"messages\", [HumanMessage(content=\"\")])[0].content,\n",
        "    }\n",
        ")  # 2. On capture la Query User (Input) depuis l'Ã©tat (args[0] = state) # On essaie de lire 'user_query', sinon on prend le premier message\n",
        "def planner_node(state: State) -> \"Command[Literal['executor']]\":\n",
        "    \"\"\"\n",
        "    Runs the planning LLM and stores the resulting plan in state.\n",
        "    \"\"\"\n",
        "    # 1. Invoke LLM with the planner prompt\n",
        "    llm_reply = reasoning_llm.invoke([plan_prompt(state)])\n",
        "\n",
        "    # 2. Validate JSON\n",
        "    try:\n",
        "        content_str = (\n",
        "            llm_reply.content\n",
        "            if isinstance(llm_reply.content, str)\n",
        "            else str(llm_reply.content)\n",
        "        )\n",
        "        parsed_plan = json.loads(content_str)\n",
        "    except json.JSONDecodeError:\n",
        "        raise ValueError(f\"Planner returned invalid JSON:\\n{llm_reply.content}\")\n",
        "\n",
        "    # 3. Store as current plan only\n",
        "    replan = state.get(\"replan_flag\", False)\n",
        "    updated_plan: Dict[str, Any] = parsed_plan\n",
        "\n",
        "    return Command(\n",
        "        update={\n",
        "            \"plan\": updated_plan,\n",
        "            \"messages\": [\n",
        "                HumanMessage(\n",
        "                    content=llm_reply.content,\n",
        "                    name=\"replan\" if replan else \"initial_plan\",\n",
        "                )\n",
        "            ],\n",
        "            \"user_query\": state.get(\"user_query\", state[\"messages\"][0].content),\n",
        "            \"current_step\": 1 if not replan else state[\"current_step\"],\n",
        "            # Preserve replan flag so executor runs planned agent once before reconsidering\n",
        "            \"replan_flag\": state.get(\"replan_flag\", False),\n",
        "            \"last_reason\": \"\",\n",
        "            \"enabled_agents\": state.get(\"enabled_agents\"),\n",
        "        },\n",
        "        goto=\"executor\",\n",
        "    )\n",
        "\n",
        "\n",
        "# ## Create executor\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "@instrument(\n",
        "    attributes=lambda ret, exception, *args, **kwargs: {\n",
        "        \"retrieved_execution\": ret.update.get(\"messages\", [HumanMessage(content=\"\")])[\n",
        "            -1\n",
        "        ].content\n",
        "    }\n",
        ")  # On capture la rÃ©ponse de l'executor depuis l'objet Command\n",
        "def executor_node(\n",
        "    state: State,\n",
        ") -> Command[\n",
        "    Literal[\n",
        "        \"web_researcher\",\n",
        "        \"cortex_researcher\",\n",
        "        \"chart_generator\",\n",
        "        \"synthesizer\",\n",
        "        \"planner\",\n",
        "    ]\n",
        "]:\n",
        "\n",
        "    plan: Dict[str, Any] = state.get(\"plan\", {})\n",
        "    step: int = state.get(\"current_step\", 1)\n",
        "\n",
        "    # 0) If we *just* replanned, run the planned agent once before reconsidering.\n",
        "    if state.get(\"replan_flag\"):\n",
        "        planned_agent = plan.get(str(step), {}).get(\"agent\")\n",
        "        return Command(\n",
        "            update={\n",
        "                \"replan_flag\": False,\n",
        "                \"current_step\": step\n",
        "                + 1,  # advance because we executed the planned agent\n",
        "            },\n",
        "            goto=planned_agent,\n",
        "        )\n",
        "\n",
        "    # 1) Build prompt & call LLM\n",
        "    llm_reply = reasoning_llm.invoke([executor_prompt(state)])\n",
        "    try:\n",
        "        content_str = (\n",
        "            llm_reply.content\n",
        "            if isinstance(llm_reply.content, str)\n",
        "            else str(llm_reply.content)\n",
        "        )\n",
        "        parsed = json.loads(content_str)\n",
        "        replan: bool = parsed[\"replan\"]\n",
        "        goto: str = parsed[\"goto\"]\n",
        "        reason: str = parsed[\"reason\"]\n",
        "        query: str = parsed[\"query\"]\n",
        "    except Exception as exc:\n",
        "        raise ValueError(f\"Invalid executor JSON:\\n{llm_reply.content}\") from exc\n",
        "\n",
        "    # Upodate the state\n",
        "    updates: Dict[str, Any] = {\n",
        "        \"messages\": [HumanMessage(content=llm_reply.content, name=\"executor\")],\n",
        "        \"last_reason\": reason,\n",
        "        \"agent_query\": query,\n",
        "    }\n",
        "\n",
        "    # Replan accounting\n",
        "    replans: Dict[int, int] = state.get(\"replan_attempts\", {}) or {}\n",
        "    step_replans = replans.get(step, 0)\n",
        "\n",
        "    # 2) Replan decision\n",
        "    if replan:\n",
        "        if step_replans < MAX_REPLANS:\n",
        "            replans[step] = step_replans + 1\n",
        "            updates.update(\n",
        "                {\n",
        "                    \"replan_attempts\": replans,\n",
        "                    \"replan_flag\": True,  # ensure next turn executes the planned agent once\n",
        "                    \"current_step\": step,  # stay on same step for the new plan\n",
        "                }\n",
        "            )\n",
        "            return Command(update=updates, goto=\"planner\")\n",
        "        else:\n",
        "            # Cap hit: skip this step; let next step (or synthesizer) handle termination\n",
        "            next_agent = plan.get(str(step + 1), {}).get(\"agent\", \"synthesizer\")\n",
        "            updates[\"current_step\"] = step + 1\n",
        "            return Command(update=updates, goto=next_agent)\n",
        "\n",
        "    # 3) Happy path: run chosen agent; advance only if following the plan\n",
        "    planned_agent = plan.get(str(step), {}).get(\"agent\")\n",
        "    updates[\"current_step\"] = step + 1 if goto == planned_agent else step\n",
        "    updates[\"replan_flag\"] = False\n",
        "    return Command(update=updates, goto=goto)\n",
        "\n",
        "\n",
        "# Set semantic model file (for analyst) and search service name\n",
        "# SEMANTIC_MODEL_FILE = \"@sales_intelligence.data.models/sales_metrics_model.yaml\"\n",
        "# CORTEX_SEARCH_SERVICE = \"sales_intelligence.data.sales_conversation_search\"\n",
        "\n",
        "# ---- Agent Setup ----\n",
        "# class CortexAgentArgs(BaseModel):\n",
        "#     query: str\n",
        "\n",
        "# class CortexAgentTool:\n",
        "# ....\n",
        "\n",
        "# def __init__(self, session: Session):\n",
        "# ....\n",
        "\n",
        "# def _consume_stream(self, stream):\n",
        "# ....\n",
        "\n",
        "# def run(self, query: str, **kwargs):\n",
        "# ....\n",
        "\n",
        "# cortex_agent_tool = CortexAgentTool(session=snowpark_session)\n",
        "\n",
        "\n",
        "@tool\n",
        "def local_rag_tool(query: str):\n",
        "    \"\"\"\n",
        "    Retrieves information from local documents to answer questions.\n",
        "    Use this for information related to internal company data or documents.\n",
        "    \"\"\"\n",
        "    return search_rag(vector_store_splits, query, k=3)\n",
        "\n",
        "\n",
        "# NEW ------------------------------------\n",
        "# 1. Wikipedia Tool (Replaces Cortex Search - Unstructured)\n",
        "@tool\n",
        "def wikipedia_rag_tool(query: str):\n",
        "    \"\"\"\n",
        "    Retrieves unstructured information from Wikipedia to answer general knowledge questions.\n",
        "    Use this for definitions, history, summaries, or non-tabular data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        search_results = wikipedia.search(query, results=1)\n",
        "        if not search_results:\n",
        "            return \"No relevant Wikipedia pages found.\"\n",
        "\n",
        "        page = wikipedia.page(search_results[0], auto_suggest=False)\n",
        "        summary = page.content[:2000]\n",
        "        return f\"Source: {page.title}\\nURL: {page.url}\\n\\nContent:\\n{summary}\"\n",
        "    except Exception as e:\n",
        "        return f\"Wikipedia Error: {e}\"\n",
        "\n",
        "\n",
        "# 2. Wikidata SPARQL Tool (Replaces Cortex Analyst - Structured)\n",
        "@tool\n",
        "def wikidata_sparql_tool(query: str):\n",
        "    \"\"\"\n",
        "    Retrieves structured data (lists, counts, dates, facts) from Wikidata.\n",
        "    The input must be a natural language question. The tool will generate and execute SPARQL.\n",
        "    Use this when you need tables, specific data points, or relationships.\n",
        "    \"\"\"\n",
        "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
        "    sparql.setReturnFormat(JSON)\n",
        "\n",
        "    # Internal helper to translate Natural Language -> SPARQL\n",
        "    # We use a small inline LLM call for this translation\n",
        "    translator_llm = ChatOpenAI(model=os.environ[\"MODEL_EXECUTOR\"], temperature=0)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Translate this question into a valid SPARQL query for Wikidata.\n",
        "    Question: {query}\n",
        "\n",
        "    Return ONLY the SPARQL code inside ```sparql ... ``` blocks.\n",
        "    Ensure prefixes like wdt: and wd: are correct.\n",
        "    Limit results to 10 unless specified.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = translator_llm.invoke(prompt)\n",
        "        content = response.content\n",
        "\n",
        "        # Extract SPARQL code block\n",
        "        if \"```sparql\" in content:\n",
        "            query_code = content.split(\"```sparql\")[1].split(\"```\")[0].strip()\n",
        "        elif \"```\" in content:\n",
        "            query_code = content.split(\"```\")[1].split(\"```\")[0].strip()\n",
        "        else:\n",
        "            query_code = content.strip()\n",
        "\n",
        "        # Execute\n",
        "        sparql.setQuery(query_code)\n",
        "        results = sparql.query().convert()\n",
        "\n",
        "        # Parse JSON results into a string table\n",
        "        bindings = results[\"results\"][\"bindings\"]\n",
        "        if not bindings:\n",
        "            return \"No results found in Wikidata.\"\n",
        "\n",
        "        output_lines = []\n",
        "        for item in bindings:\n",
        "            row = []\n",
        "            for key in item:\n",
        "                row.append(f\"{key}: {item[key]['value']}\")\n",
        "            output_lines.append(\", \".join(row))\n",
        "\n",
        "        return f\"SPARQL Query Executed:\\n{query_code}\\n\\nResults:\\n\" + \"\\n\".join(\n",
        "            output_lines\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"SPARQL Error: {e}\"\n",
        "\n",
        "\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# from prompts import agent_system_prompt\n",
        "\n",
        "llm = ChatOpenAI(model=os.environ[\"MODEL_EXECUTOR\"])\n",
        "\n",
        "_cortex_llm_with_tools = llm.bind_tools([])\n",
        "# cortex_agent = create_react_agent(llm, tools=[cortex_agent_tool.run], prompt=agent_system_prompt(f\"\"\"\n",
        "#         You are the Researcher. You can answer questions\n",
        "#         using customer deal data along with meeting notes.\n",
        "#         Do not take any further action.\n",
        "#     \"\"\"))\n",
        "# cortex_agent = create_react_agent(\n",
        "#     llm,\n",
        "#     tools=[wikipedia_rag_tool, wikidata_sparql_tool],\n",
        "#     max_iterations=3,\n",
        "#     prompt=agent_system_prompt(f\"\"\"\n",
        "#         You are the Cortex Researcher replacement.\n",
        "#         You have two tools:\n",
        "#         1. `wikidata_sparql_tool`: For STRUCTURED questions (lists, stats, facts).\n",
        "#         2. `wikipedia_rag_tool`: For UNSTRUCTURED questions (summaries, history).\n",
        "\n",
        "#         Choose the right tool based on the user's request.\n",
        "#      \"\"\"))\n",
        "\n",
        "\n",
        "@instrument(\n",
        "    span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
        "    attributes=lambda ret, exception, *args, **kwargs: {\n",
        "        SpanAttributes.RETRIEVAL.QUERY_TEXT: (\n",
        "            args[0].get(\"agent_query\") if args[0].get(\"agent_query\") else None\n",
        "        ),\n",
        "        SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: (\n",
        "            [ret.update[\"messages\"][-1].content]\n",
        "            if hasattr(ret, \"update\")\n",
        "            else \"No tool call\"\n",
        "        ),\n",
        "    },\n",
        ")\n",
        "def cortex_agents_research_node(\n",
        "    state: State,\n",
        ") -> Command[Literal[\"executor\"]]:\n",
        "    \"\"\"\n",
        "    Cortex researcher using simple tool-calling (NO ReAct loop = NO recursion issue).\n",
        "\n",
        "    Flow:\n",
        "    1. LLM decides which tool to call\n",
        "    2. Execute that tool once\n",
        "    3. Return result\n",
        "    \"\"\"\n",
        "    query = state.get(\"agent_query\", state.get(\"user_query\", \"\"))\n",
        "\n",
        "    # Prompt that guides tool selection\n",
        "    prompt = f\"\"\"You are a research assistant. Use one of your available tools to answer this query.\n",
        "\n",
        "Available tools:\n",
        "- rag tool: Use this to answer general knowledge questions by retrieving unstructured information from Wikipedia. Ideal for definitions, history, summaries, or non-tabular data.\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Call the most appropriate tool to answer this query.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Single LLM call - it will decide which tool to use\n",
        "        response = _cortex_llm_with_tools.invoke([HumanMessage(content=prompt)])\n",
        "\n",
        "        # Check if LLM made tool calls\n",
        "        if hasattr(response, \"tool_calls\") and response.tool_calls:\n",
        "            results = []\n",
        "            for tool_call in response.tool_calls:\n",
        "                tool_name = tool_call.get(\"name\", \"\")\n",
        "                tool_args = tool_call.get(\"args\", {})\n",
        "\n",
        "                # Get the query argument (tools expect 'query' parameter)\n",
        "                tool_query = tool_args.get(\"query\", query)\n",
        "\n",
        "                # Execute the tool\n",
        "                try:\n",
        "                    if tool_name == \"local_rag_tool\":\n",
        "                        result = local_rag_tool.invoke({\"query\": tool_query})\n",
        "                    else:\n",
        "                        result = f\"Unknown tool: {tool_name}\"\n",
        "                except Exception as tool_error:\n",
        "                    result = f\"Tool {tool_name} failed: {str(tool_error)}\"\n",
        "\n",
        "                results.append(f\"=== {tool_name} ===\\n{result}\")\n",
        "\n",
        "            final_content = \"\\n\\n\".join(results)\n",
        "        else:\n",
        "            # LLM didn't call a tool - use its direct response or fallback\n",
        "            final_content = (\n",
        "                response.content\n",
        "                if response.content\n",
        "                else f\"No tool was called. Query: {query}\"\n",
        "            )\n",
        "\n",
        "    except Exception as e:\n",
        "        final_content = f\"Research failed: {str(e)}\"\n",
        "\n",
        "    new_message = HumanMessage(content=final_content, name=\"cortex_researcher\")\n",
        "\n",
        "    return Command(\n",
        "        update={\"messages\": [new_message]},\n",
        "        goto=\"executor\",\n",
        "    )\n",
        "\n",
        "\n",
        "# ## Create Web Search Agent\n",
        "\n",
        "tavily_tool = TavilySearch(max_results=5)\n",
        "\n",
        "llm = ChatOpenAI(model=os.environ[\"MODEL_EXECUTOR\"])\n",
        "\n",
        "# Research agent and node\n",
        "web_search_agent = create_react_agent(\n",
        "    llm,\n",
        "    tools=[tavily_tool],\n",
        "    prompt=agent_system_prompt(\n",
        "        f\"\"\"\n",
        "        You are the Researcher. You can ONLY perform research by using the provided search tool (tavily_tool).\n",
        "        When you have found the necessary information, end your output.\n",
        "        Do NOT attempt to take further actions.\n",
        "    \"\"\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "@instrument(\n",
        "    span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
        "    attributes=lambda ret, exception, *args, **kwargs: {\n",
        "        SpanAttributes.RETRIEVAL.QUERY_TEXT: args[0].get(\"agent_query\", \"\"),\n",
        "        SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: (\n",
        "            [ret.update[\"messages\"][-1].content]\n",
        "            if hasattr(ret, \"update\")\n",
        "            else \"No tool call\"\n",
        "        ),\n",
        "    },\n",
        ")\n",
        "def web_research_node(\n",
        "    state: State,\n",
        ") -> Command[Literal[\"executor\"]]:\n",
        "    agent_query = state.get(\"agent_query\")\n",
        "    result = web_search_agent.invoke(\n",
        "        {\"messages\": agent_query}, config={\"recursion_limit\": 5}\n",
        "    )\n",
        "    messages = (\n",
        "        [HumanMessage(content=agent_query)]\n",
        "        if isinstance(agent_query, str)\n",
        "        else agent_query\n",
        "    )\n",
        "    result = web_search_agent.invoke({\"messages\": messages})\n",
        "    goto = \"executor\"\n",
        "    # wrap in a human message, as not all providers allow\n",
        "    # AI message at the last position of the input messages list\n",
        "    result[\"messages\"][-1] = HumanMessage(\n",
        "        content=result[\"messages\"][-1].content, name=\"web_researcher\"\n",
        "    )\n",
        "    return Command(\n",
        "        update={\n",
        "            # share internal message history of research agent with other agents\n",
        "            \"messages\": result[\"messages\"],\n",
        "        },\n",
        "        goto=goto,\n",
        "    )\n",
        "\n",
        "\n",
        "# ## Create Charting Agent\n",
        "\n",
        "# Chart generator agent and node\n",
        "# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
        "chart_agent = create_react_agent(\n",
        "    llm,\n",
        "    [python_repl_tool],\n",
        "    prompt=agent_system_prompt(\n",
        "        \"You can only generate charts. You are working with a researcher colleague. Print the chart first. Then, save the chart to a file in the current working directory and provide the path to the chart_summarizer.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "def chart_node(state: State) -> Command[Literal[\"chart_summarizer\"]]:\n",
        "    result = chart_agent.invoke(state)\n",
        "    # wrap in a human message, as not all providers allow\n",
        "    # AI message at the last position of the input messages list\n",
        "    result[\"messages\"][-1] = HumanMessage(\n",
        "        content=result[\"messages\"][-1].content, name=\"chart_generator\"\n",
        "    )\n",
        "    goto = \"chart_summarizer\"\n",
        "    return Command(\n",
        "        update={\n",
        "            # share internal message history of chart agent with other agents\n",
        "            \"messages\": result[\"messages\"],\n",
        "        },\n",
        "        goto=goto,\n",
        "    )\n",
        "\n",
        "\n",
        "# ## Create Chart Summary Agent\n",
        "\n",
        "chart_summary_agent = create_react_agent(\n",
        "    llm,\n",
        "    tools=[],  # Add image processing tools if available/needed.\n",
        "    prompt=agent_system_prompt(\n",
        "        \"You can only summarize the chart that was generated by the chart generator to answer the user's question. You are working with a researcher colleague and a chart generator colleague. \"\n",
        "        + \"Your task is to generate a standalone, concise summary for the provided chart image saved at a local PATH, where the PATH should be and only be provided by your chart generator colleague. The summary should be no more than 3 sentences and should not mention the chart itself.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "def chart_summary_node(\n",
        "    state: State,\n",
        ") -> Command[Literal[END]]:\n",
        "    result = chart_summary_agent.invoke(state)\n",
        "    print(f\"Chart summarizer answer: {result['messages'][-1].content}\")\n",
        "    # Ensure the summary message is attributed to chart_summarizer for downstream use\n",
        "    result[\"messages\"][-1] = HumanMessage(\n",
        "        content=result[\"messages\"][-1].content, name=\"chart_summarizer\"\n",
        "    )\n",
        "    # Send to the end node\n",
        "    goto = END\n",
        "    return Command(\n",
        "        update={\n",
        "            # share internal message history of chart agent with other agents\n",
        "            \"messages\": result[\"messages\"],\n",
        "            \"final_answer\": result[\"messages\"][-1].content,\n",
        "        },\n",
        "        goto=goto,\n",
        "    )\n",
        "\n",
        "\n",
        "# ## Create a Synthesizer Agent\n",
        "def synthesizer_node(state: State) -> Command[Literal[END]]:\n",
        "    \"\"\"\n",
        "    Creates a concise, humanâ€‘readable summary of the entire interaction,\n",
        "    **purely in prose**.\n",
        "\n",
        "    It ignores structured tables or chart IDs and instead rewrites the\n",
        "    relevant agent messages (research results, chart commentary, etc.)\n",
        "    into a short final answer.\n",
        "    \"\"\"\n",
        "    # Gather informative messages for final synthesis\n",
        "    relevant_msgs = []\n",
        "    for m in state.get(\"messages\", []):\n",
        "        if getattr(m, \"name\", None) in (\n",
        "            \"web_researcher\",\n",
        "            \"cortex_researcher\",\n",
        "            \"chart_generator\",\n",
        "            \"chart_summarizer\",\n",
        "        ):\n",
        "            # FIX: Robustly handle content types and TRUNCATE huge outputs to avoid Token Limit Errors\n",
        "            raw_content = m.content\n",
        "            if isinstance(raw_content, list):\n",
        "                # Handle multimodal content (list of dicts) by flattening to string\n",
        "                text_content = \" \".join([str(item) for item in raw_content])\n",
        "            else:\n",
        "                text_content = str(raw_content) if raw_content else \"\"\n",
        "\n",
        "            # Truncate to ~15k chars per message to be safe (keeps context manageable)\n",
        "            if len(text_content) > 15000:\n",
        "                text_content = text_content[:15000] + \"... [TRUNCATED DUE TO LENGTH]\"\n",
        "\n",
        "            relevant_msgs.append(text_content)\n",
        "\n",
        "    # Fallback for user query extraction\n",
        "    messages_list = state.get(\"messages\", [])\n",
        "    if messages_list and hasattr(messages_list[0], \"content\"):\n",
        "        default_query = messages_list[0].content\n",
        "    else:\n",
        "        default_query = \"\"\n",
        "\n",
        "    user_question = state.get(\"user_query\", default_query)\n",
        "\n",
        "    synthesis_instructions = (\n",
        "        \"You are the Synthesizer. Use the context below to directly answer the user's question. \"  # UPDATED THIS LINE\n",
        "        \"Perform any lightweight calculations, comparisons, or inferences required. \"  # ADDED THIS LINE\n",
        "        \"Do not invent facts not supported by the context. If data is missing, say what's missing and, if helpful, \"  # UPDATED THIS LINE\n",
        "        \"offer a clearly labeled best-effort estimate with assumptions.\\n\\n\"  # ADDED THIS LINE\n",
        "        \"Produce a concise response that fully answers the question, with the following guidance:\\n\"  # UPDATED THIS LINE\n",
        "        \"- Start with the direct answer (one short paragraph or a tight bullet list).\\n\"\n",
        "        \"- Include key figures from any 'Results:' tables (e.g., totals, top items).\\n\"\n",
        "        \"- If any message contains citations, include them as a brief 'Citations: [...]' line.\\n\"\n",
        "        \"- Keep the output crisp; avoid meta commentary or tool instructions.\"\n",
        "    )\n",
        "\n",
        "    summary_prompt = [\n",
        "        HumanMessage(\n",
        "            content=(\n",
        "                f\"User question: {user_question}\\n\\n\"\n",
        "                f\"{synthesis_instructions}\\n\\n\"\n",
        "                f\"Context:\\n\\n\" + \"\\n\\n---\\n\\n\".join(relevant_msgs)\n",
        "            )\n",
        "        )\n",
        "    ]\n",
        "    llm_reply = llm.invoke(summary_prompt)\n",
        "\n",
        "    reply_content = llm_reply.content\n",
        "    if isinstance(reply_content, list):\n",
        "        reply_text = \"\".join(\n",
        "            [c if isinstance(c, str) else str(c) for c in reply_content]\n",
        "        )\n",
        "    else:\n",
        "        reply_text = str(reply_content)\n",
        "    answer = reply_text.strip()\n",
        "    print(f\"Synthesizer answer: {answer}\")\n",
        "\n",
        "    return Command(\n",
        "        update={\n",
        "            \"final_answer\": answer,\n",
        "            \"messages\": [HumanMessage(content=answer, name=\"synthesizer\")],\n",
        "        },\n",
        "        goto=END,  # hand off to the END node\n",
        "    )\n",
        "\n",
        "\n",
        "##############################\n",
        "# Eval RAG Triad Evaluations #\n",
        "##############################\n",
        "provider = OpenAI(model_engine=os.environ[\"MODEL_EVAL\"])\n",
        "\n",
        "# Groundedness: retrieved contexts (RETRIEVAL spans) vs final answer (main output)\n",
        "f_groundedness = (\n",
        "    Feedback(provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
        "    .on(\n",
        "        {\n",
        "            \"source\": Selector(\n",
        "                span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
        "                span_attribute=SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS,\n",
        "                collect_list=True,\n",
        "            )\n",
        "        }\n",
        "    )\n",
        "    .on_output()\n",
        ")  # maps \"statement\" to the app main output in OTEL mode\n",
        "\n",
        "# Question/answer relevance: main input vs main output\n",
        "f_answer_relevance = (\n",
        "    Feedback(provider.relevance_with_cot_reasons, name=\"Answer Relevance\")\n",
        "    .on_input()  # maps \"prompt\" (or input) from app main input\n",
        "    .on_output()\n",
        ")  # maps \"response\" (or output) from app main output\n",
        "\n",
        "# Context relevance: main input vs each retrieved context chunk\n",
        "f_context_relevance = (\n",
        "    Feedback(provider.context_relevance_with_cot_reasons, name=\"Context Relevance\")\n",
        "    .on_input()\n",
        "    .on(\n",
        "        {\n",
        "            \"context\": Selector(\n",
        "                span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
        "                span_attribute=SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS,\n",
        "                collect_list=False,\n",
        "            )\n",
        "        }\n",
        "    )\n",
        "    .aggregate(np.mean)\n",
        ")\n",
        "\n",
        "######################\n",
        "# Eval Goal-Plan-Act #\n",
        "######################\n",
        "gpa_eval_provider = OpenAI(model_engine=os.environ[\"MODEL_EVAL\"])\n",
        "\n",
        "f_logical_consistency = Feedback(\n",
        "    gpa_eval_provider.logical_consistency_with_cot_reasons, name=\"Logical Consistency\"\n",
        ").on({\"trace\": Selector(trace_level=True)})\n",
        "\n",
        "f_execution_efficiency = Feedback(\n",
        "    gpa_eval_provider.execution_efficiency_with_cot_reasons, name=\"Execution Efficiency\"\n",
        ").on({\"trace\": Selector(trace_level=True)})\n",
        "\n",
        "f_plan_adherence = Feedback(\n",
        "    gpa_eval_provider.relevance_with_cot_reasons, name=\"Plan Adherence\"\n",
        ").on(\n",
        "    {\n",
        "        \"prompt\": Selector(\n",
        "            span_attribute=\"retrieved_plan\"\n",
        "        ),  # On lit l'Ã©tiquette du Planner\n",
        "        \"response\": Selector(\n",
        "            span_attribute=\"retrieved_execution\"\n",
        "        ),  # On lit l'Ã©tiquette de l'Executor\n",
        "    }\n",
        ")\n",
        "\n",
        "f_plan_quality = Feedback(\n",
        "    gpa_eval_provider.relevance_with_cot_reasons, name=\"Plan Quality\"\n",
        ").on(\n",
        "    {\n",
        "        # CORRECTION : On utilise le Selector sur l'attribut qu'on vient de crÃ©er\n",
        "        \"prompt\": Selector(span_attribute=\"retrieved_query\"),\n",
        "        # Le plan (inchangÃ©)\n",
        "        \"response\": Selector(span_attribute=\"retrieved_plan\"),\n",
        "    }\n",
        ")\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "\n",
        "def display_eval_reason(text, width=800):\n",
        "    # Strip any trailing \"Score: X\" from the end of the text\n",
        "    raw_text = str(text).rstrip()\n",
        "    cleaned_text = re.sub(\n",
        "        r\"\\s*Score:\\s*-?\\d+(?:\\.\\d+)?\\s*$\", \"\", raw_text, flags=re.IGNORECASE\n",
        "    )\n",
        "    # Convert newlines to HTML line breaks, then wrap\n",
        "    html_text = cleaned_text.replace(\"\\n\", \"<br><br>\")\n",
        "    display(\n",
        "        HTML(\n",
        "            f'<div style=\"font-size: 15px; word-wrap: break-word; width: {width}px;\">{html_text}</div>'\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2X76Ix3smig"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import warnings\n",
        "\n",
        "load_dotenv(override=True)\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQnIEtuSrSej"
      },
      "source": [
        "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
        "<p> ğŸ’» &nbsp; <b>To access <code>requirements.txt</code>, <code>env.template</code>, <code>prompts.py</code>, and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook 2) click on <em>\"Open\"</em>.\n",
        "\n",
        "<p> â¬‡ &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT0DTuaYrSek"
      },
      "source": [
        "## 6.1 Add inline evaluations (skipped, already set in helpers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCfJPCIyrSel"
      },
      "source": [
        "## 6.2 Update the planning prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAUWHUW8rSel"
      },
      "source": [
        "Add pre-conditions, post-conditions, and goals to each step in the agent's plan.\n",
        "\n",
        "Adding this explicit detail helps the executor understand the goal of each step, which improves tool calling and agent decisions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfxnL3yJrSem"
      },
      "outputs": [],
      "source": [
        "# import helper\n",
        "# import prompts\n",
        "# from langchain.schema import HumanMessage\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "RECURSION_LIMIT = 15\n",
        "\n",
        "original_plan_prompt_fn = plan_prompt\n",
        "\n",
        "\n",
        "def patched_plan_prompt(state):\n",
        "    # FIX: Call the saved original function, NOT the global 'plan_prompt'\n",
        "    base = original_plan_prompt_fn(state).content\n",
        "    insertion = '\"action\": \"string\",\\n            \"pre_conditions\": [\"string\", ...],\\n            \"post_conditions\": [\"string\", ...],\\n            \"goal\": \"string\",'\n",
        "    base = base.replace('\"action\": \"string\",', insertion)\n",
        "\n",
        "    current_step = state.get(\"current_step\", 1)\n",
        "    used = max(0, int(current_step) - 1)\n",
        "    remaining = max(0, RECURSION_LIMIT - used)\n",
        "    base += f\"\\n\\n<budget> Actions Budget Used: {used}, Max Budget Remaining: {remaining}.  ## IMPORTANT: Make the best use of the available resources. </budget>\"\n",
        "\n",
        "    return HumanMessage(content=base)\n",
        "\n",
        "\n",
        "plan_prompt = patched_plan_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSd6pZTWrSem"
      },
      "source": [
        "## 6.3 Build the graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUqYw7ehrSem"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "\n",
        "# from helper import State, planner_node, executor_node, chart_node, chart_summary_node, synthesizer_node, web_research_node, cortex_agents_research_node\n",
        "\n",
        "workflow = StateGraph(State)\n",
        "workflow.add_node(\"planner\", planner_node)\n",
        "workflow.add_node(\"executor\", executor_node)\n",
        "workflow.add_node(\"web_researcher\", web_research_node)\n",
        "workflow.add_node(\"cortex_researcher\", cortex_agents_research_node)\n",
        "workflow.add_node(\"chart_generator\", chart_node)\n",
        "workflow.add_node(\"chart_summarizer\", chart_summary_node)\n",
        "workflow.add_node(\"synthesizer\", synthesizer_node)\n",
        "\n",
        "workflow.add_edge(START, \"planner\")\n",
        "\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Preconfigure recursion_limit once (avoid passing it on every invoke).\n",
        "try:\n",
        "    graph = graph.with_config({\"recursion_limit\": RECURSION_LIMIT})\n",
        "except Exception:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKWpOyHKrSem"
      },
      "source": [
        "## 6.4 Create a TruLens session for logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me21sjYtrSem",
        "outputId": "8b1ed10c-7080-4509-fc0f-d67b54542645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¦‘ Initialized with db url sqlite:///default.sqlite .\n",
            "ğŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n"
          ]
        }
      ],
      "source": [
        "from trulens.core.session import TruSession\n",
        "from trulens.core.database.connector.default import DefaultDBConnector\n",
        "\n",
        "# Initialize connector with SQLite database one folder back\n",
        "connector = DefaultDBConnector(database_url=\"sqlite:///default.sqlite\")\n",
        "\n",
        "# Create TruSession with the custom connector\n",
        "session = TruSession(connector=connector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLu2tCuErSen"
      },
      "source": [
        "## 6.5 Register the new version of the agent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t2-vzKprSen"
      },
      "source": [
        "<div style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\">\n",
        "    <p>ğŸš¨ &nbsp; In this notebook, you are directly provided with the results obtained during filming. This is to help eliminate waiting time, and to prevent potential rate limit errors that might occur in this learning environment (this learning environment is constrained, and the GPA evaluation metrics consume a significant number of tokens).\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5NKZo3wOiKd",
        "outputId": "6e77eef7-4996-446f-f1d0-0c1fa744b419"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "instrumenting <class 'langgraph.graph.state.StateGraph'> for base <class 'langgraph.graph.state.StateGraph'>\n",
            "instrumenting <class 'langgraph.graph.state.CompiledStateGraph'> for base <class 'langgraph.graph.state.CompiledStateGraph'>\n",
            "\tinstrumenting invoke\n",
            "\tinstrumenting ainvoke\n",
            "\tinstrumenting stream\n",
            "\tinstrumenting astream\n",
            "\tinstrumenting astream_events\n",
            "\tinstrumenting stream\n",
            "\tinstrumenting astream\n",
            "\tinstrumenting astream_events\n",
            "\tinstrumenting invoke\n",
            "\tinstrumenting ainvoke\n",
            "\tinstrumenting stream\n",
            "\tinstrumenting astream\n",
            "\tinstrumenting stream_mode\n",
            "instrumenting <class 'langgraph.graph.state.CompiledStateGraph'> for base <class 'langgraph.pregel.main.Pregel'>\n",
            "\tinstrumenting invoke\n",
            "\tinstrumenting ainvoke\n",
            "\tinstrumenting stream\n",
            "\tinstrumenting astream\n",
            "\tinstrumenting astream_events\n",
            "\tinstrumenting stream\n",
            "\tinstrumenting astream\n",
            "\tinstrumenting astream_events\n",
            "\tinstrumenting invoke\n",
            "\tinstrumenting ainvoke\n",
            "\tinstrumenting stream\n",
            "\tinstrumenting astream\n",
            "\tinstrumenting stream_mode\n"
          ]
        }
      ],
      "source": [
        "from trulens.apps.langgraph import TruGraph\n",
        "from trulens.core.schema.feedback import FeedbackMode\n",
        "\n",
        "# from helper import f_answer_relevance, f_context_relevance, f_groundedness, f_logical_consistency, f_execution_efficiency, f_plan_adherence, f_plan_quality\n",
        "\n",
        "selected_feedbacks = [\n",
        "    f_answer_relevance,\n",
        "    f_context_relevance,\n",
        "    f_groundedness,\n",
        "    f_logical_consistency,\n",
        "    f_execution_efficiency,\n",
        "    f_plan_adherence,\n",
        "    f_plan_quality,\n",
        "]\n",
        "\n",
        "tru_recorder = TruGraph(\n",
        "    graph,\n",
        "    app_name=\"Research Data Agent\",\n",
        "    app_version=\"L6: Inline evals + sub-goals in planning prompt\",\n",
        "    feedbacks=selected_feedbacks,\n",
        "    feedback_mode=FeedbackMode.WITH_APP_THREAD,\n",
        "    selector_check_warning=True,\n",
        "    # selector_nocheck=True # selector_check_warning=False, # selector_nocheck=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr5SfyxVpyzI",
        "outputId": "a3fd0739-e1d2-41ce-ed94-f018b46f8cc6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNI [opentelemetry.instrumentation.instrumentor] Attempting to instrument while already instrumented\n"
          ]
        }
      ],
      "source": [
        "# @title Faire un seul TracerProvider global + export Phoenix + instrumentation LangChain ---\n",
        "from opentelemetry import trace\n",
        "from opentelemetry.sdk.trace import TracerProvider as SDKTracerProvider\n",
        "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
        "\n",
        "tp = trace.get_tracer_provider()\n",
        "if not isinstance(tp, SDKTracerProvider):\n",
        "    print(\n",
        "        \"âš ï¸ TracerProvider OTEL inattendu. Assurez-vous que TRULENS_OTEL_TRACING=1 et que TruGraph est initialisÃ© avant ce bloc.\"\n",
        "    )\n",
        "\n",
        "# Ajouter un exporter Phoenix (OTLP HTTP) AU provider global (au lieu de laisser Phoenix/TruLens se battre)\n",
        "_exporter = None\n",
        "try:\n",
        "    from phoenix.otel import HTTPSpanExporter  # type: ignore\n",
        "\n",
        "    _exporter = HTTPSpanExporter(endpoint=os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"])\n",
        "except Exception:\n",
        "    try:\n",
        "        from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter  # type: ignore\n",
        "\n",
        "        _exporter = OTLPSpanExporter(endpoint=os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"])\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Phoenix exporter non initialisÃ©: {e}\")\n",
        "\n",
        "if _exporter is not None:\n",
        "    try:\n",
        "        tp.add_span_processor(BatchSpanProcessor(_exporter))\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Impossible d'ajouter le span processor Phoenix: {e}\")\n",
        "\n",
        "# Instrumentation OpenInference (spans LLM/tools) branchÃ©e sur le provider global TruLens\n",
        "try:\n",
        "    LangChainInstrumentor().instrument(tracer_provider=tp)\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Instrumentation LangChain dÃ©jÃ  active ou erreur: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3st18lorSen"
      },
      "source": [
        "## 6.6 Re-test the agent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7InGlbkorSen"
      },
      "source": [
        "<div style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\">\n",
        "    <p>ğŸš¨ &nbsp;<b>Run Results:</b> In this notebook, you are directly provided with the results obtained during filming. This is to help eliminate waiting time, and to prevent potential rate limit errors that might occur in this learning environment (this learning environment is constrained, and the GPA evaluation metrics consume a significant number of tokens).\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLTWHO_TROs8"
      },
      "source": [
        "**Query 1**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "hDP1BTjyOoug",
        "outputId": "abac5b2d-17e6-4ce2-9b3a-68a792fca1e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What are the top 5 largest cities in France by population ? Chart the population value for each.\n",
            "\u001b[1m[tasks]\u001b[0m {'id': '48d6764c-f377-62fb-5ba8-0b900ca7d53a', 'name': 'planner', 'input': {'messages': [HumanMessage(content='What are the top 5 largest cities in France by population ? Chart the population value for each.', additional_kwargs={}, response_metadata={}, id='8c01f243-f24e-44bc-8530-e171942df065')], 'enabled_agents': ['cortex_researcher', 'web_researcher', 'chart_generator', 'chart_summarizer', 'synthesizer'], 'user_query': 'What are the top 5 largest cities in France by population ? Chart the population value for each.', 'remaining_steps': 14}, 'triggers': ('branch:to:planner',)}\n",
            "\u001b[1m[debug]\u001b[0m {'step': 1, 'timestamp': '2026-01-08T07:33:04.885667+00:00', 'type': 'task', 'payload': {'id': '48d6764c-f377-62fb-5ba8-0b900ca7d53a', 'name': 'planner', 'input': {'messages': [HumanMessage(content='What are the top 5 largest cities in France by population ? Chart the population value for each.', additional_kwargs={}, response_metadata={}, id='8c01f243-f24e-44bc-8530-e171942df065')], 'enabled_agents': ['cortex_researcher', 'web_researcher', 'chart_generator', 'chart_summarizer', 'synthesizer'], 'user_query': 'What are the top 5 largest cities in France by population ? Chart the population value for each.', 'remaining_steps': 14}, 'triggers': ('branch:to:planner',)}}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR [trulens.core.otel.instrument] Error setting attributes: 'NoneType' object has no attribute 'update'\n"
          ]
        },
        {
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j6q74hnze5sb1qc9pg0xfvpk` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5604, Requested 751. Please try again in 3.55s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1580204302.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0;34m\"enabled_agents\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"cortex_researcher\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"web_researcher\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chart_generator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chart_summarizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"synthesizer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             }\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tasks\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"updates\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"debug\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3066\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3068\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3069\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/apps/langgraph/tru_graph.py\u001b[0m in \u001b[0;36minstrumented_generator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    827\u001b[0m                 \u001b[0;31m# Handle sync generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0minstrumented_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moriginal_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m                         \u001b[0;31m# Each chunk typically contains node updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2641\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2642\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2643\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2644\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/apps/langgraph/tru_graph.py\u001b[0m in \u001b[0;36mfiltered_wrapper\u001b[0;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m   1449\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1451\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minstrumented_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \u001b[0;31m# Apply the wrapper using wrapt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2330382181.py\u001b[0m in \u001b[0;36mplanner_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# 1. Invoke LLM with the planner prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mllm_reply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreasoning_llm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplan_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# 2. Validate JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m             cast(\n\u001b[1;32m    401\u001b[0m                 \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                 self.generate_prompt(\n\u001b[0m\u001b[1;32m    403\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1120\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m                 results.append(\n\u001b[0;32m--> 931\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    932\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1226\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraw_response\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"http_response\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_response\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m         if (\n\u001b[1;32m   1388\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_response_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m                     raw_response = (\n\u001b[0;32m-> 1354\u001b[0;31m                         self.root_client.chat.completions.with_raw_response.parse(\n\u001b[0m\u001b[1;32m   1355\u001b[0m                             \u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m                         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"extra_headers\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLegacyAPIResponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    181\u001b[0m             )\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j6q74hnze5sb1qc9pg0xfvpk` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5604, Requested 751. Please try again in 3.55s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "with tru_recorder as recording:\n",
        "    query = \"What are the top 5 largest cities in France by population ? Chart the population value for each.\"\n",
        "    print(f\"Query: {query}\")\n",
        "    state = {\n",
        "        \"messages\": [HumanMessage(content=query)],\n",
        "        \"user_query\": query,\n",
        "        \"enabled_agents\": [\n",
        "            \"cortex_researcher\",\n",
        "            \"web_researcher\",\n",
        "            \"chart_generator\",\n",
        "            \"chart_summarizer\",\n",
        "            \"synthesizer\",\n",
        "        ],\n",
        "    }\n",
        "    graph.invoke(state, print_mode=[\"tasks\", \"updates\", \"debug\"])\n",
        "\n",
        "    print(\"--------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAkVRuo_rSen"
      },
      "outputs": [],
      "source": [
        "records, feedback = session.get_records_and_feedback()\n",
        "if not records.empty:\n",
        "    print(f\"Query: {records.iloc[-1]['input']}\\n\")\n",
        "    print(f\"Output: {records.iloc[-1]['output']}\\n\")\n",
        "else:\n",
        "    print(\"âŒ No records found. Check for errors in the output above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCMGL6KerSen"
      },
      "source": [
        "**Query 2**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehcg__GcRGAV"
      },
      "outputs": [],
      "source": [
        "with tru_recorder as recording:\n",
        "    query = \"Identify our pending deals, research if they may be experiencing regulatory changes, and using the meeting notes for each customer, provide a new value proposition for each given the regulatory changes.\"\n",
        "    print(f\"Query: {query}\")\n",
        "    state = {\n",
        "        \"messages\": [HumanMessage(content=query)],\n",
        "        \"user_query\": query,\n",
        "        \"enabled_agents\": [\n",
        "            \"cortex_researcher\",\n",
        "            \"web_researcher\",\n",
        "            \"chart_generator\",\n",
        "            \"chart_summarizer\",\n",
        "            \"synthesizer\",\n",
        "        ],\n",
        "    }\n",
        "    graph.invoke(state, print_mode=[\"tasks\", \"updates\", \"debug\"])\n",
        "\n",
        "    print(\"--------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc3-VIqQrSeo"
      },
      "outputs": [],
      "source": [
        "records, feedback = session.get_records_and_feedback()\n",
        "if not records.empty:\n",
        "    print(f\"Query: {records.iloc[-1]['input']}\\n\")\n",
        "    print(f\"Output: {records.iloc[-1]['output']}\\n\")\n",
        "else:\n",
        "    print(\"âŒ No records found. Check for errors in the output above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6faL_OQVrSeo"
      },
      "source": [
        "**Query 3**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_idPk2cRYd3"
      },
      "outputs": [],
      "source": [
        "with tru_recorder as recording:\n",
        "    query = \"Identify the largest laboratories studying and developping LLM, then find major topics of those companies in 2026, and find news article about top topics.\"\n",
        "    print(f\"Query: {query}\")\n",
        "    state = {\n",
        "        \"messages\": [HumanMessage(content=query)],\n",
        "        \"user_query\": query,\n",
        "        \"enabled_agents\": [\n",
        "            \"cortex_researcher\",\n",
        "            \"web_researcher\",\n",
        "            \"chart_generator\",\n",
        "            \"chart_summarizer\",\n",
        "            \"synthesizer\",\n",
        "        ],\n",
        "    }\n",
        "    graph.invoke(state, print_mode=[\"tasks\", \"updates\", \"debug\"])\n",
        "\n",
        "    print(\"--------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M-xIWy0rSeo"
      },
      "outputs": [],
      "source": [
        "records, feedback = session.get_records_and_feedback()\n",
        "if not records.empty:\n",
        "    print(f\"Query: {records.iloc[-1]['input']}\\n\")\n",
        "    print(f\"Output: {records.iloc[-1]['output']}\\n\")\n",
        "else:\n",
        "    print(\"âŒ No records found. Check for errors in the output above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYNQla7zrSeo"
      },
      "source": [
        "## 6.7 Launch TruLens dashboard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sydoRIBerSeo"
      },
      "source": [
        "By comparing to the previous version, we can validate the changes.\n",
        "\n",
        "**Note:** Make sure to click on the second link (not the localhost) to open the TruLens dashboard.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkdZ9D11w1E7"
      },
      "outputs": [],
      "source": [
        "# @title ğŸš€ Launch Dashboard (Force Port 8502)\n",
        "!pip install -q trulens-dashboard\n",
        "from google.colab import output\n",
        "from trulens.core import TruSession\n",
        "import time\n",
        "\n",
        "session = TruSession()\n",
        "\n",
        "# Stop any existing dashboards\n",
        "try:\n",
        "    from trulens.dashboard import stop_dashboard\n",
        "    stop_dashboard(force=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(\"â³ Starting Dashboard on port 8502...\")\n",
        "session.start_dashboard(port=8502, force=True)\n",
        "time.sleep(5) # Give it time to spin up\n",
        "\n",
        "print(\"âœ… Dashboard ready.\")\n",
        "#output.serve_kernel_port_as_iframe(8502, height=1000)\n",
        "output.serve_kernel_port_as_window(8502)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfjuz6uMxHsL"
      },
      "outputs": [],
      "source": [
        "# @title Alternative Analysis ğŸ“Š > View Leaderboard as DataFrame\n",
        "from trulens.core import TruSession\n",
        "import pandas as pd\n",
        "\n",
        "max_records = 10\n",
        "session = TruSession()\n",
        "\n",
        "# Get the leaderboard (aggregates metrics by App ID)\n",
        "print(\"ğŸ“Š Leaderboard:\")\n",
        "display(session.get_leaderboard())\n",
        "\n",
        "# OPTIONAL: Get all raw records to debug specific failures\n",
        "print(f\"\\nğŸ“ Last {max_records} Raw Records:\")\n",
        "records, feedback = session.get_records_and_feedback()\n",
        "if not records.empty:\n",
        "    # Show relevant columns only\n",
        "    cols = [\"input\", \"output\", \"latency\", \"total_cost\"] + [\n",
        "        c for c in records.columns if \"Groundedness\" in c or \"Relevance\" in c\n",
        "    ]\n",
        "    # Filter columns that actually exist\n",
        "    valid_cols = [c for c in cols if c in records.columns]\n",
        "    display(records[valid_cols].tail(max_records))\n",
        "else:\n",
        "    print(\"No records found yet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW4QHDdZrSep"
      },
      "source": [
        "**What other improvements could be also done?**\n",
        "\n",
        "-   In this course, we focused on evaluating the end-to-end agent behavior. We could have also tested the behavior of each specialized agent separately to optimize their prompt and design.\n",
        "-   We could have added other metrics for inline-evaluations.\n",
        "-   We could also updated the prompt of the executor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xqf_odiwc73"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwNMoBWeuKr_"
      },
      "source": [
        "# Ajout des modules RAG et _SQL_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbs1wgUQuQ-g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHYjF8tuuZkA"
      },
      "source": [
        "# Vers l'optimisation\n",
        "\n",
        "> TODO: vÃ©rifications en cours: https://chatgpt.com/c/69583b8d-16b4-8327-9cc0-3b5baff84b01\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IS9dJ-dku1UE"
      },
      "outputs": [],
      "source": [
        "#@title Instalation de l'optimiseur gÃ©nÃ©ratif Trace avec un mÃ©canisme de log externe\n",
        "!pip install \"git+https://github.com/doxav/NewTrace.git@json-logs-and-traces-IO\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBxC-S3-vkVD"
      },
      "source": [
        "RÃ©alisez dans la cellule ci-dessous deux exemples d'optimisations avec Trace avec succÃ¨s depuis un exemple existant:\n",
        "\n",
        "-   https://github.com/AgentOpt/OpenTrace/tree/main/examples\n",
        "-   https://agentopt.github.io/OpenTrace/#code-examples (attention la doc a Ã©tÃ© gÃ©nÃ©rÃ©e par IA gÃ©nÃ©rative, il peut y avoir des incohÃ©rences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-KFEvjYweCz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29eOwLnQw7Ad"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RucotjFCucbE"
      },
      "outputs": [],
      "source": [
        "# @title CrÃ©ation de trace_optimize_runtime.py (Attention le code de cette section du TP va Ã©voluer)\n",
        "%%writefile trace_optimize_runtime.py\n",
        "\"\"\"\n",
        "trace_optimize_runtime.py\n",
        "\n",
        "Pont minimal et **non-intrusif** entre :\n",
        "\n",
        "- des exÃ©cutions LangGraph instrumentÃ©es par TruLens (au format *records JSON* TruLens et/ou spans OpenTelemetry),\n",
        "- des feedbacks TruLens (RAG triad + GPA, ou toute autre mÃ©trique),\n",
        "- et l'optimiseur de la lib Trace/OptoPrime (fichiers `JSON_OTEL_trace_optim_demo_*.py`).\n",
        "\n",
        "Objectif : permettre une boucle \"run â†’ trace â†’ feedback â†’ optimise â†’ patch\" **sans modifier**\n",
        "le code du graphe LangGraph (nÅ“uds/agents) dÃ©jÃ  existant.\n",
        "\n",
        "Principes clÃ©s\n",
        "-------------\n",
        "1) **PrÃ©server le graphe causal** : on ne \"aplatit\" pas la trace. Les paramÃ¨tres `param.*`\n",
        "   sont attachÃ©s aux spans qui reprÃ©sentent *rÃ©ellement* les Ã©tapes (planner/executor/â€¦),\n",
        "   et une span `evaluator` est ajoutÃ©e uniquement pour porter `eval.*`.\n",
        "2) **CompatibilitÃ© double** :\n",
        "   - si vous avez une trace OTEL/OTLP (ex: TruLens OTEL activÃ©), on l'utilise directement ;\n",
        "   - sinon, on reconstruit une trace OTLP minimale depuis un *Record* TruLens (JSON standard).\n",
        "3) **Optimisation de code** (pas seulement du prompt tuning) :\n",
        "   on expose du code comme paramÃ¨tre trainable via `param.__code_<key>` et on applique\n",
        "   les patches via compilation + hotpatch (in-place si possible, ou remplacement symbolique).\n",
        "\n",
        "Cette implÃ©mentation vise une approche gÃ©nÃ©rique :\n",
        "- pas de fonctions nommÃ©es \"for_l6\" ;\n",
        "- tout est pilotÃ© par des *configurations* (matchers, specs, targets).\n",
        "\n",
        "PrÃ©-requis au runtime\n",
        "---------------------\n",
        "- TruLens : utilisÃ© pour capturer les records et produire les feedbacks.\n",
        "- (Optionnel) OpenTelemetry : si TruLens exporte des spans OTEL, on peut les \"flusher\".\n",
        "- Trace/Opto (repo Trace/opto) : utilisÃ© pour `otlp_traces_to_trace_json`, `ingest_tgj`,\n",
        "  et l'optimiseur OptoPrimeV2.\n",
        "\n",
        "Remarque : ce fichier ne dÃ©pend pas de LangGraph ni TruLens Ã  l'import.\n",
        "Il se contente de manipuler des JSON/dicts, et d'appliquer des patches Python.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import copy\n",
        "import dataclasses\n",
        "import datetime as _dt\n",
        "import inspect\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import textwrap\n",
        "import time\n",
        "import types\n",
        "import uuid\n",
        "from dataclasses import dataclass, field\n",
        "from typing import (\n",
        "    Any,\n",
        "    Callable,\n",
        "    Dict,\n",
        "    Iterable,\n",
        "    Iterator,\n",
        "    List,\n",
        "    Mapping,\n",
        "    MutableMapping,\n",
        "    Optional,\n",
        "    Sequence,\n",
        "    Tuple,\n",
        "    Union,\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Types simples\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "JSONDict = Dict[str, Any]\n",
        "SpanDict = Dict[str, Any]\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Utilitaires JSON / texte\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def safe_json_dumps(obj: Any, *, max_len: int = 4000) -> str:\n",
        "    \"\"\"\n",
        "    SÃ©rialise `obj` en JSON de maniÃ¨re robuste (fallback str), puis tronque.\n",
        "\n",
        "    Args:\n",
        "        obj: objet Ã  sÃ©rialiser.\n",
        "        max_len: longueur max en caractÃ¨res (au-delÃ , on tronque).\n",
        "\n",
        "    Returns:\n",
        "        str JSON (ou string fallback), tronquÃ©e si nÃ©cessaire.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        s = json.dumps(obj, ensure_ascii=False, default=str)\n",
        "    except Exception:\n",
        "        s = str(obj)\n",
        "    if max_len and len(s) > max_len:\n",
        "        return s[: max_len - 3] + \"...\"\n",
        "    return s\n",
        "\n",
        "\n",
        "def normalize_whitespace(s: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalise lÃ©gÃ¨rement un texte (espaces, lignes vides) pour stabiliser des diffs.\n",
        "\n",
        "    Args:\n",
        "        s: texte.\n",
        "\n",
        "    Returns:\n",
        "        texte normalisÃ©.\n",
        "    \"\"\"\n",
        "    s2 = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "    # Ã©vite de dÃ©truire la mise en forme: on enlÃ¨ve juste les trailing spaces\n",
        "    s2 = \"\\n\".join(line.rstrip() for line in s2.splitlines())\n",
        "    return s2.strip() + (\"\\n\" if s2.endswith(\"\\n\") else \"\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# OTLP helpers (structure JSON)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def _otlp_attr_value(value: Any) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Encode une valeur Python en valeur OTLP JSON (stringValue/doubleValue/intValue/boolValue).\n",
        "\n",
        "    Note:\n",
        "        Pour rester simple et compatible, on privilÃ©gie stringValue.\n",
        "        Les nombres sont encodÃ©s en doubleValue si possibles.\n",
        "\n",
        "    Returns:\n",
        "        dict au format OTLP \"AnyValue\".\n",
        "    \"\"\"\n",
        "    if isinstance(value, bool):\n",
        "        return {\"boolValue\": bool(value)}\n",
        "    if isinstance(value, int) and not isinstance(value, bool):\n",
        "        # OTLP accepte intValue sous forme de chaÃ®ne ou int selon l'impl; on met int.\n",
        "        return {\"intValue\": int(value)}\n",
        "    if isinstance(value, float):\n",
        "        return {\"doubleValue\": float(value)}\n",
        "    # fallback string\n",
        "    return {\"stringValue\": str(value)}\n",
        "\n",
        "\n",
        "def _otlp_kv(key: str, value: Any) -> Dict[str, Any]:\n",
        "    \"\"\"Construit un attribut OTLP (key/value).\"\"\"\n",
        "    return {\"key\": key, \"value\": _otlp_attr_value(value)}\n",
        "\n",
        "\n",
        "def otlp_is_payload(obj: Any) -> bool:\n",
        "    \"\"\"\n",
        "    DÃ©tecte si `obj` ressemble Ã  un payload OTLP traces JSON.\n",
        "\n",
        "    Args:\n",
        "        obj: objet quelconque.\n",
        "\n",
        "    Returns:\n",
        "        True si la structure contient `resourceSpans`.\n",
        "    \"\"\"\n",
        "    return isinstance(obj, dict) and \"resourceSpans\" in obj\n",
        "\n",
        "\n",
        "def otlp_iter_spans(otlp: JSONDict) -> Iterator[SpanDict]:\n",
        "    \"\"\"\n",
        "    ItÃ¨re sur tous les spans d'un payload OTLP.\n",
        "\n",
        "    Args:\n",
        "        otlp: payload OTLP (dict).\n",
        "\n",
        "    Yields:\n",
        "        chaque span (dict) *mutable*.\n",
        "    \"\"\"\n",
        "    for rs in otlp.get(\"resourceSpans\", []) or []:\n",
        "        for ss in rs.get(\"scopeSpans\", []) or []:\n",
        "            for sp in ss.get(\"spans\", []) or []:\n",
        "                yield sp\n",
        "\n",
        "\n",
        "def otlp_span_attrs_to_dict(span: SpanDict) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Convertit la liste `span[\"attributes\"]` en dict {key: python_value}.\n",
        "\n",
        "    Args:\n",
        "        span: dict OTLP span.\n",
        "\n",
        "    Returns:\n",
        "        dict (valeurs simplifiÃ©es).\n",
        "    \"\"\"\n",
        "    out: Dict[str, Any] = {}\n",
        "    for kv in span.get(\"attributes\", []) or []:\n",
        "        k = kv.get(\"key\")\n",
        "        v = kv.get(\"value\", {})\n",
        "        if not k:\n",
        "            continue\n",
        "        # choisir un champ OTLP\n",
        "        if \"stringValue\" in v:\n",
        "            out[k] = v[\"stringValue\"]\n",
        "        elif \"doubleValue\" in v:\n",
        "            out[k] = float(v[\"doubleValue\"])\n",
        "        elif \"intValue\" in v:\n",
        "            out[k] = int(v[\"intValue\"])\n",
        "        elif \"boolValue\" in v:\n",
        "            out[k] = bool(v[\"boolValue\"])\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "\n",
        "def otlp_set_span_attribute(span: SpanDict, key: str, value: Any) -> None:\n",
        "    \"\"\"\n",
        "    Ajoute ou remplace un attribut OTLP sur un span.\n",
        "\n",
        "    Args:\n",
        "        span: span OTLP mutable.\n",
        "        key: clÃ© d'attribut.\n",
        "        value: valeur (sera encodÃ©e).\n",
        "    \"\"\"\n",
        "    attrs = span.get(\"attributes\")\n",
        "    if attrs is None:\n",
        "        attrs = []\n",
        "        span[\"attributes\"] = attrs\n",
        "\n",
        "    # replace if exists\n",
        "    for kv in attrs:\n",
        "        if kv.get(\"key\") == key:\n",
        "            kv[\"value\"] = _otlp_attr_value(value)\n",
        "            return\n",
        "\n",
        "    attrs.append(_otlp_kv(key, value))\n",
        "\n",
        "\n",
        "def otlp_get_trace_id(otlp: JSONDict) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Renvoie un traceId (hex) du payload OTLP si prÃ©sent.\n",
        "\n",
        "    Args:\n",
        "        otlp: payload OTLP.\n",
        "\n",
        "    Returns:\n",
        "        traceId (32 hex chars) ou None.\n",
        "    \"\"\"\n",
        "    for sp in otlp_iter_spans(otlp):\n",
        "        tid = sp.get(\"traceId\")\n",
        "        if tid:\n",
        "            return tid\n",
        "    return None\n",
        "\n",
        "\n",
        "def _new_trace_id_hex() -> str:\n",
        "    \"\"\"GÃ©nÃ¨re un traceId OTLP (32 hex chars).\"\"\"\n",
        "    return uuid.uuid4().hex  # 32 hex\n",
        "\n",
        "\n",
        "def _new_span_id_hex() -> str:\n",
        "    \"\"\"GÃ©nÃ¨re un spanId OTLP (16 hex chars).\"\"\"\n",
        "    return f\"{random.getrandbits(64):016x}\"\n",
        "\n",
        "\n",
        "def ensure_otlp_shell(\n",
        "    *,\n",
        "    service_name: str = \"app\",\n",
        "    scope_name: str = \"trace_opt\",\n",
        ") -> JSONDict:\n",
        "    \"\"\"\n",
        "    Construit un \"shell\" OTLP vide compatible avec `otlp_traces_to_trace_json`.\n",
        "\n",
        "    Args:\n",
        "        service_name: nom de ressource OTEL.\n",
        "        scope_name: nom du scope.\n",
        "\n",
        "    Returns:\n",
        "        dict OTLP avec `resourceSpans/scopeSpans/spans`.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"resourceSpans\": [\n",
        "            {\n",
        "                \"resource\": {\n",
        "                    \"attributes\": [\n",
        "                        _otlp_kv(\"service.name\", service_name),\n",
        "                    ]\n",
        "                },\n",
        "                \"scopeSpans\": [\n",
        "                    {\n",
        "                        \"scope\": {\"name\": scope_name, \"version\": \"\"},\n",
        "                        \"spans\": [],\n",
        "                    }\n",
        "                ],\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "\n",
        "def otlp_append_span(otlp: JSONDict, span: SpanDict) -> None:\n",
        "    \"\"\"\n",
        "    Ajoute un span Ã  la premiÃ¨re scopeSpan du payload.\n",
        "\n",
        "    Args:\n",
        "        otlp: payload OTLP.\n",
        "        span: span dict.\n",
        "    \"\"\"\n",
        "    rs_list = otlp.setdefault(\"resourceSpans\", [])\n",
        "    if not rs_list:\n",
        "        otlp.update(ensure_otlp_shell())\n",
        "        rs_list = otlp[\"resourceSpans\"]\n",
        "    rs0 = rs_list[0]\n",
        "    ss_list = rs0.setdefault(\"scopeSpans\", [])\n",
        "    if not ss_list:\n",
        "        ss_list.append({\"scope\": {\"name\": \"trace_opt\", \"version\": \"\"}, \"spans\": []})\n",
        "    ss0 = ss_list[0]\n",
        "    spans = ss0.setdefault(\"spans\", [])\n",
        "    spans.append(span)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Capture OTEL -> OTLP (optionnel)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def try_attach_inmemory_span_exporter() -> Tuple[Optional[Any], Optional[Any], str]:\n",
        "    \"\"\"\n",
        "    Tente d'attacher un `InMemorySpanExporter` au TracerProvider global OpenTelemetry.\n",
        "\n",
        "    Pourquoi:\n",
        "        TruLens peut exporter des spans OTEL (OpenTelemetry). Si on peut accrocher un\n",
        "        exporter en mÃ©moire, on peut rÃ©cupÃ©rer la trace OTLP **sans** modifier le graphe.\n",
        "\n",
        "    Returns:\n",
        "        (exporter, processor, status)\n",
        "\n",
        "        - exporter: instance InMemorySpanExporter ou None\n",
        "        - processor: SimpleSpanProcessor ou None\n",
        "        - status: message (ok / warning / error)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from opentelemetry import trace as otel_trace  # type: ignore\n",
        "        from opentelemetry.sdk.trace.export import InMemorySpanExporter, SimpleSpanProcessor  # type: ignore\n",
        "    except Exception as e:\n",
        "        return None, None, f\"OpenTelemetry SDK indisponible: {e}\"\n",
        "\n",
        "    provider = otel_trace.get_tracer_provider()\n",
        "    if not hasattr(provider, \"add_span_processor\"):\n",
        "        return None, None, \"TracerProvider global n'a pas add_span_processor (provider non-SDK ?)\"\n",
        "\n",
        "    try:\n",
        "        exporter = InMemorySpanExporter()\n",
        "        processor = SimpleSpanProcessor(exporter)\n",
        "        provider.add_span_processor(processor)  # type: ignore[attr-defined]\n",
        "        return exporter, processor, \"ok\"\n",
        "    except Exception as e:\n",
        "        return None, None, f\"Erreur lors de l'attachement de l'exporter: {e}\"\n",
        "\n",
        "\n",
        "def flush_inmemory_exporter_to_otlp(\n",
        "    exporter: Any,\n",
        "    *,\n",
        "    service_name: str = \"app\",\n",
        "    scope_name: str = \"inmemory\",\n",
        "    clear: bool = True,\n",
        ") -> JSONDict:\n",
        "    \"\"\"\n",
        "    Convertit les spans collectÃ©s par `InMemorySpanExporter` en payload OTLP JSON.\n",
        "\n",
        "    Args:\n",
        "        exporter: instance InMemorySpanExporter.\n",
        "        service_name: resource.service.name.\n",
        "        scope_name: scopeSpans.scope.name.\n",
        "        clear: si True, vider l'exporter aprÃ¨s lecture.\n",
        "\n",
        "    Returns:\n",
        "        OTLP payload dict.\n",
        "    \"\"\"\n",
        "    otlp = ensure_otlp_shell(service_name=service_name, scope_name=scope_name)\n",
        "\n",
        "    spans = list(getattr(exporter, \"get_finished_spans\")() or [])\n",
        "    if clear and hasattr(exporter, \"clear\"):\n",
        "        exporter.clear()\n",
        "\n",
        "    for sp in spans:\n",
        "        try:\n",
        "            ctx = sp.get_span_context()\n",
        "            trace_id = f\"{ctx.trace_id:032x}\"\n",
        "            span_id = f\"{ctx.span_id:016x}\"\n",
        "        except Exception:\n",
        "            # fallback (rare)\n",
        "            trace_id = _new_trace_id_hex()\n",
        "            span_id = _new_span_id_hex()\n",
        "\n",
        "        parent_span_id = \"\"\n",
        "        try:\n",
        "            parent = getattr(sp, \"parent\", None)\n",
        "            if parent is not None:\n",
        "                parent_span_id = f\"{parent.span_id:016x}\"\n",
        "        except Exception:\n",
        "            parent_span_id = \"\"\n",
        "\n",
        "        name = getattr(sp, \"name\", \"span\")\n",
        "        start_ns = int(getattr(sp, \"start_time\", time.time_ns()))\n",
        "        end_ns = int(getattr(sp, \"end_time\", start_ns + 1_000_000))\n",
        "\n",
        "        attrs_list: List[Dict[str, Any]] = []\n",
        "        attrs = getattr(sp, \"attributes\", {}) or {}\n",
        "        if isinstance(attrs, dict):\n",
        "            for k, v in attrs.items():\n",
        "                # Pour rester robuste, on encode en string (Trace/otel_adapter sait parser stringValue).\n",
        "                attrs_list.append(_otlp_kv(str(k), safe_json_dumps(v, max_len=8000)))\n",
        "\n",
        "        otlp_append_span(\n",
        "            otlp,\n",
        "            {\n",
        "                \"traceId\": trace_id,\n",
        "                \"spanId\": span_id,\n",
        "                \"parentSpanId\": parent_span_id,\n",
        "                \"name\": str(name),\n",
        "                \"kind\": \"INTERNAL\",\n",
        "                \"startTimeUnixNano\": start_ns,\n",
        "                \"endTimeUnixNano\": end_ns,\n",
        "                \"attributes\": attrs_list,\n",
        "            },\n",
        "        )\n",
        "\n",
        "    return otlp\n",
        "\n",
        "\n",
        "# TruLens record JSON -> OTLP (fallback si pas de spans OTEL disponibles)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def trulens_is_record(obj: Any) -> bool:\n",
        "    \"\"\"\n",
        "    Heuristique: dÃ©tecte si `obj` ressemble Ã  un Record TruLens (JSON standard).\n",
        "\n",
        "    Un Record TruLens (voir doc) contient typiquement `record_id` et `calls`.\n",
        "\n",
        "    Args:\n",
        "        obj: objet.\n",
        "\n",
        "    Returns:\n",
        "        True si on dÃ©tecte des champs \"record_id\" ou \"calls\".\n",
        "    \"\"\"\n",
        "    return isinstance(obj, dict) and (\"calls\" in obj or \"record_id\" in obj or \"main_input\" in obj)\n",
        "\n",
        "\n",
        "def _parse_dt_to_ns(value: Any) -> Optional[int]:\n",
        "    \"\"\"\n",
        "    Tente de parser des timestamps TruLens (perf.start_time / perf.end_time) vers ns Unix.\n",
        "\n",
        "    Formats acceptÃ©s (best-effort):\n",
        "      - int / float : supposÃ© Ãªtre des secondes (float) ou ns (int trÃ¨s grand).\n",
        "      - str ISO 8601 : ex \"2025-01-02T12:34:56.123Z\"\n",
        "      - datetime.\n",
        "\n",
        "    Returns:\n",
        "        int nanosecondes, ou None si impossible.\n",
        "    \"\"\"\n",
        "    if value is None:\n",
        "        return None\n",
        "\n",
        "    if isinstance(value, int):\n",
        "        # Heuristique: si trÃ¨s grand, c'est dÃ©jÃ  du ns\n",
        "        if value > 10_000_000_000_000:  # > ~1970 + 4h en ns\n",
        "            return value\n",
        "        # sinon secondes\n",
        "        return int(value * 1_000_000_000)\n",
        "\n",
        "    if isinstance(value, float):\n",
        "        return int(value * 1_000_000_000)\n",
        "\n",
        "    if isinstance(value, _dt.datetime):\n",
        "        if value.tzinfo is None:\n",
        "            value = value.replace(tzinfo=_dt.timezone.utc)\n",
        "        return int(value.timestamp() * 1_000_000_000)\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        s = value.strip()\n",
        "        # Z -> +00:00\n",
        "        if s.endswith(\"Z\"):\n",
        "            s = s[:-1] + \"+00:00\"\n",
        "        try:\n",
        "            dt = _dt.datetime.fromisoformat(s)\n",
        "            if dt.tzinfo is None:\n",
        "                dt = dt.replace(tzinfo=_dt.timezone.utc)\n",
        "            return int(dt.timestamp() * 1_000_000_000)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def _trulens_call_name(call: JSONDict) -> str:\n",
        "    \"\"\"\n",
        "    Produit un nom de span \"lisible\" pour un call TruLens.\n",
        "\n",
        "    TruLens record appelle ces objets `RecordAppCall` avec un champ `stack` contenant\n",
        "    des Ã©lÃ©ments `RecordAppCallMethod` incluant `path` et `method`.\n",
        "\n",
        "    StratÃ©gie:\n",
        "      - si on a un `path`, on prend son dernier segment (souvent proche du nom de nÅ“ud)\n",
        "      - sinon, on prend `method.name`\n",
        "      - sinon fallback \"call\"\n",
        "\n",
        "    Returns:\n",
        "        str\n",
        "    \"\"\"\n",
        "    stack = call.get(\"stack\") or []\n",
        "    top = stack[-1] if isinstance(stack, list) and stack else {}\n",
        "    method = (top.get(\"method\") or {}) if isinstance(top, dict) else {}\n",
        "    path = top.get(\"path\") if isinstance(top, dict) else None\n",
        "\n",
        "    # path est souvent un Lens (liste de segments)\n",
        "    last_seg: Optional[str] = None\n",
        "    if isinstance(path, (list, tuple)) and path:\n",
        "        last = path[-1]\n",
        "        if isinstance(last, str):\n",
        "            last_seg = last\n",
        "        else:\n",
        "            last_seg = str(last)\n",
        "    elif isinstance(path, str) and path:\n",
        "        # ex: \"nodes/planner\"\n",
        "        parts = re.split(r\"[\\\\/]+\", path)\n",
        "        last_seg = parts[-1] if parts else path\n",
        "\n",
        "    mname = None\n",
        "    if isinstance(method, dict):\n",
        "        mname = method.get(\"name\") or method.get(\"method_name\") or method.get(\"function_name\")\n",
        "\n",
        "    if last_seg and last_seg not in {\"__call__\", \"invoke\", \"run\"}:\n",
        "        return str(last_seg)\n",
        "    if mname:\n",
        "        return str(mname)\n",
        "    return \"call\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class _CallSpan:\n",
        "    \"\"\"Structure interne pour reconstruire une hiÃ©rarchie approximative.\"\"\"\n",
        "    call_idx: int\n",
        "    name: str\n",
        "    call_id: str\n",
        "    start_ns: Optional[int]\n",
        "    end_ns: Optional[int]\n",
        "    stack_sig: Tuple[str, ...]\n",
        "    args: Any = None\n",
        "    rets: Any = None\n",
        "    error: Optional[str] = None\n",
        "    parent_idx: Optional[int] = None\n",
        "    span_id: str = field(default_factory=_new_span_id_hex)\n",
        "\n",
        "\n",
        "def _call_stack_signature(call: JSONDict) -> Tuple[str, ...]:\n",
        "    \"\"\"\n",
        "    Construit une signature (tuple) Ã  partir de `call.stack` pour aider Ã  infÃ©rer la hiÃ©rarchie.\n",
        "\n",
        "    Returns:\n",
        "        tuple de strings.\n",
        "    \"\"\"\n",
        "    sig: List[str] = []\n",
        "    stack = call.get(\"stack\") or []\n",
        "    if not isinstance(stack, list):\n",
        "        return tuple()\n",
        "\n",
        "    for frame in stack:\n",
        "        if not isinstance(frame, dict):\n",
        "            continue\n",
        "        path = frame.get(\"path\")\n",
        "        method = frame.get(\"method\") or {}\n",
        "        # path normalisÃ©\n",
        "        if isinstance(path, (list, tuple)):\n",
        "            p = \"/\".join(str(x) for x in path)\n",
        "        else:\n",
        "            p = str(path) if path is not None else \"\"\n",
        "        m = \"\"\n",
        "        if isinstance(method, dict):\n",
        "            m = str(method.get(\"name\") or method.get(\"method_name\") or method.get(\"function_name\") or \"\")\n",
        "        sig.append(f\"{p}::{m}\".strip(\":\"))\n",
        "    return tuple(sig)\n",
        "\n",
        "\n",
        "def trulens_record_to_otlp(\n",
        "    record: JSONDict,\n",
        "    *,\n",
        "    service_name: str = \"trulens\",\n",
        "    scope_name: str = \"trulens_record\",\n",
        "    trace_id: Optional[str] = None,\n",
        "    include_root_span: bool = True,\n",
        "    max_io_chars: int = 4000,\n",
        ") -> JSONDict:\n",
        "    \"\"\"\n",
        "    Convertit un Record TruLens (JSON) en payload OTLP minimal.\n",
        "\n",
        "    Cette conversion est un *fallback* quand vous n'avez pas de spans OTEL disponibles.\n",
        "    Elle reconstruit une hiÃ©rarchie de spans Ã  partir des `perf` timestamps (si prÃ©sents),\n",
        "    sinon Ã  partir de signatures de stack (heuristique).\n",
        "\n",
        "    Args:\n",
        "        record: dict JSON TruLens (record).\n",
        "        service_name: service.name OTEL.\n",
        "        scope_name: scope OTEL.\n",
        "        trace_id: si fourni, utilisÃ© comme traceId.\n",
        "        include_root_span: ajoute un span racine \"record\" (recommandÃ©).\n",
        "        max_io_chars: taille max pour input.value / output.value.\n",
        "\n",
        "    Returns:\n",
        "        OTLP payload dict.\n",
        "    \"\"\"\n",
        "    if not trulens_is_record(record):\n",
        "        raise ValueError(\"L'objet fourni ne ressemble pas Ã  un Record TruLens JSON.\")\n",
        "\n",
        "    trace_id = trace_id or _new_trace_id_hex()\n",
        "    otlp = ensure_otlp_shell(service_name=service_name, scope_name=scope_name)\n",
        "\n",
        "    calls = record.get(\"calls\") or []\n",
        "    if not isinstance(calls, list):\n",
        "        calls = []\n",
        "\n",
        "    spans: List[_CallSpan] = []\n",
        "    for idx, call in enumerate(calls):\n",
        "        if not isinstance(call, dict):\n",
        "            continue\n",
        "        call_id = str(call.get(\"call_id\") or call.get(\"callId\") or f\"call-{idx}\")\n",
        "        name = _trulens_call_name(call)\n",
        "        perf = call.get(\"perf\") or {}\n",
        "        start_ns = _parse_dt_to_ns(perf.get(\"start_time\") or perf.get(\"startTime\") or perf.get(\"start\"))\n",
        "        end_ns = _parse_dt_to_ns(perf.get(\"end_time\") or perf.get(\"endTime\") or perf.get(\"end\"))\n",
        "        stack_sig = _call_stack_signature(call)\n",
        "\n",
        "        spans.append(\n",
        "            _CallSpan(\n",
        "                call_idx=idx,\n",
        "                name=name,\n",
        "                call_id=call_id,\n",
        "                start_ns=start_ns,\n",
        "                end_ns=end_ns,\n",
        "                stack_sig=stack_sig,\n",
        "                args=call.get(\"args\"),\n",
        "                rets=call.get(\"rets\"),\n",
        "                error=call.get(\"error\"),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Heuristique de hiÃ©rarchie:\n",
        "    # 1) si des timestamps existent pour la majoritÃ©, on utilise l'inclusion d'intervalles\n",
        "    # 2) sinon, on utilise la relation \"stack prefix\" la plus rÃ©cente\n",
        "    have_times = sum(1 for s in spans if s.start_ns is not None and s.end_ns is not None)\n",
        "    use_interval = have_times >= max(2, int(0.6 * len(spans))) if spans else False\n",
        "\n",
        "    if use_interval:\n",
        "        # Ordre: start asc, end desc (pour bien gÃ©rer les enveloppes)\n",
        "        spans_sorted = sorted(\n",
        "            spans,\n",
        "            key=lambda s: (\n",
        "                s.start_ns if s.start_ns is not None else 0,\n",
        "                -(s.end_ns if s.end_ns is not None else 0),\n",
        "            ),\n",
        "        )\n",
        "        stack: List[_CallSpan] = []\n",
        "        for s in spans_sorted:\n",
        "            s_start = s.start_ns if s.start_ns is not None else 0\n",
        "            # pop les spans qui se terminent avant le start courant\n",
        "            while stack and (stack[-1].end_ns is not None) and s_start >= (stack[-1].end_ns or 0):\n",
        "                stack.pop()\n",
        "            if stack:\n",
        "                s.parent_idx = stack[-1].call_idx\n",
        "            stack.append(s)\n",
        "        # spans_sorted contient des objets de la mÃªme liste => parent_idx est appliquÃ©\n",
        "    else:\n",
        "        # stack prefix: on mappe signature -> dernier idx\n",
        "        last_by_sig: Dict[Tuple[str, ...], int] = {}\n",
        "        for s in spans:\n",
        "            parent_sig = s.stack_sig[:-1] if s.stack_sig else tuple()\n",
        "            if parent_sig in last_by_sig:\n",
        "                s.parent_idx = last_by_sig[parent_sig]\n",
        "            # enregistrer ce call comme dernier pour sa signature\n",
        "            last_by_sig[s.stack_sig] = s.call_idx\n",
        "\n",
        "    # Root span optionnel\n",
        "    root_span_id = _new_span_id_hex()\n",
        "    root_end = max((s.end_ns or 0) for s in spans) if spans else time.time_ns()\n",
        "    root_start = min((s.start_ns or root_end) for s in spans) if spans else root_end - 1_000_000\n",
        "\n",
        "    if include_root_span:\n",
        "        root_span: SpanDict = {\n",
        "            \"traceId\": trace_id,\n",
        "            \"spanId\": root_span_id,\n",
        "            \"parentSpanId\": \"\",\n",
        "            \"name\": \"record\",\n",
        "            \"kind\": \"INTERNAL\",\n",
        "            \"startTimeUnixNano\": int(root_start),\n",
        "            \"endTimeUnixNano\": int(root_end),\n",
        "            \"attributes\": [\n",
        "                _otlp_kv(\"trulens.record_id\", record.get(\"record_id\") or record.get(\"recordId\") or \"\"),\n",
        "                _otlp_kv(\"input.value\", safe_json_dumps(record.get(\"main_input\"), max_len=max_io_chars)),\n",
        "                _otlp_kv(\"output.value\", safe_json_dumps(record.get(\"main_output\"), max_len=max_io_chars)),\n",
        "            ],\n",
        "        }\n",
        "        otlp_append_span(otlp, root_span)\n",
        "\n",
        "    # Convert calls to spans\n",
        "    now_ns = time.time_ns()\n",
        "    for s in spans:\n",
        "        start = s.start_ns or (now_ns + s.call_idx * 1_000_000)\n",
        "        end = s.end_ns or (start + 500_000)\n",
        "\n",
        "        parent_span_id = \"\"\n",
        "        if s.parent_idx is not None:\n",
        "            # retrouver le parent span_id\n",
        "            parent = next((p for p in spans if p.call_idx == s.parent_idx), None)\n",
        "            if parent is not None:\n",
        "                parent_span_id = parent.span_id\n",
        "        elif include_root_span:\n",
        "            parent_span_id = root_span_id\n",
        "\n",
        "        span: SpanDict = {\n",
        "            \"traceId\": trace_id,\n",
        "            \"spanId\": s.span_id,\n",
        "            \"parentSpanId\": parent_span_id,\n",
        "            \"name\": s.name,\n",
        "            \"kind\": \"INTERNAL\",\n",
        "            \"startTimeUnixNano\": int(start),\n",
        "            \"endTimeUnixNano\": int(end),\n",
        "            \"attributes\": [\n",
        "                _otlp_kv(\"trulens.call_id\", s.call_id),\n",
        "                _otlp_kv(\"input.value\", safe_json_dumps(s.args, max_len=max_io_chars)),\n",
        "                _otlp_kv(\"output.value\", safe_json_dumps(s.rets, max_len=max_io_chars)),\n",
        "            ],\n",
        "        }\n",
        "        if s.error:\n",
        "            span[\"attributes\"].append(_otlp_kv(\"error.value\", s.error))\n",
        "        otlp_append_span(otlp, span)\n",
        "\n",
        "    return otlp\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# SÃ©lection de spans / injection de paramÃ¨tres\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class SpanMatcher:\n",
        "    \"\"\"\n",
        "    SÃ©lecteur de spans (OTLP) basÃ© sur des heuristiques simples.\n",
        "\n",
        "    Vous pouvez matcher par :\n",
        "    - substring(s) sur le nom (`name_contains`)\n",
        "    - regex(s) sur le nom (`name_regex`)\n",
        "    - prÃ©sence de certaines clÃ©s d'attributs (`has_attrs`)\n",
        "    - substring(s) sur la valeur d'un attribut (`attr_contains`)\n",
        "\n",
        "    C'est volontairement simple pour rester gÃ©nÃ©rique et portable.\n",
        "    \"\"\"\n",
        "    name_contains: Tuple[str, ...] = ()\n",
        "    name_regex: Tuple[str, ...] = ()\n",
        "    has_attrs: Tuple[str, ...] = ()\n",
        "    attr_contains: Mapping[str, Tuple[str, ...]] = dataclasses.field(default_factory=dict)\n",
        "\n",
        "    def matches(self, span: SpanDict) -> bool:\n",
        "        \"\"\"Retourne True si `span` satisfait ce matcher.\"\"\"\n",
        "        name = str(span.get(\"name\") or \"\")\n",
        "        lname = name.lower()\n",
        "\n",
        "        if self.name_contains:\n",
        "            if not any(sub.lower() in lname for sub in self.name_contains):\n",
        "                return False\n",
        "\n",
        "        if self.name_regex:\n",
        "            ok = False\n",
        "            for pat in self.name_regex:\n",
        "                try:\n",
        "                    if re.search(pat, name):\n",
        "                        ok = True\n",
        "                        break\n",
        "                except re.error:\n",
        "                    continue\n",
        "            if not ok:\n",
        "                return False\n",
        "\n",
        "        if self.has_attrs or self.attr_contains:\n",
        "            attrs = otlp_span_attrs_to_dict(span)\n",
        "            if self.has_attrs:\n",
        "                if not all(k in attrs for k in self.has_attrs):\n",
        "                    return False\n",
        "            for k, subs in self.attr_contains.items():\n",
        "                v = str(attrs.get(k, \"\"))\n",
        "                lv = v.lower()\n",
        "                if not any(s.lower() in lv for s in subs):\n",
        "                    return False\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "def select_spans(otlp: JSONDict, matcher: SpanMatcher) -> List[SpanDict]:\n",
        "    \"\"\"\n",
        "    Retourne la liste des spans matching `matcher`.\n",
        "\n",
        "    Args:\n",
        "        otlp: payload OTLP.\n",
        "        matcher: SpanMatcher.\n",
        "\n",
        "    Returns:\n",
        "        liste de spans (dicts mutables).\n",
        "    \"\"\"\n",
        "    return [sp for sp in otlp_iter_spans(otlp) if matcher.matches(sp)]\n",
        "\n",
        "\n",
        "def select_one_span(otlp: JSONDict, matcher: SpanMatcher) -> Optional[SpanDict]:\n",
        "    \"\"\"\n",
        "    Retourne le premier span matching (ou None).\n",
        "\n",
        "    Astuce:\n",
        "        Pratique pour choisir le parent d'un span evaluator, etc.\n",
        "\n",
        "    Returns:\n",
        "        span dict ou None.\n",
        "    \"\"\"\n",
        "    for sp in otlp_iter_spans(otlp):\n",
        "        if matcher.matches(sp):\n",
        "            return sp\n",
        "    return None\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# SpÃ©cifications de paramÃ¨tres entraÃ®nables\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class ParamSpec:\n",
        "    \"\"\"\n",
        "    DÃ©crit un paramÃ¨tre Ã  :\n",
        "      1) exposer dans la trace (OTLP) via `param.<name>`\n",
        "      2) Ã©ventuellement appliquer au runtime lors d'une update.\n",
        "\n",
        "    Attributes:\n",
        "        name: nom logique (ex: \"planner_addendum\" ou \"__code_planner_node\").\n",
        "        get_value: fonction 0-arg retournant la valeur courante (str conseillÃ©).\n",
        "        apply_update: fonction (new_value:str) -> None, appliquant une update.\n",
        "        attach_to: SpanMatcher indiquant sur quel(s) span(s) Ã©crire l'attribut param.*.\n",
        "        trainable: si False, l'optimiseur ne doit pas toucher ce param.\n",
        "        description: description courte injectÃ©e cÃ´tÃ© optimiseur (conseillÃ©e pour code).\n",
        "        normalize: optionnel, transforme la valeur avant injection (ex: normaliser espaces).\n",
        "    \"\"\"\n",
        "    name: str\n",
        "    get_value: Callable[[], Any]\n",
        "    apply_update: Optional[Callable[[str], None]] = None\n",
        "    attach_to: Optional[SpanMatcher] = None\n",
        "    trainable: bool = True\n",
        "    description: str = \"\"\n",
        "    normalize: Optional[Callable[[str], str]] = normalize_whitespace\n",
        "\n",
        "    def value_as_str(self) -> str:\n",
        "        \"\"\"Renvoie la valeur courante en string (avec normalisation si configurÃ©e).\"\"\"\n",
        "        v = self.get_value()\n",
        "        s = v if isinstance(v, str) else safe_json_dumps(v, max_len=8000)\n",
        "        if self.normalize:\n",
        "            try:\n",
        "                s = self.normalize(s)\n",
        "            except Exception:\n",
        "                pass\n",
        "        return s\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Prompt tuning gÃ©nÃ©rique (addendum non-intrusif)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "class TextOverrideStore:\n",
        "    \"\"\"\n",
        "    Store simple (en mÃ©moire) pour des overrides textuels.\n",
        "\n",
        "    Usage typique:\n",
        "        store = TextOverrideStore()\n",
        "        store.set(\"planner_addendum\", \"...\")\n",
        "\n",
        "    On peut l'utiliser avec `wrap_prompt_builder_with_addendum` pour modifier\n",
        "    une fonction qui retourne un prompt (str ou BaseMessage LangChain).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._values: Dict[str, str] = {}\n",
        "\n",
        "    def get(self, key: str, default: str = \"\") -> str:\n",
        "        return str(self._values.get(key, default))\n",
        "\n",
        "    def set(self, key: str, value: str) -> None:\n",
        "        self._values[str(key)] = str(value)\n",
        "\n",
        "    def as_param_spec(\n",
        "        self,\n",
        "        *,\n",
        "        name: str,\n",
        "        attach_to: Optional[SpanMatcher],\n",
        "        trainable: bool = True,\n",
        "        description: str = \"\",\n",
        "    ) -> ParamSpec:\n",
        "        \"\"\"\n",
        "        Construit un ParamSpec \"texte\" connectÃ© Ã  ce store.\n",
        "\n",
        "        Args:\n",
        "            name: nom du param (clÃ© dans le store).\n",
        "            attach_to: oÃ¹ accrocher dans la trace.\n",
        "            trainable: bool.\n",
        "            description: aide l'optimiseur.\n",
        "\n",
        "        Returns:\n",
        "            ParamSpec.\n",
        "        \"\"\"\n",
        "        return ParamSpec(\n",
        "            name=name,\n",
        "            get_value=lambda: self.get(name, \"\"),\n",
        "            apply_update=lambda v: self.set(name, v),\n",
        "            attach_to=attach_to,\n",
        "            trainable=trainable,\n",
        "            description=description,\n",
        "            normalize=normalize_whitespace,\n",
        "        )\n",
        "\n",
        "\n",
        "def _clone_langchain_message_with_content(msg: Any, new_content: str) -> Any:\n",
        "    \"\"\"\n",
        "    Clone un message LangChain (BaseMessage) en remplaÃ§ant `content`, best-effort.\n",
        "\n",
        "    On Ã©vite d'importer LangChain Ã  l'import du module; l'import est fait ici si possible.\n",
        "\n",
        "    Args:\n",
        "        msg: objet message.\n",
        "        new_content: contenu final.\n",
        "\n",
        "    Returns:\n",
        "        nouveau message (ou fallback str si impossible).\n",
        "    \"\"\"\n",
        "    # cas simple: string\n",
        "    if isinstance(msg, str):\n",
        "        return new_content\n",
        "\n",
        "    # Tentatives LangChain (pydantic/dataclass)\n",
        "    try:\n",
        "        from langchain_core.messages import BaseMessage  # type: ignore\n",
        "        if isinstance(msg, BaseMessage):\n",
        "            # Pydantic v2\n",
        "            if hasattr(msg, \"model_copy\"):\n",
        "                return msg.model_copy(update={\"content\": new_content})\n",
        "            # Pydantic v1\n",
        "            if hasattr(msg, \"copy\"):\n",
        "                try:\n",
        "                    return msg.copy(update={\"content\": new_content})\n",
        "                except TypeError:\n",
        "                    return msg.copy()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Generic: tenter reconstruction via __class__(**fields)\n",
        "    try:\n",
        "        if hasattr(msg, \"model_dump\"):\n",
        "            d = msg.model_dump()\n",
        "        elif hasattr(msg, \"dict\"):\n",
        "            d = msg.dict()\n",
        "        elif dataclasses.is_dataclass(msg):\n",
        "            d = dataclasses.asdict(msg)\n",
        "        else:\n",
        "            d = dict(getattr(msg, \"__dict__\", {}))\n",
        "        d[\"content\"] = new_content\n",
        "        cls = msg.__class__\n",
        "        return cls(**d)\n",
        "    except Exception:\n",
        "        # fallback string\n",
        "        return new_content\n",
        "\n",
        "\n",
        "def wrap_prompt_builder_with_addendum(\n",
        "    prompt_fn: Callable[..., Any],\n",
        "    *,\n",
        "    store: TextOverrideStore,\n",
        "    addendum_key: str,\n",
        "    header: str = \"\\n\\n# Addendum\\n\",\n",
        ") -> Callable[..., Any]:\n",
        "    \"\"\"\n",
        "    Wrap une fonction de prompt pour lui ajouter un \"addendum\" contrÃ´lÃ© par `store`.\n",
        "\n",
        "    - Si `store.get(addendum_key)` est vide â†’ comportement inchangÃ©.\n",
        "    - Sinon â†’ on concatÃ¨ne `original_content + header + addendum`.\n",
        "\n",
        "    CompatibilitÃ©:\n",
        "        - si la fonction renvoie un `str`, on renvoie un `str`\n",
        "        - si elle renvoie un message LangChain, on renvoie un message du mÃªme type (best-effort)\n",
        "\n",
        "    Args:\n",
        "        prompt_fn: fonction originale (ex: prompts.plan_prompt).\n",
        "        store: TextOverrideStore.\n",
        "        addendum_key: nom du param d'override.\n",
        "        header: sÃ©parateur ajoutÃ© avant l'addendum.\n",
        "\n",
        "    Returns:\n",
        "        fonction wrapper.\n",
        "    \"\"\"\n",
        "    def _wrapped(*args, **kwargs):\n",
        "        out = prompt_fn(*args, **kwargs)\n",
        "        add = store.get(addendum_key, \"\").strip()\n",
        "        if not add:\n",
        "            return out\n",
        "\n",
        "        # Extraire le contenu initial\n",
        "        if isinstance(out, str):\n",
        "            base = out\n",
        "        else:\n",
        "            base = str(getattr(out, \"content\", out))\n",
        "\n",
        "        new_content = base + header + add\n",
        "        return _clone_langchain_message_with_content(out, new_content)\n",
        "\n",
        "    # garder un minimum de metadata\n",
        "    try:\n",
        "        _wrapped.__name__ = getattr(prompt_fn, \"__name__\", \"prompt_wrapper\")\n",
        "        _wrapped.__doc__ = getattr(prompt_fn, \"__doc__\", None)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return _wrapped\n",
        "\n",
        "\n",
        "def inject_params_into_otlp(\n",
        "    otlp: JSONDict,\n",
        "    param_specs: Sequence[ParamSpec],\n",
        "    *,\n",
        "    default_attach_to: Optional[SpanMatcher] = None,\n",
        ") -> JSONDict:\n",
        "    \"\"\"\n",
        "    Ajoute des attributs `param.<name>` aux spans OTLP, selon les ParamSpec.\n",
        "\n",
        "    Args:\n",
        "        otlp: payload OTLP (sera copiÃ©).\n",
        "        param_specs: liste des ParamSpec Ã  exposer.\n",
        "        default_attach_to: matcher fallback si ParamSpec.attach_to est None.\n",
        "\n",
        "    Returns:\n",
        "        copie modifiÃ©e du payload OTLP.\n",
        "    \"\"\"\n",
        "    otlp2 = copy.deepcopy(otlp)\n",
        "    for spec in param_specs:\n",
        "        matcher = spec.attach_to or default_attach_to\n",
        "        if matcher is None:\n",
        "            # pas d'endroit oÃ¹ accrocher => skip\n",
        "            continue\n",
        "        val = spec.value_as_str()\n",
        "        for sp in select_spans(otlp2, matcher):\n",
        "            otlp_set_span_attribute(sp, f\"param.{spec.name}\", val)\n",
        "            otlp_set_span_attribute(sp, f\"param.{spec.name}.trainable\", \"true\" if spec.trainable else \"false\")\n",
        "    return otlp2\n",
        "\n",
        "\n",
        "def add_evaluator_span(\n",
        "    otlp: JSONDict,\n",
        "    *,\n",
        "    score: float,\n",
        "    metrics: Mapping[str, float],\n",
        "    reasons: str = \"\",\n",
        "    parent_matcher: Optional[SpanMatcher] = None,\n",
        "    evaluator_span_name: str = \"evaluator\",\n",
        ") -> JSONDict:\n",
        "    \"\"\"\n",
        "    Ajoute un span OTLP `evaluator` portant `eval.*` (score, mÃ©triques, raisons).\n",
        "\n",
        "    Important:\n",
        "        On n'attache PAS les `param.*` sur ce span (sauf si vous le dÃ©cidez),\n",
        "        pour Ã©viter l'optimisation boÃ®te noire.\n",
        "\n",
        "    Args:\n",
        "        otlp: payload OTLP (copiÃ©).\n",
        "        score: score global (0..1).\n",
        "        metrics: dict mÃ©triques (0..1).\n",
        "        reasons: texte explicatif.\n",
        "        parent_matcher: oÃ¹ accrocher l'evaluator (typiquement le span \"synthesizer\").\n",
        "        evaluator_span_name: nom de span.\n",
        "\n",
        "    Returns:\n",
        "        payload OTLP modifiÃ©.\n",
        "    \"\"\"\n",
        "    otlp2 = copy.deepcopy(otlp)\n",
        "    trace_id = otlp_get_trace_id(otlp2) or _new_trace_id_hex()\n",
        "\n",
        "    # choisir le parent span id\n",
        "    parent_span_id = \"\"\n",
        "    if parent_matcher is not None:\n",
        "        parent = select_one_span(otlp2, parent_matcher)\n",
        "        if parent is not None:\n",
        "            parent_span_id = str(parent.get(\"spanId\") or \"\")\n",
        "\n",
        "    # fallback: dernier span par endTimeUnixNano\n",
        "    if not parent_span_id:\n",
        "        spans = list(otlp_iter_spans(otlp2))\n",
        "        if spans:\n",
        "            spans_sorted = sorted(spans, key=lambda s: int(s.get(\"endTimeUnixNano\") or 0))\n",
        "            parent_span_id = str(spans_sorted[-1].get(\"spanId\") or \"\")\n",
        "\n",
        "    now_ns = time.time_ns()\n",
        "    span: SpanDict = {\n",
        "        \"traceId\": trace_id,\n",
        "        \"spanId\": _new_span_id_hex(),\n",
        "        \"parentSpanId\": parent_span_id,\n",
        "        \"name\": evaluator_span_name,\n",
        "        \"kind\": \"INTERNAL\",\n",
        "        \"startTimeUnixNano\": int(now_ns),\n",
        "        \"endTimeUnixNano\": int(now_ns + 500_000),\n",
        "        \"attributes\": [\n",
        "            _otlp_kv(\"eval.score\", str(float(score))),\n",
        "            _otlp_kv(\"eval.reasons\", reasons or \"\"),\n",
        "        ],\n",
        "    }\n",
        "    for k, v in metrics.items():\n",
        "        span[\"attributes\"].append(_otlp_kv(f\"eval.{k}\", str(float(v))))\n",
        "    # Optionnel: input/output.value pour donner de la \"matiÃ¨re\" au graphe\n",
        "    span[\"attributes\"].append(_otlp_kv(\"input.value\", \"TruLens feedback\"))\n",
        "    span[\"attributes\"].append(_otlp_kv(\"output.value\", reasons or \"\"))\n",
        "\n",
        "    otlp_append_span(otlp2, span)\n",
        "    return otlp2\n",
        "\n",
        "\n",
        "def coerce_to_otlp(\n",
        "    trace_or_record: Any,\n",
        "    *,\n",
        "    service_name: str = \"app\",\n",
        "    scope_name: str = \"trace_opt\",\n",
        ") -> JSONDict:\n",
        "    \"\"\"\n",
        "    Convertit une entrÃ©e \"trace-like\" en OTLP.\n",
        "\n",
        "    Supporte:\n",
        "      - payload OTLP natif (dict avec `resourceSpans`)\n",
        "      - Record TruLens JSON (dict avec `calls` / `record_id`) -> OTLP minimal\n",
        "\n",
        "    Args:\n",
        "        trace_or_record: OTLP ou Record TruLens.\n",
        "        service_name: utilisÃ© si conversion TruLens -> OTLP.\n",
        "        scope_name: utilisÃ© si conversion TruLens -> OTLP.\n",
        "\n",
        "    Returns:\n",
        "        payload OTLP.\n",
        "    \"\"\"\n",
        "    if otlp_is_payload(trace_or_record):\n",
        "        return trace_or_record  # type: ignore[return-value]\n",
        "    if trulens_is_record(trace_or_record):\n",
        "        return trulens_record_to_otlp(trace_or_record, service_name=service_name, scope_name=scope_name)  # type: ignore[arg-type]\n",
        "    raise ValueError(\"EntrÃ©e non reconnue: attendu OTLP ou Record TruLens JSON.\")\n",
        "\n",
        "\n",
        "def param_descriptions_from_specs(param_specs: Sequence[ParamSpec]) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Construit un mapping semantic_name -> description Ã  partir des ParamSpec.\n",
        "\n",
        "    Astuce:\n",
        "        `semantic_name` correspond au nom de param tel qu'il apparaÃ®t dans Trace\n",
        "        (sans prÃ©fixe runX:).\n",
        "\n",
        "    Args:\n",
        "        param_specs: specs.\n",
        "\n",
        "    Returns:\n",
        "        dict.\n",
        "    \"\"\"\n",
        "    out: Dict[str, str] = {}\n",
        "    for s in param_specs:\n",
        "        if s.description:\n",
        "            out[s.name] = s.description\n",
        "    return out\n",
        "\n",
        "\n",
        "def prepare_otlp_for_optimizer(\n",
        "    trace_or_record: Any,\n",
        "    *,\n",
        "    param_specs: Sequence[ParamSpec],\n",
        "    score: float,\n",
        "    metrics: Mapping[str, float],\n",
        "    reasons: str = \"\",\n",
        "    default_param_attach_to: Optional[SpanMatcher] = None,\n",
        "    evaluator_parent_matcher: Optional[SpanMatcher] = None,\n",
        "    service_name: str = \"app\",\n",
        "    scope_name: str = \"trace_opt\",\n",
        ") -> JSONDict:\n",
        "    \"\"\"\n",
        "    Pipeline \"one-shot\" : (trace|record) -> OTLP -> inject params -> add evaluator.\n",
        "\n",
        "    Args:\n",
        "        trace_or_record: OTLP ou Record TruLens.\n",
        "        param_specs: paramÃ¨tres trainables Ã  exposer.\n",
        "        score: score global.\n",
        "        metrics: dict mÃ©triques.\n",
        "        reasons: texte explicatif.\n",
        "        default_param_attach_to: fallback pour ParamSpec.attach_to.\n",
        "        evaluator_parent_matcher: span parent pour l'evaluator.\n",
        "        service_name: service.name si conversion TruLens -> OTLP.\n",
        "        scope_name: scope.name si conversion TruLens -> OTLP.\n",
        "\n",
        "    Returns:\n",
        "        payload OTLP prÃªt Ã  Ãªtre ingÃ©rÃ© dans Trace.\n",
        "    \"\"\"\n",
        "    otlp0 = coerce_to_otlp(trace_or_record, service_name=service_name, scope_name=scope_name)\n",
        "    otlp1 = inject_params_into_otlp(otlp0, param_specs, default_attach_to=default_param_attach_to)\n",
        "    otlp2 = add_evaluator_span(\n",
        "        otlp1,\n",
        "        score=score,\n",
        "        metrics=metrics,\n",
        "        reasons=reasons,\n",
        "        parent_matcher=evaluator_parent_matcher,\n",
        "    )\n",
        "    return otlp2\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Extraction mÃ©triques TruLens (depuis DataFrame row OU JSON)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def extract_metrics_from_mapping(\n",
        "    obj: Mapping[str, Any],\n",
        "    *,\n",
        "    metric_keys: Sequence[str],\n",
        "    default_metric: float = 0.5,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Extrait des mÃ©triques depuis un mapping (dict-like) via des clÃ©s.\n",
        "\n",
        "    Args:\n",
        "        obj: mapping (ex: row.to_dict()).\n",
        "        metric_keys: noms de colonnes / champs.\n",
        "        default_metric: fallback si manquant.\n",
        "\n",
        "    Returns:\n",
        "        dict mÃ©trique -> float.\n",
        "    \"\"\"\n",
        "    out: Dict[str, float] = {}\n",
        "    for k in metric_keys:\n",
        "        val = obj.get(k, default_metric)\n",
        "        try:\n",
        "            out[k] = float(val)\n",
        "        except Exception:\n",
        "            out[k] = float(default_metric)\n",
        "    return out\n",
        "\n",
        "\n",
        "def compute_score(\n",
        "    metrics: Mapping[str, float],\n",
        "    *,\n",
        "    weights: Optional[Mapping[str, float]] = None,\n",
        "    clamp_0_1: bool = True,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calcule un score scalaire Ã  partir d'un dict de mÃ©triques.\n",
        "\n",
        "    Args:\n",
        "        metrics: dict mÃ©trique -> float.\n",
        "        weights: dict mÃ©trique -> poids (sinon moyenne uniforme).\n",
        "        clamp_0_1: clamp le rÃ©sultat entre [0, 1].\n",
        "\n",
        "    Returns:\n",
        "        float score.\n",
        "    \"\"\"\n",
        "    if not metrics:\n",
        "        return 0.5\n",
        "    if weights:\n",
        "        num = 0.0\n",
        "        den = 0.0\n",
        "        for k, v in metrics.items():\n",
        "            w = float(weights.get(k, 0.0))\n",
        "            num += w * float(v)\n",
        "            den += w\n",
        "        score = num / den if den > 0 else sum(float(v) for v in metrics.values()) / len(metrics)\n",
        "    else:\n",
        "        score = sum(float(v) for v in metrics.values()) / len(metrics)\n",
        "    if clamp_0_1:\n",
        "        score = max(0.0, min(1.0, score))\n",
        "    return score\n",
        "\n",
        "\n",
        "\n",
        "def select_latest_item(container: Any) -> Any:\n",
        "    \"\"\"\n",
        "    SÃ©lectionne \"le dernier Ã©lÃ©ment\" d'un container.\n",
        "\n",
        "    Supporte:\n",
        "      - pandas.DataFrame / pandas.Series via `.iloc[-1]`\n",
        "      - list/tuple via `[-1]`\n",
        "      - dict: renvoie tel quel (considÃ©rÃ© dÃ©jÃ  comme 1 record)\n",
        "\n",
        "    Args:\n",
        "        container: objet.\n",
        "\n",
        "    Returns:\n",
        "        dernier Ã©lÃ©ment ou l'objet lui-mÃªme (dict).\n",
        "\n",
        "    Raises:\n",
        "        ValueError si vide/incompatible.\n",
        "    \"\"\"\n",
        "    if container is None:\n",
        "        raise ValueError(\"container is None\")\n",
        "\n",
        "    if isinstance(container, dict):\n",
        "        return container\n",
        "\n",
        "    # pandas DataFrame/Series\n",
        "    if hasattr(container, \"iloc\"):\n",
        "        try:\n",
        "            if getattr(container, \"shape\", (0,))[0] == 0:\n",
        "                raise ValueError(\"container is empty\")\n",
        "            return container.iloc[-1]\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if isinstance(container, (list, tuple)):\n",
        "        if not container:\n",
        "            raise ValueError(\"container is empty\")\n",
        "        return container[-1]\n",
        "\n",
        "    raise ValueError(f\"Type non supportÃ© pour select_latest_item: {type(container)}\")\n",
        "\n",
        "\n",
        "def extract_mapping(obj: Any) -> Mapping[str, Any]:\n",
        "    \"\"\"\n",
        "    Convertit best-effort un objet en mapping (dict-like).\n",
        "\n",
        "    Supporte:\n",
        "      - dict: renvoie tel quel\n",
        "      - pandas.Series: `.to_dict()`\n",
        "      - objets avec `model_dump()` (pydantic v2) ou `dict()` (pydantic v1)\n",
        "\n",
        "    Args:\n",
        "        obj: objet.\n",
        "\n",
        "    Returns:\n",
        "        mapping.\n",
        "    \"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        return obj\n",
        "\n",
        "    if hasattr(obj, \"to_dict\"):\n",
        "        try:\n",
        "            return obj.to_dict()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if hasattr(obj, \"model_dump\"):\n",
        "        try:\n",
        "            return obj.model_dump()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if hasattr(obj, \"dict\"):\n",
        "        try:\n",
        "            return obj.dict()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # fallback\n",
        "    return {\"value\": obj}\n",
        "\n",
        "\n",
        "def extract_trulens_record_json(obj: Any) -> Optional[JSONDict]:\n",
        "    \"\"\"\n",
        "    Extrait un Record TruLens JSON depuis diffÃ©rents conteneurs.\n",
        "\n",
        "    Cas gÃ©rÃ©s:\n",
        "      - si `obj` est dÃ©jÃ  un record dict (trulens_is_record) -> renvoie obj\n",
        "      - si `obj` est une row (Series/dict) contenant un champ `record_json` ou `record`\n",
        "        (dict ou JSON str) -> parse et renvoie.\n",
        "      - sinon None\n",
        "\n",
        "    Args:\n",
        "        obj: record-like.\n",
        "\n",
        "    Returns:\n",
        "        dict record ou None.\n",
        "    \"\"\"\n",
        "    if obj is None:\n",
        "        return None\n",
        "\n",
        "    if isinstance(obj, dict) and trulens_is_record(obj):\n",
        "        return obj\n",
        "\n",
        "    m = extract_mapping(obj)\n",
        "\n",
        "    for key in (\"record_json\", \"record\", \"record_jsons\", \"record_json_str\"):\n",
        "        if key in m:\n",
        "            raw = m.get(key)\n",
        "            if isinstance(raw, dict) and trulens_is_record(raw):\n",
        "                return raw\n",
        "            if isinstance(raw, str):\n",
        "                try:\n",
        "                    parsed = json.loads(raw)\n",
        "                    if isinstance(parsed, dict) and trulens_is_record(parsed):\n",
        "                        return parsed\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    # parfois le record est stockÃ© sous une clÃ© \"calls\" + \"record_id\" etc.\n",
        "    if isinstance(m, dict) and trulens_is_record(m):\n",
        "        return dict(m)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def render_feedback_text(\n",
        "    *,\n",
        "    score: float,\n",
        "    metrics: Mapping[str, float],\n",
        "    reasons: str = \"\",\n",
        "    extra: Optional[Mapping[str, Any]] = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Rend un texte de feedback (Ã  passer Ã  OptoPrime) Ã  partir du score/mÃ©triques.\n",
        "\n",
        "    Args:\n",
        "        score: score global.\n",
        "        metrics: dict mÃ©triques.\n",
        "        reasons: texte explicatif (si dispo).\n",
        "        extra: infos additionnelles (ex: query, output, etc).\n",
        "\n",
        "    Returns:\n",
        "        str.\n",
        "    \"\"\"\n",
        "    parts = [f\"score={score:.3f}\"]\n",
        "    if metrics:\n",
        "        parts.append(\"metrics=\" + \", \".join(f\"{k}={v:.3f}\" for k, v in metrics.items()))\n",
        "    if reasons:\n",
        "        parts.append(\"reasons=\" + reasons.strip())\n",
        "    if extra:\n",
        "        for k, v in extra.items():\n",
        "            parts.append(f\"{k}={safe_json_dumps(v, max_len=600)}\")\n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Code targets / patching (optimisation de code)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class CodeTarget:\n",
        "    \"\"\"\n",
        "    Cible de patching pour l'optimisation de code.\n",
        "\n",
        "    Un CodeTarget est associÃ© Ã  un paramÃ¨tre trainable :\n",
        "        param.__code_<key>\n",
        "\n",
        "    Attributes:\n",
        "        key: identifiant stable (ex: \"planner_node\").\n",
        "        get_callable: fonction retournant l'objet callable courant Ã  patcher.\n",
        "        set_callable: optionnel, pour remplacer le symbole (module.attr = new_fn).\n",
        "        attach_to: SpanMatcher oÃ¹ accrocher le paramÃ¨tre code dans la trace.\n",
        "        trainable: bool.\n",
        "        description: aide l'optimiseur (ex: signature / rÃ´le).\n",
        "    \"\"\"\n",
        "    key: str\n",
        "    get_callable: Callable[[], Callable[..., Any]]\n",
        "    set_callable: Optional[Callable[[Callable[..., Any]], None]] = None\n",
        "    attach_to: Optional[SpanMatcher] = None\n",
        "    trainable: bool = True\n",
        "    description: str = \"\"\n",
        "\n",
        "    @property\n",
        "    def param_name(self) -> str:\n",
        "        \"\"\"Nom du paramÃ¨tre exposÃ© dans la trace.\"\"\"\n",
        "        return f\"__code_{self.key}\"\n",
        "\n",
        "    def get_source(self) -> str:\n",
        "        \"\"\"\n",
        "        Extrait le code source de la fonction cible via inspect.getsource.\n",
        "\n",
        "        Returns:\n",
        "            str code python.\n",
        "        \"\"\"\n",
        "        fn = self.get_callable()\n",
        "        try:\n",
        "            return inspect.getsource(fn)\n",
        "        except OSError:\n",
        "            # ex: fonctions dÃ©finies dans un notebook sans source dispo\n",
        "            return f\"# Source indisponible pour {getattr(fn, '__name__', self.key)}\\n\"\n",
        "\n",
        "    def infer_description(self) -> str:\n",
        "        \"\"\"\n",
        "        DÃ©duit une description courte si `description` n'est pas fourni.\n",
        "\n",
        "        Returns:\n",
        "            str.\n",
        "        \"\"\"\n",
        "        if self.description:\n",
        "            return self.description\n",
        "        fn = self.get_callable()\n",
        "        try:\n",
        "            sig = str(inspect.signature(fn))\n",
        "        except Exception:\n",
        "            sig = \"(...)\"\n",
        "        return f\"{getattr(fn, '__name__', self.key)}{sig}\"\n",
        "\n",
        "\n",
        "def hotpatch_function_in_place(target_fn: Callable[..., Any], new_fn: Callable[..., Any]) -> None:\n",
        "    \"\"\"\n",
        "    Hotpatch \"in-place\" : remplace le bytecode (`__code__`) de `target_fn` par celui de `new_fn`.\n",
        "\n",
        "    Avantage:\n",
        "        Si LangGraph a capturÃ© une *rÃ©fÃ©rence* vers `target_fn`, le patch est effectif\n",
        "        sans recompiler le graphe.\n",
        "\n",
        "    Limites:\n",
        "        Ne marche pas si la fonction utilise des closures incompatibles.\n",
        "\n",
        "    Args:\n",
        "        target_fn: fonction originale (objet) utilisÃ©e par le graphe.\n",
        "        new_fn: fonction compilÃ©e Ã  partir d'un nouveau source.\n",
        "\n",
        "    Raises:\n",
        "        TypeError si pas patchable.\n",
        "    \"\"\"\n",
        "    if not (isinstance(target_fn, types.FunctionType) and isinstance(new_fn, types.FunctionType)):\n",
        "        raise TypeError(\"hotpatch_function_in_place ne supporte que des functions Python.\")\n",
        "    target_fn.__code__ = new_fn.__code__\n",
        "    target_fn.__defaults__ = new_fn.__defaults__\n",
        "    target_fn.__kwdefaults__ = new_fn.__kwdefaults__\n",
        "    target_fn.__annotations__ = getattr(new_fn, \"__annotations__\", {})\n",
        "    target_fn.__doc__ = getattr(new_fn, \"__doc__\", None)\n",
        "\n",
        "\n",
        "def compile_function_from_source(source: str, fn_name: str, *, glb: Optional[Dict[str, Any]] = None) -> Callable[..., Any]:\n",
        "    \"\"\"\n",
        "    Compile un source python contenant une dÃ©finition `def <fn_name>(...)` et renvoie cette fonction.\n",
        "\n",
        "    Args:\n",
        "        source: code python (doit dÃ©finir fn_name).\n",
        "        fn_name: nom de la fonction Ã  extraire.\n",
        "        glb: globals Ã  utiliser (permet d'accÃ©der aux imports existants).\n",
        "\n",
        "    Returns:\n",
        "        function object.\n",
        "\n",
        "    Raises:\n",
        "        ValueError si la fonction n'existe pas aprÃ¨s exec.\n",
        "    \"\"\"\n",
        "    glb = glb or {}\n",
        "    loc: Dict[str, Any] = {}\n",
        "    compiled = compile(source, \"<optimized>\", \"exec\")\n",
        "    exec(compiled, glb, loc)\n",
        "    fn = loc.get(fn_name) or glb.get(fn_name)\n",
        "    if not callable(fn):\n",
        "        raise ValueError(f\"Le source ne dÃ©finit pas la fonction attendue: {fn_name}\")\n",
        "    return fn  # type: ignore\n",
        "\n",
        "\n",
        "def apply_code_update(\n",
        "    *,\n",
        "    update_source: str,\n",
        "    target: CodeTarget,\n",
        "    patch_mode: str = \"in_place_or_replace\",\n",
        "    global_ns: Optional[Dict[str, Any]] = None,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Applique un patch de code produit par l'optimiseur Ã  une cible.\n",
        "\n",
        "    Modes:\n",
        "      - \"in_place\": hotpatch sur l'objet callable actuel uniquement.\n",
        "      - \"replace\": remplace le symbole via target.set_callable (ou error si absent).\n",
        "      - \"in_place_or_replace\": tente in_place, sinon fallback replace.\n",
        "      - \"replace_and_in_place\": fait replace puis hotpatch (utile si le graphe a capturÃ© l'ancien objet).\n",
        "\n",
        "    Args:\n",
        "        update_source: code python complet (def ...).\n",
        "        target: CodeTarget.\n",
        "        patch_mode: stratÃ©gie.\n",
        "        global_ns: dict globals pour exec (souvent globals()).\n",
        "\n",
        "    Raises:\n",
        "        Exception si impossible.\n",
        "    \"\"\"\n",
        "    fn0 = target.get_callable()\n",
        "    fn_name = getattr(fn0, \"__name__\", None) or target.key\n",
        "    global_ns = global_ns or {}\n",
        "\n",
        "    new_fn = compile_function_from_source(update_source, fn_name, glb=global_ns)\n",
        "\n",
        "    if patch_mode == \"in_place\":\n",
        "        hotpatch_function_in_place(fn0, new_fn)\n",
        "        return\n",
        "\n",
        "    if patch_mode == \"replace\":\n",
        "        if target.set_callable is None:\n",
        "            raise ValueError(f\"target.set_callable manquant pour {target.key}\")\n",
        "        target.set_callable(new_fn)\n",
        "        return\n",
        "\n",
        "    if patch_mode == \"replace_and_in_place\":\n",
        "        if target.set_callable is None:\n",
        "            raise ValueError(f\"target.set_callable manquant pour {target.key}\")\n",
        "        target.set_callable(new_fn)\n",
        "        # tenter hotpatch sur l'ancien objet\n",
        "        try:\n",
        "            hotpatch_function_in_place(fn0, new_fn)\n",
        "        except Exception:\n",
        "            pass\n",
        "        return\n",
        "\n",
        "    # in_place_or_replace\n",
        "    try:\n",
        "        hotpatch_function_in_place(fn0, new_fn)\n",
        "        return\n",
        "    except Exception:\n",
        "        if target.set_callable is None:\n",
        "            raise\n",
        "        target.set_callable(new_fn)\n",
        "        return\n",
        "\n",
        "\n",
        "def build_code_param_specs(code_targets: Sequence[CodeTarget]) -> List[ParamSpec]:\n",
        "    \"\"\"\n",
        "    Convertit des CodeTarget en ParamSpec (pour injection OTLP + updates).\n",
        "\n",
        "    Args:\n",
        "        code_targets: cibles de code.\n",
        "\n",
        "    Returns:\n",
        "        liste ParamSpec.\n",
        "    \"\"\"\n",
        "    specs: List[ParamSpec] = []\n",
        "    for t in code_targets:\n",
        "        # closure pour get_source\n",
        "        def _make_getter(tt: CodeTarget) -> Callable[[], Any]:\n",
        "            return lambda: tt.get_source()\n",
        "\n",
        "        def _make_applier(tt: CodeTarget) -> Callable[[str], None]:\n",
        "            return lambda src: apply_code_update(update_source=src, target=tt, patch_mode=\"in_place_or_replace\", global_ns=globals())\n",
        "\n",
        "        specs.append(\n",
        "            ParamSpec(\n",
        "                name=t.param_name,\n",
        "                get_value=_make_getter(t),\n",
        "                apply_update=_make_applier(t),\n",
        "                attach_to=t.attach_to,\n",
        "                trainable=t.trainable,\n",
        "                description=t.infer_description(),\n",
        "                normalize=normalize_whitespace,\n",
        "            )\n",
        "        )\n",
        "    return specs\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Optimisation Trace/OptoPrime (Ã  partir d'OTLP)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "# Petits wrappers autour des imports Trace pour rester optionnels\n",
        "def _require_trace_imports():\n",
        "    \"\"\"\n",
        "    Importe dynamiquement les composants Trace nÃ©cessaires.\n",
        "\n",
        "    Raises:\n",
        "        ImportError si la lib Trace/opto n'est pas installÃ©e/disponible.\n",
        "    \"\"\"\n",
        "    from opto.trace.io.otel_adapter import otlp_traces_to_trace_json  # type: ignore\n",
        "    from opto.trace.io.tgj_ingest import ingest_tgj  # type: ignore\n",
        "    from opto.trace.nodes import MessageNode, ParameterNode  # type: ignore\n",
        "    from opto.optimizers import OptoPrimeV2  # type: ignore\n",
        "    from opto.optimizers.utils import OptimizerPromptSymbolSetJSON  # type: ignore\n",
        "    from opto.trainer.algorithms.basic_algorithms import batchify  # type: ignore\n",
        "\n",
        "    return otlp_traces_to_trace_json, ingest_tgj, MessageNode, ParameterNode, OptoPrimeV2, OptimizerPromptSymbolSetJSON, batchify\n",
        "\n",
        "\n",
        "def find_target(nodes: Dict[str, Any], *, prefer_name_contains: str = \"evaluator\") -> Optional[Any]:\n",
        "    \"\"\"\n",
        "    Trouve le nÅ“ud cible (MessageNode) Ã  optimiser.\n",
        "\n",
        "    Heuristique:\n",
        "      - si un MessageNode contient `prefer_name_contains` dans son nom â†’ on le prend\n",
        "      - sinon, on prend le \"dernier\" MessageNode rencontrÃ©.\n",
        "\n",
        "    Args:\n",
        "        nodes: dict name->node (rÃ©sultat ingest_tgj).\n",
        "        prefer_name_contains: substring.\n",
        "\n",
        "    Returns:\n",
        "        MessageNode ou None.\n",
        "    \"\"\"\n",
        "    _, _, MessageNode, _, _, _, _ = _require_trace_imports()\n",
        "    last = None\n",
        "    for n in nodes.values():\n",
        "        if isinstance(n, MessageNode):\n",
        "            last = n\n",
        "            if prefer_name_contains.lower() in (n.name or \"\").lower():\n",
        "                return n\n",
        "    return last\n",
        "\n",
        "\n",
        "def visualize_graph(nodes: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Visualise un graphe Trace (paramÃ¨tres + messages) sous forme texte.\n",
        "\n",
        "    Args:\n",
        "        nodes: dict name->node.\n",
        "\n",
        "    Returns:\n",
        "        str multi-ligne.\n",
        "    \"\"\"\n",
        "    _, _, MessageNode, ParameterNode, _, _, _ = _require_trace_imports()\n",
        "    params = []\n",
        "    messages = []\n",
        "    for name, node in nodes.items():\n",
        "        if isinstance(node, ParameterNode):\n",
        "            data = getattr(node, \"data\", \"\")\n",
        "            data_s = data[:80] + (\"...\" if isinstance(data, str) and len(data) > 80 else \"\")\n",
        "            params.append(f\"[PARAM] {node.name}: {data_s!r}\")\n",
        "        elif isinstance(node, MessageNode):\n",
        "            parents = getattr(node, \"parents\", []) or []\n",
        "            parent_names = [getattr(p, \"name\", \"?\") for p in parents]\n",
        "            messages.append(f\"[MSG] {node.name} â† {parent_names if parent_names else 'ROOT'}\")\n",
        "    return \"\\n\".join(params + messages)\n",
        "\n",
        "\n",
        "def check_reachability(target: Any, params: List[Any]) -> Dict[str, bool]:\n",
        "    \"\"\"\n",
        "    VÃ©rifie si chaque paramÃ¨tre est atteignable depuis `target` via les parents.\n",
        "\n",
        "    Utile pour dÃ©tecter un paramÃ¨tre accrochÃ© Ã  un span \"isolÃ©\" (non causalement reliÃ©).\n",
        "\n",
        "    Args:\n",
        "        target: MessageNode cible.\n",
        "        params: liste de ParameterNode.\n",
        "\n",
        "    Returns:\n",
        "        dict param.name -> bool.\n",
        "    \"\"\"\n",
        "    _, _, _, ParameterNode, _, _, _ = _require_trace_imports()\n",
        "    seen = set()\n",
        "    stack = [target]\n",
        "    reachable = set()\n",
        "    while stack:\n",
        "        node = stack.pop()\n",
        "        if node in seen:\n",
        "            continue\n",
        "        seen.add(node)\n",
        "        if hasattr(node, \"parents\"):\n",
        "            for p in getattr(node, \"parents\") or []:\n",
        "                if p not in seen:\n",
        "                    stack.append(p)\n",
        "        if isinstance(node, ParameterNode):\n",
        "            reachable.add(node.name)\n",
        "    return {p.name: p.name in reachable for p in params}\n",
        "\n",
        "\n",
        "def _remap_params_in_graph(node: Any, param_mapping: Dict[int, Any], visited=None) -> None:\n",
        "    \"\"\"\n",
        "    Remappe rÃ©cursivement des ParameterNode dans un graphe Trace.\n",
        "\n",
        "    Lorsqu'on rÃ©utilise un optimiseur entre itÃ©rations, on veut que les graphs\n",
        "    utilisent *les mÃªmes objets ParameterNode* (ceux de l'optimiseur), sinon\n",
        "    l'optimiseur considÃ¨re des params diffÃ©rents.\n",
        "\n",
        "    Args:\n",
        "        node: nÅ“ud courant.\n",
        "        param_mapping: dict id(old_param) -> optimizer_param.\n",
        "        visited: set d'ids dÃ©jÃ  visitÃ©s.\n",
        "    \"\"\"\n",
        "    if visited is None:\n",
        "        visited = set()\n",
        "\n",
        "    node_id = id(node)\n",
        "    if node_id in visited:\n",
        "        return\n",
        "    visited.add(node_id)\n",
        "\n",
        "    # Remap inputs\n",
        "    if hasattr(node, \"_inputs\") and isinstance(getattr(node, \"_inputs\"), dict):\n",
        "        inputs = getattr(node, \"_inputs\")\n",
        "        for key, input_node in list(inputs.items()):\n",
        "            in_id = id(input_node)\n",
        "            if in_id in param_mapping:\n",
        "                inputs[key] = param_mapping[in_id]\n",
        "            else:\n",
        "                _remap_params_in_graph(input_node, param_mapping, visited)\n",
        "\n",
        "    # Remap parents list\n",
        "    if hasattr(node, \"parents\") and isinstance(getattr(node, \"parents\"), list):\n",
        "        parents = getattr(node, \"parents\")\n",
        "        for i, parent in enumerate(list(parents)):\n",
        "            p_id = id(parent)\n",
        "            if p_id in param_mapping:\n",
        "                parents[i] = param_mapping[p_id]\n",
        "            else:\n",
        "                _remap_params_in_graph(parent, param_mapping, visited)\n",
        "\n",
        "\n",
        "def show_prompt_diff(before: str, after: str, *, context_lines: int = 2) -> str:\n",
        "    \"\"\"\n",
        "    Produit un diff textuel compact pour des prompts (ou code).\n",
        "\n",
        "    Args:\n",
        "        before: texte original.\n",
        "        after: texte modifiÃ©.\n",
        "        context_lines: lignes de contexte.\n",
        "\n",
        "    Returns:\n",
        "        diff str.\n",
        "    \"\"\"\n",
        "    import difflib\n",
        "    before_lines = normalize_whitespace(before).splitlines(True)\n",
        "    after_lines = normalize_whitespace(after).splitlines(True)\n",
        "    diff = difflib.unified_diff(before_lines, after_lines, fromfile=\"before\", tofile=\"after\", n=context_lines)\n",
        "    return \"\".join(diff)\n",
        "\n",
        "\n",
        "def compute_change_stats(before: str, after: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calcule des statistiques simples sur un changement (longueur, delta, etc).\n",
        "\n",
        "    Args:\n",
        "        before: texte original.\n",
        "        after: texte modifiÃ©.\n",
        "\n",
        "    Returns:\n",
        "        dict stats.\n",
        "    \"\"\"\n",
        "    b = before or \"\"\n",
        "    a = after or \"\"\n",
        "    return {\n",
        "        \"len_before\": len(b),\n",
        "        \"len_after\": len(a),\n",
        "        \"delta\": len(a) - len(b),\n",
        "        \"delta_pct\": ((len(a) - len(b)) / len(b) * 100.0) if len(b) else None,\n",
        "        \"lines_before\": b.count(\"\\n\") + 1 if b else 0,\n",
        "        \"lines_after\": a.count(\"\\n\") + 1 if a else 0,\n",
        "    }\n",
        "\n",
        "\n",
        "def _ensure_param_descriptions_on_optimizer(optimizer: Any, params: Sequence[Any], desc_by_name: Mapping[str, str]) -> None:\n",
        "    \"\"\"\n",
        "    Ajoute/complÃ¨te les descriptions de paramÃ¨tres cÃ´tÃ© optimiseur (si le champ existe).\n",
        "\n",
        "    Args:\n",
        "        optimizer: OptoPrimeV2.\n",
        "        params: ParameterNode (de l'optimiseur).\n",
        "        desc_by_name: mapping param_name -> description.\n",
        "    \"\"\"\n",
        "    # OptoPrime garde des params avec attributs name/data/desc (selon versions).\n",
        "    for p in getattr(optimizer, \"parameters\", []) or []:\n",
        "        full_name = getattr(p, \"name\", \"\")\n",
        "        semantic_name = full_name.split(\":\")[0].split(\"/\")[-1]\n",
        "        if semantic_name in desc_by_name:\n",
        "            if not getattr(p, \"desc\", \"\"):\n",
        "                try:\n",
        "                    p.desc = desc_by_name[semantic_name]\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RunResult:\n",
        "    \"\"\"\n",
        "    RÃ©sultat d'un run Ã  optimiser.\n",
        "\n",
        "    Attributes:\n",
        "        otlp: payload OTLP.\n",
        "        score: score global.\n",
        "        metrics: dict mÃ©triques.\n",
        "        feedback: texte de feedback (utilisÃ© par l'optimiseur).\n",
        "        meta: infos additionnelles (query, output, etc).\n",
        "    \"\"\"\n",
        "    otlp: JSONDict\n",
        "    score: float\n",
        "    metrics: Dict[str, float]\n",
        "    feedback: str\n",
        "    meta: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "\n",
        "\n",
        "def optimize_iteration(\n",
        "    runs: Sequence[RunResult],\n",
        "    *,\n",
        "    optimizer: Optional[Any],\n",
        "    llm_client: Any,\n",
        "    objective: str,\n",
        "    param_name_substrings: Sequence[str] = (\"__code_\",),\n",
        "    memory_size: int = 12,\n",
        "    verbose_graph: bool = False,\n",
        "    param_descriptions: Optional[Mapping[str, str]] = None,\n",
        "    prefer_target_name_contains: str = \"evaluator\",\n",
        ") -> Tuple[Dict[str, str], Any]:\n",
        "    \"\"\"\n",
        "    ExÃ©cute une itÃ©ration OptoPrime sur un batch de runs.\n",
        "\n",
        "    Points importants (par rapport aux dÃ©mos) :\n",
        "      - compatible multi-runs (plusieurs requÃªtes) via batchify\n",
        "      - **remap** des ParameterNode quand on rÃ©utilise un optimiseur entre itÃ©rations,\n",
        "        afin que les nouveaux graphs pointent vers les *mÃªmes objets* paramÃ¨tres\n",
        "      - filtre simple sur les paramÃ¨tres Ã  optimiser via `param_name_substrings`\n",
        "\n",
        "    Args:\n",
        "        runs: liste de runs (idÃ©alement plusieurs requÃªtes pour un signal plus robuste).\n",
        "        optimizer: OptoPrimeV2 existant (ou None au 1er tour).\n",
        "        llm_client: client LLM pour l'optimiseur (comme dans les dÃ©mos).\n",
        "        objective: instruction globale (\"maximize eval.score ...\").\n",
        "        param_name_substrings: filtre sur le champ `ParameterNode.name`.\n",
        "        memory_size: mÃ©moire de l'optimiseur.\n",
        "        verbose_graph: si True, imprime une visualisation texte des graphs.\n",
        "        param_descriptions: mapping semantic_name -> description (optionnel).\n",
        "        prefer_target_name_contains: substring pour choisir la cible (default \"evaluator\").\n",
        "\n",
        "    Returns:\n",
        "        (updates, optimizer)\n",
        "        updates: dict semantic_param_name -> new_value\n",
        "    \"\"\"\n",
        "    (\n",
        "        otlp_traces_to_trace_json,\n",
        "        ingest_tgj,\n",
        "        MessageNode,\n",
        "        ParameterNode,\n",
        "        OptoPrimeV2,\n",
        "        OptimizerPromptSymbolSetJSON,\n",
        "        batchify,\n",
        "    ) = _require_trace_imports()\n",
        "\n",
        "    # Mapping semantic_name -> optimizer ParameterNode (si optimizer dÃ©jÃ  crÃ©Ã©)\n",
        "    opt_params_by_semantic: Dict[str, Any] = {}\n",
        "    if optimizer is not None:\n",
        "        for p in getattr(optimizer, \"parameters\", []) or []:\n",
        "            full = getattr(p, \"name\", \"\") or \"\"\n",
        "            semantic = full.split(\":\")[0].split(\"/\")[-1]\n",
        "            opt_params_by_semantic[semantic] = p\n",
        "\n",
        "    all_targets: List[Any] = []\n",
        "    all_feedbacks: List[str] = []\n",
        "    iter_params_by_semantic: Dict[str, Any] = {}\n",
        "\n",
        "    for i, run in enumerate(runs):\n",
        "        tgj_docs = list(\n",
        "            otlp_traces_to_trace_json(\n",
        "                run.otlp,\n",
        "                agent_id_hint=f\"run{i}\",\n",
        "                use_temporal_hierarchy=True,\n",
        "            )\n",
        "        )\n",
        "        if not tgj_docs:\n",
        "            continue\n",
        "        nodes = ingest_tgj(tgj_docs[0])\n",
        "\n",
        "        target = find_target(nodes, prefer_name_contains=prefer_target_name_contains)\n",
        "        if target is None:\n",
        "            continue\n",
        "\n",
        "        # ParamÃ¨tres trainables filtrÃ©s\n",
        "        params_in_graph: List[Any] = []\n",
        "        for n in nodes.values():\n",
        "            if isinstance(n, ParameterNode) and getattr(n, \"trainable\", False):\n",
        "                nname = getattr(n, \"name\", \"\") or \"\"\n",
        "                if any(sub in nname for sub in param_name_substrings):\n",
        "                    params_in_graph.append(n)\n",
        "\n",
        "        # Remap vers optimizer params si possible\n",
        "        id_mapping: Dict[int, Any] = {}\n",
        "        new_params_to_add: List[Any] = []\n",
        "        for p in params_in_graph:\n",
        "            full = getattr(p, \"name\", \"\") or \"\"\n",
        "            semantic = full.split(\":\")[0].split(\"/\")[-1]\n",
        "            if semantic in opt_params_by_semantic:\n",
        "                id_mapping[id(p)] = opt_params_by_semantic[semantic]\n",
        "                iter_params_by_semantic.setdefault(semantic, opt_params_by_semantic[semantic])\n",
        "            else:\n",
        "                # nouveau paramÃ¨tre jamais vu\n",
        "                iter_params_by_semantic.setdefault(semantic, p)\n",
        "                new_params_to_add.append(p)\n",
        "\n",
        "        if id_mapping:\n",
        "            _remap_params_in_graph(target, id_mapping)\n",
        "\n",
        "        # si optimizer existe, on lui ajoute les nouveaux paramÃ¨tres\n",
        "        if optimizer is not None:\n",
        "            for p in new_params_to_add:\n",
        "                optimizer.parameters.append(p)  # type: ignore[attr-defined]\n",
        "                full = getattr(p, \"name\", \"\") or \"\"\n",
        "                semantic = full.split(\":\")[0].split(\"/\")[-1]\n",
        "                opt_params_by_semantic[semantic] = p\n",
        "\n",
        "        if verbose_graph:\n",
        "            print(\"\\n--- Graph (run\", i, \") ---\")\n",
        "            print(visualize_graph(nodes))\n",
        "\n",
        "        # Reachability diagnostic (aprÃ¨s remap)\n",
        "        # On vÃ©rifie l'atteignabilitÃ© des paramÃ¨tres *utilisÃ©s* dans ce graph.\n",
        "        params_for_reach = list(iter_params_by_semantic.values())\n",
        "        reach = check_reachability(target, params_for_reach)\n",
        "        unreachable = [pname for pname, ok in reach.items() if not ok]\n",
        "        if unreachable:\n",
        "            print(f\"âš ï¸ Params non atteignables depuis target: {unreachable[:6]}{'...' if len(unreachable)>6 else ''}\")\n",
        "\n",
        "        all_targets.append(target)\n",
        "        all_feedbacks.append(run.feedback)\n",
        "\n",
        "    if not all_targets:\n",
        "        return {}, optimizer\n",
        "\n",
        "    # CrÃ©er l'optimiseur au 1er tour\n",
        "    if optimizer is None:\n",
        "        optimizer = OptoPrimeV2(\n",
        "            iter_params_by_semantic.values(),\n",
        "            llm=llm_client,\n",
        "            memory_size=memory_size,\n",
        "            log=True,\n",
        "            optimizer_prompt_symbol_set=OptimizerPromptSymbolSetJSON(),\n",
        "            objective=objective,\n",
        "        )\n",
        "        # initialiser mapping pour la suite\n",
        "        opt_params_by_semantic = {\n",
        "            (p.name.split(\":\")[0].split(\"/\")[-1]): p for p in getattr(optimizer, \"parameters\", []) or []\n",
        "        }\n",
        "\n",
        "    # Ajouter des descriptions si fournies\n",
        "    if param_descriptions:\n",
        "        _ensure_param_descriptions_on_optimizer(optimizer, list(iter_params_by_semantic.values()), param_descriptions)\n",
        "\n",
        "    # Batchify et optimiser\n",
        "    batched_target = batchify(*all_targets).data\n",
        "    batched_feedback = batchify(*all_feedbacks).data\n",
        "\n",
        "    optimizer.zero_feedback()\n",
        "    optimizer.backward(batched_target, batched_feedback)\n",
        "    optimizer.step(verbose=False)\n",
        "\n",
        "    updates: Dict[str, str] = {}\n",
        "    for p in getattr(optimizer, \"parameters\", []) or []:\n",
        "        full_name = getattr(p, \"name\", \"\") or \"\"\n",
        "        semantic_name = full_name.split(\":\")[0].split(\"/\")[-1]\n",
        "        updates[semantic_name] = getattr(p, \"data\", \"\")\n",
        "\n",
        "    return updates, optimizer\n",
        "\n",
        "\n",
        "\n",
        "def apply_updates(\n",
        "    updates: Mapping[str, str],\n",
        "    *,\n",
        "    param_specs: Sequence[ParamSpec],\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Applique un dict d'updates (sortie OptoPrime) sur les ParamSpec.\n",
        "\n",
        "    Les ParamSpec dont `apply_update` est None sont ignorÃ©s.\n",
        "\n",
        "    Args:\n",
        "        updates: mapping semantic_name -> new_value.\n",
        "        param_specs: specs connus.\n",
        "\n",
        "    Returns:\n",
        "        dict \"appliquÃ©\" : semantic_name -> \"ok\"/\"skipped\"/\"error:...\"\n",
        "    \"\"\"\n",
        "    specs_by_name = {s.name: s for s in param_specs}\n",
        "    out: Dict[str, str] = {}\n",
        "\n",
        "    for semantic, new_val in updates.items():\n",
        "        spec = specs_by_name.get(semantic)\n",
        "        if spec is None:\n",
        "            out[semantic] = \"skipped: unknown_param\"\n",
        "            continue\n",
        "        if spec.apply_update is None:\n",
        "            out[semantic] = \"skipped: no_apply_update\"\n",
        "            continue\n",
        "        try:\n",
        "            spec.apply_update(str(new_val))\n",
        "            out[semantic] = \"ok\"\n",
        "        except Exception as e:\n",
        "            out[semantic] = f\"error: {type(e).__name__}: {e}\"\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE8g-aJyxBOp"
      },
      "outputs": [],
      "source": [
        "# --- Trace/OptoPrime optimisation (non-intrusive) ---\n",
        "# PrÃ©requis (dÃ©jÃ  faits dans le notebook) :\n",
        "#   - `graph` : LangGraph compilÃ©\n",
        "#   - `tru_recorder` : TruGraph (TruLens) qui wrap le graph\n",
        "#   - `session` : TruSession (ou adaptez ci-dessous)\n",
        "#   - `thread_config` : config LangGraph (ou adaptez)\n",
        "#\n",
        "# Et ajoutez le fichier `trace_optimize_runtime.py` Ã  cÃ´tÃ© du notebook\n",
        "# (ou mettez-le dans votre PYTHONPATH).\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# 1) Assurez-vous de pouvoir importer le runtime\n",
        "if str(Path(\".\").resolve()) not in sys.path:\n",
        "    sys.path.append(str(Path(\".\").resolve()))\n",
        "\n",
        "import trace_optimize_runtime as tor\n",
        "\n",
        "# 2) (Optionnel) attacher un exporter OTEL en mÃ©moire (si TruLens OTEL est actif)\n",
        "exporter, processor, status = tor.try_attach_inmemory_span_exporter()\n",
        "print(\"OTEL in-memory exporter status:\", status)\n",
        "\n",
        "# 3) Prompt addendums (tuning non intrusif) + wrappers\n",
        "store = tor.TextOverrideStore()\n",
        "\n",
        "# NOTE: dans L6, ces fonctions viennent souvent de `helper.py`.\n",
        "# Adaptez ces imports si besoin.\n",
        "\n",
        "plan_prompt = tor.wrap_prompt_builder_with_addendum(\n",
        "    plan_prompt, store=store, addendum_key=\"planner_addendum\"\n",
        ")\n",
        "executor_prompt = tor.wrap_prompt_builder_with_addendum(\n",
        "    executor_prompt, store=store, addendum_key=\"executor_addendum\"\n",
        ")\n",
        "\n",
        "planner_addendum = store.as_param_spec(\n",
        "    name=\"planner_addendum\",\n",
        "    attach_to=tor.SpanMatcher(name_contains=(\"planner\",)),\n",
        "    trainable=True,\n",
        "    description=\"Append-only instructions added to the planner prompt.\",\n",
        ")\n",
        "executor_addendum = store.as_param_spec(\n",
        "    name=\"executor_addendum\",\n",
        "    attach_to=tor.SpanMatcher(name_contains=(\"executor\",)),\n",
        "    trainable=True,\n",
        "    description=\"Append-only instructions added to the executor prompt.\",\n",
        ")\n",
        "\n",
        "# 4) Code targets (optimisation de code)\n",
        "# IMPORTANT: key doit Ãªtre stable; ici on utilise les noms de fonctions.\n",
        "# Si vos fonctions sont dans le notebook (pas de source inspectable), l'optimisation de code sera limitÃ©e.\n",
        "\n",
        "CODE_TARGETS = []\n",
        "try:\n",
        "    # from helper import planner_node, executor_node, synthesizer_node\n",
        "    CODE_TARGETS += [\n",
        "        tor.CodeTarget(\n",
        "            key=\"planner_node\",\n",
        "            get_callable=lambda: planner_node,\n",
        "            attach_to=tor.SpanMatcher(name_contains=(\"planner\",)),\n",
        "            description=\"LangGraph node that produces/updates the plan JSON.\",\n",
        "        ),\n",
        "        tor.CodeTarget(\n",
        "            key=\"executor_node\",\n",
        "            get_callable=lambda: executor_node,\n",
        "            attach_to=tor.SpanMatcher(name_contains=(\"executor\",)),\n",
        "            description=\"LangGraph node that executes one plan step.\",\n",
        "        ),\n",
        "        tor.CodeTarget(\n",
        "            key=\"synthesizer_node\",\n",
        "            get_callable=lambda: synthesizer_node,\n",
        "            attach_to=tor.SpanMatcher(name_contains=(\"synthesizer\",)),\n",
        "            description=\"Final synthesis / answer node.\",\n",
        "        ),\n",
        "    ]\n",
        "except Exception as e:\n",
        "    print(\"Could not import code targets from helper:\", e)\n",
        "\n",
        "code_param_specs = tor.build_code_param_specs(CODE_TARGETS)\n",
        "\n",
        "PARAM_SPECS = [planner_addendum, executor_addendum] + code_param_specs\n",
        "PARAM_DESC = tor.param_descriptions_from_specs(PARAM_SPECS)\n",
        "\n",
        "# 5) MÃ©triques TruLens (adapter si vos colonnes diffÃ¨rent)\n",
        "METRIC_KEYS = [\n",
        "    \"Groundedness\",\n",
        "    \"Answer Relevance\",\n",
        "    \"Context Relevance\",\n",
        "    \"Logical Consistency\",\n",
        "    \"Execution Efficiency\",\n",
        "    \"Plan Adherence\",\n",
        "    \"Plan Quality\",\n",
        "]\n",
        "METRIC_WEIGHTS = {k: 1.0 for k in METRIC_KEYS}\n",
        "\n",
        "# 6) Objectif OptoPrime\n",
        "OBJECTIVE = \"\"\"You are optimizing a multi-agent LangGraph workflow.\n",
        "\n",
        "Goal:\n",
        "- Maximize eval.score (0..1), which aggregates eval.<metrics>.\n",
        "\n",
        "Constraints:\n",
        "- Keep function signatures unchanged.\n",
        "- Prefer minimal diffs.\n",
        "- Do not remove safety constraints.\n",
        "- If you edit code, keep it readable and deterministic.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 7) Petit helper pour exÃ©cuter une requÃªte et collecter RunResult\n",
        "def run_query_collect(query: str):\n",
        "    # Clear exporter to isolate this run (OTEL path)\n",
        "    if exporter is not None and hasattr(exporter, \"clear\"):\n",
        "        exporter.clear()\n",
        "\n",
        "    # Run LangGraph under TruLens recorder\n",
        "    with tru_recorder as recording:\n",
        "        out = graph.invoke({\"messages\": [(\"user\", query)]}, config=thread_config)\n",
        "\n",
        "    # Get OTLP from OTEL exporter if available\n",
        "    otlp = None\n",
        "    if exporter is not None:\n",
        "        otlp = tor.flush_inmemory_exporter_to_otlp(\n",
        "            exporter, service_name=\"l6\", scope_name=\"trulens_otel\", clear=True\n",
        "        )\n",
        "\n",
        "    # Fetch latest TruLens record + feedback\n",
        "    try:\n",
        "        recs, fbs = session.get_records_and_feedback(app_ids=[tru_recorder.app_id])\n",
        "    except Exception:\n",
        "        recs, fbs = session.get_records_and_feedback()\n",
        "\n",
        "    row = tor.select_latest_item(recs)\n",
        "    row_map = tor.extract_mapping(row)\n",
        "\n",
        "    # Fallback: if no OTEL spans, build OTLP from TruLens record JSON\n",
        "    if (otlp is None) or (len(list(tor.otlp_iter_spans(otlp))) == 0):\n",
        "        record_json = tor.extract_trulens_record_json(row)\n",
        "        if record_json is None:\n",
        "            raise RuntimeError(\n",
        "                \"No OTEL spans and no record_json found. Cannot build trace.\"\n",
        "            )\n",
        "        otlp = tor.trulens_record_to_otlp(\n",
        "            record_json, service_name=\"l6\", scope_name=\"trulens_record\"\n",
        "        )\n",
        "\n",
        "    # Compute metrics + score\n",
        "    metrics = tor.extract_metrics_from_mapping(\n",
        "        row_map, metric_keys=METRIC_KEYS, default_metric=0.5\n",
        "    )\n",
        "    score = tor.compute_score(metrics, weights=METRIC_WEIGHTS)\n",
        "\n",
        "    # Best-effort reasons extraction\n",
        "    reasons = \"\"\n",
        "    for k in METRIC_KEYS:\n",
        "        for rk in (f\"{k}_reasons\", f\"{k}.reasons\", f\"{k}_reason\", f\"{k}.reason\"):\n",
        "            if rk in row_map and row_map[rk]:\n",
        "                reasons += f\"\\n[{k}] {row_map[rk]}\"\n",
        "\n",
        "    feedback = tor.render_feedback_text(\n",
        "        score=score, metrics=metrics, reasons=reasons, extra={\"query\": query}\n",
        "    )\n",
        "\n",
        "    otlp_ready = tor.prepare_otlp_for_optimizer(\n",
        "        otlp,\n",
        "        param_specs=PARAM_SPECS,\n",
        "        score=score,\n",
        "        metrics=metrics,\n",
        "        reasons=reasons,\n",
        "        evaluator_parent_matcher=tor.SpanMatcher(name_contains=(\"synthesizer\",)),\n",
        "        service_name=\"l6\",\n",
        "        scope_name=\"trace_opt\",\n",
        "    )\n",
        "\n",
        "    return tor.RunResult(\n",
        "        otlp=otlp_ready,\n",
        "        score=score,\n",
        "        metrics=metrics,\n",
        "        feedback=feedback,\n",
        "        meta={\"query\": query},\n",
        "    )\n",
        "\n",
        "\n",
        "# 8) Boucle d'optimisation\n",
        "QUERIES = [\n",
        "    \"Give me a plan and then answer: Compare France vs Germany GDP growth since 2010.\",\n",
        "    \"What are the key drivers of inflation in 2024-2025? Give citations.\",\n",
        "]\n",
        "N_ITER = 2\n",
        "optimizer = None\n",
        "\n",
        "from opto.utils.llm import LLM\n",
        "\n",
        "LLM_CLIENT = LLM()\n",
        "\n",
        "for it in range(N_ITER):\n",
        "    runs = [run_query_collect(q) for q in QUERIES]\n",
        "    print(f\"\\n=== Iteration {it} ===\")\n",
        "    print(\"Scores:\", [round(r.score, 3) for r in runs])\n",
        "\n",
        "    updates, optimizer = tor.optimize_iteration(\n",
        "        runs,\n",
        "        optimizer=optimizer,\n",
        "        llm_client=LLM_CLIENT,\n",
        "        objective=OBJECTIVE,\n",
        "        param_name_substrings=(\"__code_\", \"planner_addendum\", \"executor_addendum\"),\n",
        "        memory_size=12,\n",
        "        verbose_graph=False,\n",
        "        param_descriptions=PARAM_DESC,\n",
        "        prefer_target_name_contains=\"evaluator\",\n",
        "    )\n",
        "\n",
        "    applied = tor.apply_updates(updates, param_specs=PARAM_SPECS)\n",
        "    print(\n",
        "        \"Applied:\", {k: v for k, v in applied.items() if v != \"skipped: unknown_param\"}\n",
        "    )\n",
        "\n",
        "print(\"\\nFinal addendums:\")\n",
        "print(\"planner_addendum:\\n\", store.get(\"planner_addendum\"))\n",
        "print(\"executor_addendum:\\n\", store.get(\"executor_addendum\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
