{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "68c29b2a",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Ollama est déjà installé.\n"
               ]
            }
         ],
         "source": [
            "import subprocess\n",
            "import time\n",
            "import os\n",
            "\n",
            "def fetch_ollama():\n",
            "    !curl -fsSL https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\n",
            "    !mkdir -p /usr/local/bin\n",
            "    !tar -C /usr/local -xzf ollama-linux-amd64.tgz\n",
            "    !chmod +x /usr/local/bin/ollama\n",
            "\n",
            "def install_ollama():\n",
            "    if not os.path.isfile('/usr/local/bin/ollama'):\n",
            "        fetch_ollama()\n",
            "    process = subprocess.Popen(\n",
            "        ['/usr/local/bin/ollama', 'serve'],\n",
            "        stdout=subprocess.PIPE,\n",
            "        stderr=subprocess.PIPE,\n",
            "        env={**os.environ, 'OLLAMA_HOST': '0.0.0.0:11434'}\n",
            "    )\n",
            "\n",
            "    # Esperar a que el servidor se inicie\n",
            "    time.sleep(5)\n",
            "\n",
            "def check_ollama():\n",
            "    import requests\n",
            "    try:\n",
            "        response = requests.get(\"http://localhost:11434/ping\")\n",
            "        print(\"Ollama est déjà installé.\")\n",
            "    except requests.ConnectionError:\n",
            "        print(\"Ollama n'est pas installé. Installation en cours...\")\n",
            "        install_ollama()\n",
            "        print(\"Ollama a été installé avec succès.\")\n",
            "\n",
            "# Vérifie si le code est exécuté sur Google Colab\n",
            "if 'COLAB_GPU' in os.environ:\n",
            "    # Commandes à exécuter uniquement sur Google Colab\n",
            "    if os.path.isdir('tp-rag'):\n",
            "        %cd tp-rag\n",
            "    if os.path.isdir('.git'):\n",
            "        # Already in the git repository, just pull\n",
            "        # Pull updates; only check/install if no updates\n",
            "        !git pull | grep -q 'Already up to date.' || pip install -r requirements.txt\n",
            "    else:\n",
            "        # Clone the repository\n",
            "        !git clone https://github.com/Florian-Audouard/tp-rag\n",
            "        %cd tp-rag\n",
            "        !pip install -r requirements.txt\n",
            "    check_ollama()\n",
            "    !/usr/local/bin/ollama pull qwen3:8b\n",
            "\n",
            "else:\n",
            "    # Commandes à exécuter si ce n'est pas sur Google Colab\n",
            "    print(\"Pas sur Google Colab, ces commandes ne seront pas exécutées.\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 76,
         "id": "825b9f24",
         "metadata": {},
         "outputs": [
            {
               "ename": "ModuleNotFoundError",
               "evalue": "No module named 'langchain_ollama'",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
                  "\u001b[0;32m/tmp/ipython-input-1252214088.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDirectoryLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_text_splitters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecursiveCharacterTextSplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_ollama\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOllama\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatPromptTemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMessagesPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_history\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInMemoryChatMessageHistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_ollama'",
                  "",
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
               ]
            }
         ],
         "source": [
            "from langchain_chroma import Chroma\n",
            "from langchain_huggingface import HuggingFaceEmbeddings\n",
            "from langchain_community.document_loaders import DirectoryLoader\n",
            "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
            "from langchain_ollama import ChatOllama\n",
            "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
            "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
            "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
            "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
            "\n",
            "\n",
            "EMBESSINGS_MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
            "DATA_FOLDER = \"data/\"\n",
            "CHUNK_SIZE = 1000\n",
            "CHUNK_OVERLAP = CHUNK_SIZE // 5"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 50,
         "id": "0615ece5",
         "metadata": {},
         "outputs": [],
         "source": [
            "embeddings = HuggingFaceEmbeddings(model_name=EMBESSINGS_MODEL_NAME)\n",
            "vector_store_splits = Chroma(\n",
            "    collection_name=\"split_data_collection\",\n",
            "    embedding_function=embeddings,\n",
            "    persist_directory=\"./chroma_langchain_split_db\",  # Where to save data locally, remove if not necessary\n",
            ")\n",
            "vector_store_full = Chroma(\n",
            "    collection_name=\"full_data_collection\",\n",
            "    embedding_function=embeddings,\n",
            "    persist_directory=\"./chroma_langchain_full_db\",  # Where to save data locally, remove if not necessary\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 51,
         "id": "75f98b66",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Number of documents loaded: 63\n"
               ]
            }
         ],
         "source": [
            "loader = DirectoryLoader(DATA_FOLDER)\n",
            "documents = loader.load()\n",
            "print(f\"Number of documents loaded: {len(documents)}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 52,
         "id": "63857e50",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Number of paragraphs created: 8847\n"
               ]
            }
         ],
         "source": [
            "text_splitter = RecursiveCharacterTextSplitter(\n",
            "    chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, add_start_index=True\n",
            ")\n",
            "all_splits = text_splitter.split_documents(documents)\n",
            "print(f\"Number of paragraphs created: {len(all_splits)}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 53,
         "id": "47f295c0",
         "metadata": {},
         "outputs": [],
         "source": [
            "_ = vector_store_full.add_documents(documents=documents)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 54,
         "id": "9b61e59a",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Added batch 1: 5000 documents\n",
                  "Added batch 2: 3847 documents\n",
                  "All 8847 documents added to the vector store.\n"
               ]
            }
         ],
         "source": [
            "# Add documents in batches to avoid exceeding max batch size\n",
            "BATCH_SIZE = 5000\n",
            "for i in range(0, len(all_splits), BATCH_SIZE):\n",
            "    batch = all_splits[i : i + BATCH_SIZE]\n",
            "    vector_store_splits.add_documents(documents=batch)\n",
            "    print(f\"Added batch {i//BATCH_SIZE + 1}: {len(batch)} documents\")\n",
            "print(f\"All {len(all_splits)} documents added to the vector store.\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 56,
         "id": "2d648ad1",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "[Document(id='5753a59f-f862-4b2a-8eb6-0b2f8796f1b6', metadata={'start_index': 52738, 'source': 'data/autres_articles/2412.18609v1.pdf'}, page_content='F. Broader Impact\\n\\nWe introduce Video-Panda, an encoder-free Video Lan- guage Model for video understanding. Our model addresses key ethical and practical challenges in large-scale AI de- ployment. While many VLMs raise concerns about data bias, privacy, and computational costs, Video-Panda miti- gates these issues through two key design choices: training exclusively on publicly available datasets and eliminating the need for a pretrained encoder. This approach not only reduces ethical concerns but also significantly lowers com- putational requirements and deployment costs, making the model more accessible and environmentally sustainable.')]"
                  ]
               },
               "execution_count": 56,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "def generate_query(vector_store, query, k=3, score=False):\n",
            "    if score:\n",
            "        return vector_store.similarity_search_with_score(query, k=k)\n",
            "    return vector_store.similarity_search(query, k=k)\n",
            "\n",
            "\n",
            "print()\n",
            "generate_query(vector_store_splits, \"what is Video-Panda ?\", k=1)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "ff8725a2",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Response from llama-3.1-8b-instant: Hello, world. It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
                  "Response from groq/compound-mini: Hello! How can I help you today?\n"
               ]
            }
         ],
         "source": [
            "llm = llm = ChatOllama(\n",
            "    model=\"qwen3:8b\",\n",
            "    temperature=0,\n",
            ")\n",
            "\n",
            "res1 = llm.invoke(\"Hello, world!\").content\n",
            "\n",
            "print(\"Response from\", MODEL_NAME_SIMPLE + \":\", res1)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 58,
         "id": "6925b7c5",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "application/vnd.google.colaboratory.intrinsic+json": {
                     "type": "string"
                  },
                  "text/plain": [
                     "'Video-Panda is an encoder-free Video Language Model for video understanding.'"
                  ]
               },
               "execution_count": 58,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant that helps people find information. Use the provided DOCUMENTS to answer the question at the end. If you don't know the answer, just say you don't know, don't try to make up an answer.\"\"\"\n",
            "USER_PROMPT = \"\"\"DOCUMENTS:\n",
            "{context}\n",
            "QUESTION: {question}\n",
            "Answer:\"\"\"\n",
            "\n",
            "\n",
            "def generate_answer(\n",
            "    agent, question, get_session_history=lambda x: InMemoryChatMessageHistory()\n",
            "):\n",
            "    results = generate_query(vector_store_splits, question, k=3)\n",
            "    context = \"\"\n",
            "    for i, documents in enumerate(results):\n",
            "        context += f\"DOCUMENT {i}\" + \":\\n\"\n",
            "        context += documents.page_content + \"\\n\\n\"\n",
            "\n",
            "    prompt = ChatPromptTemplate.from_messages(\n",
            "        [\n",
            "            (\"system\", SYSTEM_PROMPT),\n",
            "            MessagesPlaceholder(variable_name=\"history\"),\n",
            "            (\"human\", USER_PROMPT),\n",
            "        ]\n",
            "    )\n",
            "    chain = prompt | agent\n",
            "    chain_with_memory = RunnableWithMessageHistory(\n",
            "        chain,\n",
            "        get_session_history,\n",
            "        input_messages_key=\"question\",\n",
            "        history_messages_key=\"history\",\n",
            "    )\n",
            "\n",
            "    response = chain_with_memory.invoke(\n",
            "        {\"context\": context, \"question\": question},\n",
            "        config={\"configurable\": {\"session_id\": \"user-1\"}},\n",
            "    )\n",
            "    return response.content\n",
            "\n",
            "\n",
            "generate_answer(llm, \"What is Video-Panda?\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 59,
         "id": "3eefa477",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Answer 1: Video-Panda is an encoder-free Video Language Model for video understanding.\n",
                  "Answer 2: We discussed Video-Panda, but then you provided some documents and asked me to answer a question based on those documents. However, there is no question in the documents you provided.\n"
               ]
            }
         ],
         "source": [
            "store = {}\n",
            "\n",
            "\n",
            "def get_session_history(session_id: str):\n",
            "    if session_id not in store:\n",
            "        store[session_id] = InMemoryChatMessageHistory()\n",
            "    return store[session_id]\n",
            "\n",
            "\n",
            "ans1 = generate_answer(llm, \"What is Video-Panda?\", get_session_history)\n",
            "ans2 = generate_answer(llm, \"Tell me what we discussed earlier?\", get_session_history)\n",
            "print(\"Answer 1:\", ans1)\n",
            "print(\"Answer 2:\", ans2)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "19e65500",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Document to summarize: data/autres_articles/2412.18609v1.pdf\n"
               ]
            },
            {
               "ename": "APIStatusError",
               "evalue": "Error code: 413 - {'error': {'message': 'Request Entity Too Large', 'type': 'invalid_request_error', 'code': 'request_too_large'}}",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)",
                  "\u001b[0;32m/tmp/ipython-input-999742149.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sumarry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Video-Panda\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                  "\u001b[0;32m/tmp/ipython-input-999742149.py\u001b[0m in \u001b[0;36mcreate_sumarry\u001b[0;34m(document, debug)\u001b[0m\n\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mllm_high_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"document\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3141\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3143\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3144\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3145\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m             cast(\n\u001b[1;32m    397\u001b[0m                 \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m                 self.generate_prompt(\n\u001b[0m\u001b[1;32m    399\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1115\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1116\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m                 results.append(\n\u001b[0;32m--> 927\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    928\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1222\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             )\n",
                  "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         }\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    459\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \"\"\"\n\u001b[0;32m--> 461\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    462\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             body=maybe_transform(\n",
                  "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     def patch(\n",
                  "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;31mAPIStatusError\u001b[0m: Error code: 413 - {'error': {'message': 'Request Entity Too Large', 'type': 'invalid_request_error', 'code': 'request_too_large'}}"
               ]
            }
         ],
         "source": [
            "SYSTEM_PROMPT_SUMMARY = \"\"\"You are a helpful AI assistant that helps people summarize documents. Use the provided DOCUMENT to create a concise summary.\"\"\"\n",
            "\n",
            "USER_PROMPT_SUMMARY = \"\"\"DOCUMENT:{document}\"\"\"\n",
            "\n",
            "\n",
            "def create_sumarry(document, debug=False):\n",
            "    document = generate_query(vector_store_full, document, k=1)[0]\n",
            "    if debug:\n",
            "        print(\"Document to summarize:\", document.metadata[\"source\"])\n",
            "    prompt = ChatPromptTemplate.from_messages(\n",
            "        [\n",
            "            (\"system\", SYSTEM_PROMPT_SUMMARY),\n",
            "            (\"human\", USER_PROMPT_SUMMARY),\n",
            "        ]\n",
            "    )\n",
            "    chain = prompt | llm\n",
            "    summary = chain.invoke({\"document\": document.page_content})\n",
            "    return summary.content\n",
            "\n",
            "\n",
            "summary = create_sumarry(\"Video-Panda\", debug=True)"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.12"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
