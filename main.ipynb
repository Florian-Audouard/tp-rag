{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c29b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/tp-rag\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Vérifie si le code est exécuté sur Google Colab\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    # Commandes à exécuter uniquement sur Google Colab\n",
    "    if os.path.isdir('tp-rag'):\n",
    "        %cd tp-rag\n",
    "    if os.path.isdir('.git'):\n",
    "        # Already in the git repository, just pull\n",
    "        # Pull updates; only check/install if no updates\n",
    "        !git pull | grep -q 'Already up to date.' || pip install -r requirements.txt\n",
    "    else:\n",
    "        # Clone the repository\n",
    "        !git clone https://github.com/Florian-Audouard/tp-rag\n",
    "        %cd tp-rag\n",
    "        !pip install -r requirements.txt\n",
    "else:\n",
    "    # Commandes à exécuter si ce n'est pas sur Google Colab\n",
    "    print(\"Pas sur Google Colab, ces commandes ne seront pas exécutées.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "825b9f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from uuid import uuid4\n",
    "\n",
    "\n",
    "EMBESSINGS_MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
    "DATA_FOLDER = \"data/\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = CHUNK_SIZE // 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0615ece5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=EMBESSINGS_MODEL_NAME)\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f98b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Number of documents loaded: 63\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(DATA_FOLDER)\n",
    "documents = loader.load()\n",
    "print(f\"Number of documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63857e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paragraphs created: 8847\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "print(f\"Number of paragraphs created: {len(all_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b61e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents in batches to avoid exceeding max batch size\n",
    "BATCH_SIZE = 5000\n",
    "for i in range(0, len(all_splits), BATCH_SIZE):\n",
    "    batch = all_splits[i : i + BATCH_SIZE]\n",
    "    vector_store.add_documents(documents=batch)\n",
    "    print(f\"Added batch {i//BATCH_SIZE + 1}: {len(batch)} documents\")\n",
    "print(f\"All {len(all_splits)} documents added to the vector store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d648ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1 document title: data/autres_articles/2412.18604v1.pdf\n",
      "4 2 0 2 c e D 4 2\n",
      "\n",
      "]\n",
      "\n",
      "V C . s c [\n",
      "\n",
      "1 v 4 0 6 8 1 . 2 1 4 2 : v i X r a\n",
      "\n",
      "0.99\n",
      "\n",
      "0.41\n",
      "\n",
      "Perceived Younger\n",
      "\n",
      "Perceived Older\n",
      "\n",
      "MoreTraditional\n",
      "\n",
      "MoreModern\n",
      "\n",
      "0.0\n",
      "\n",
      "0.96\n",
      "\n",
      "0.95\n",
      "\n",
      "0.92\n",
      "\n",
      "HealthyFood\n",
      "\n",
      "JunkFood\n",
      "\n",
      "FormalFit\n",
      "\n",
      "CasualFit\n",
      "\n",
      "0.88\n",
      "\n",
      "0.94\n",
      "\n",
      "MoreDog\n",
      "\n",
      "MoreCat\n",
      "\n",
      "MoreGray-Crowned\n",
      "\n",
      "LessGray-Crowned\n",
      "\n",
      "0.31\n",
      "\n",
      "0.50\n",
      "\n",
      "Healthy Retina\n",
      "\n",
      "Unhealthy Retina\n",
      "\n",
      "SickLeaf\n",
      "\n",
      "HealthyLeaf\n",
      "\n",
      "0.99\n",
      "\n",
      "0.18\n",
      "\n",
      "0.64\n",
      "\n",
      "0.70\n",
      "\n",
      "0.83\n",
      "\n",
      "0.12\n",
      "\n",
      "Explaining in Diffusion: Explaining a Classifier Through Hierarchical Semantics with Text-to-Image Diffusion Models\n",
      "\n",
      "Tahira Kazimi† Ritika Allada† Pinar Yanardag Virginia Tech {tahirakazimi, ritika88, pinary}@vt.edu explain-in-diffusion.github.io\n",
      "\n",
      "Result 2 document title: data/autres_articles/2412.18604v1.pdf\n",
      "this method has several\n",
      "\n",
      "Recent approaches have begun using diffusion mod- els to generate counterfactual examples. One method utilizes shortcut learning to generate counterfactual im- ages but fails to make semantically meaningful edits for certain attributes [59]. Another study explores mod- ifying the diffusion process via adaptive parametriza- tion and cone regularization to produce realistic coun- terfactual images; however, this approach depends on a robust model, which can be difficult to train [3]. [22] explored counterfactual image generation, however their approach is computationally demanding and uses DDPM [19] models trained on single domains. As a result, it does not take advantage of large-scale latent diffusion models like Stable Diffusion, which can han-\n",
      "\n",
      "3\n",
      "\n",
      "Result 3 document title: data/autres_articles/2412.18604v1.pdf\n",
      "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\n",
      "\n",
      "Sun. Deep residual learning for image recognition. (arXiv:1512.03385), 2015. arXiv:1512.03385. 1 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural infor- mation processing systems, 33:6840–6851, 2020. 3 [20] David P. Hughes and Marcel Salathe. An open ac- cess repository of images on plant health to enable the development of mobile disease diagnostics. (arXiv:1511.08060), 2016. arXiv:1511.08060 [cs]. 6 [21] Indu Ilanchezian, Valentyn Boreiko, Laura K¨uhlewein, Ziwei Huang, Murat Sec¸kin Ayhan, Matthias Hein, Lisa Koch, and Philipp Berens. Generating realistic counter- factuals for retinal fundus and oct images using diffusion models. arXiv preprint arXiv:2311.11629, 2023. 3 [22] Guillaume Jeanneret, Lo¨ıc Simon, and Fr´ed´eric Jurie. Diffusion models for counterfactual explanations. In Proceedings of the Asian Conference on Computer Vi- sion, pages 858–876, 2022. 3\n",
      "\n",
      "Result 4 document title: data/autres_articles/2412.18596v1.pdf\n",
      "Latent Diffusion Models: By performing the diffusion process in a lower-dimensional latent space instead of the original pixel space, latent diffusion models [42] achieve significant in training and inference effi- ciency. Combined with efficient sampling strategies [51], LDMs exhibit notable inference speed-ups over pixel-space Denoising Diffusion Probabilistic Model (DDPM) [16] while mostly retaining both visual quality and diversity of generated images [42].\n",
      "\n",
      "improvement\n",
      "\n",
      "Beyond LDMs: Techniques speeding up LDMs can be broadly classified into two categories. First, there are many techniques that focus on efficient sampling, i.e. searching for a more efficient path that goes from noise to data with fewer steps [5, 8, 19, 20, 26, 27, 29, 30, 37, 51, 63, 65].\n",
      "\n",
      "Result 5 document title: data/autres_articles/2412.18604v1.pdf\n",
      "Tahira Kazimi† Ritika Allada† Pinar Yanardag Virginia Tech {tahirakazimi, ritika88, pinary}@vt.edu explain-in-diffusion.github.io\n",
      "\n",
      "Figure 1. DiffEx explains the decisions of domain-specific classifiers by identifying the most influential semantics affecting their predictions. Classifier scores for each example are displayed in the top-left corner, demonstrating how classifier predictions change in response to the manipulation of different semantics (original images are shown with red borders). Our approach is capable of explaining classifiers that concentrate on individual concepts such as faces or animals (top row) as well as those that manage complex scenes involving multiple objects, such as a formal/casual fit in a fashion context (bottom row).\n",
      "\n",
      "Abstract\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tmp_wtf(query, k=3, score=False):\n",
    "    if score:\n",
    "        return vector_store.vector_store.similarity_search_with_score(query, k=k)\n",
    "    return vector_store.vector_store.similarity_search(query, k=k)\n",
    "\n",
    "\n",
    "print()\n",
    "tmp_wtf(\"How does Diffusion Models work?\", k=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
