{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 85,
         "id": "68c29b2a",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "remote: Enumerating objects: 7, done.\u001b[K\n",
                  "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
                  "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
                  "Unpacking objects: 100% (4/4), 9.85 KiB | 4.92 MiB/s, done.\n",
                  "remote: Total 4 (delta 2), reused 4 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
                  "From https://github.com/Florian-Audouard/tp-rag\n",
                  "   e689dd8..26f98c7  master     -> origin/master\n",
                  "Requirement already satisfied: langchain==1.2.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.2.3)\n",
                  "Requirement already satisfied: langchain-core==1.2.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (1.2.3)\n",
                  "Requirement already satisfied: langchain-community==0.4.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.4.1)\n",
                  "Requirement already satisfied: langchain-text-splitters==1.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (1.1.0)\n",
                  "Requirement already satisfied: langchain-huggingface==1.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (1.2.0)\n",
                  "Requirement already satisfied: langchain-chroma==1.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.1.0)\n",
                  "Requirement already satisfied: langchain-ollama in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (1.0.1)\n",
                  "Requirement already satisfied: langchain_groq in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (1.1.1)\n",
                  "Requirement already satisfied: chromadb>=1.3.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (1.4.0)\n",
                  "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (2.32.5)\n",
                  "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (1.0.5)\n",
                  "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (5.50.0)\n",
                  "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain==1.2.3->-r requirements.txt (line 1)) (2.12.3)\n",
                  "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core==1.2.3->-r requirements.txt (line 2)) (1.33)\n",
                  "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core==1.2.3->-r requirements.txt (line 2)) (0.4.59)\n",
                  "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core==1.2.3->-r requirements.txt (line 2)) (25.0)\n",
                  "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core==1.2.3->-r requirements.txt (line 2)) (6.0.3)\n",
                  "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core==1.2.3->-r requirements.txt (line 2)) (9.1.2)\n",
                  "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core==1.2.3->-r requirements.txt (line 2)) (4.15.0)\n",
                  "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core==1.2.3->-r requirements.txt (line 2)) (0.12.0)\n",
                  "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4.1->-r requirements.txt (line 4)) (1.0.0)\n",
                  "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4.1->-r requirements.txt (line 4)) (2.0.45)\n",
                  "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4.1->-r requirements.txt (line 4)) (3.13.2)\n",
                  "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4.1->-r requirements.txt (line 4)) (0.6.7)\n",
                  "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4.1->-r requirements.txt (line 4)) (2.12.0)\n",
                  "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4.1->-r requirements.txt (line 4)) (0.4.3)\n",
                  "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community==0.4.1->-r requirements.txt (line 4)) (2.0.2)\n",
                  "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface==1.2.0->-r requirements.txt (line 6)) (0.36.0)\n",
                  "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface==1.2.0->-r requirements.txt (line 6)) (0.22.1)\n",
                  "Requirement already satisfied: ollama<1.0.0,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from langchain-ollama->-r requirements.txt (line 8)) (0.6.1)\n",
                  "Requirement already satisfied: groq<1.0.0,>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from langchain_groq->-r requirements.txt (line 9)) (0.37.1)\n",
                  "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (1.4.0)\n",
                  "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (1.4.3)\n",
                  "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.3.5->-r requirements.txt (line 11)) (0.38.0)\n",
                  "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (5.4.0)\n",
                  "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (1.23.2)\n",
                  "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (1.39.1)\n",
                  "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (1.39.1)\n",
                  "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (1.39.1)\n",
                  "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (0.48.9)\n",
                  "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (4.67.1)\n",
                  "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (7.7.0)\n",
                  "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (6.5.2)\n",
                  "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (1.76.0)\n",
                  "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (5.0.0)\n",
                  "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (0.20.0)\n",
                  "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (34.1.0)\n",
                  "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (5.2.0)\n",
                  "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (3.11.5)\n",
                  "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (0.28.1)\n",
                  "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (13.9.4)\n",
                  "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.3.5->-r requirements.txt (line 11)) (4.25.1)\n",
                  "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->-r requirements.txt (line 12)) (3.4.4)\n",
                  "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->-r requirements.txt (line 12)) (3.11)\n",
                  "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->-r requirements.txt (line 12)) (2.3.0)\n",
                  "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->-r requirements.txt (line 12)) (2025.11.12)\n",
                  "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph->-r requirements.txt (line 14)) (3.0.1)\n",
                  "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph->-r requirements.txt (line 14)) (1.0.5)\n",
                  "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph->-r requirements.txt (line 14)) (0.3.0)\n",
                  "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph->-r requirements.txt (line 14)) (3.6.0)\n",
                  "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 16)) (24.1.0)\n",
                  "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 16)) (4.12.0)\n",
                  "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 16)) (1.2.0)\n",
                  "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 16)) (0.123.10)\n",
                  "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 16)) (1.0.0)\n",
                  "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 16)) (1.14.0)\n",
                  "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 16)) (0.1.2)\n",
                  "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 16)) (3.1.6)\n",
                  "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 16)) (3.0.3)\n",
                  "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 16)) (2.2.2)\n",
                  "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 16)) (11.3.0)\n",
                  "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 16)) (0.25.1)\n",
                  "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 16)) (0.0.20)\n",
                  "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 16)) (0.14.9)\n",
                  "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 16)) (0.1.7)\n",
                  "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 16)) (2.10.0)\n",
                  "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 16)) (0.50.0)\n",
                  "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 16)) (0.13.3)\n",
                  "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio->-r requirements.txt (line 16)) (2025.3.0)\n",
                  "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio->-r requirements.txt (line 16)) (15.0.1)\n",
                  "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1->-r requirements.txt (line 4)) (2.6.1)\n",
                  "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1->-r requirements.txt (line 4)) (1.4.0)\n",
                  "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1->-r requirements.txt (line 4)) (25.4.0)\n",
                  "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1->-r requirements.txt (line 4)) (1.8.0)\n",
                  "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1->-r requirements.txt (line 4)) (6.7.0)\n",
                  "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1->-r requirements.txt (line 4)) (0.4.1)\n",
                  "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1->-r requirements.txt (line 4)) (1.22.0)\n",
                  "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb>=1.3.5->-r requirements.txt (line 11)) (1.2.0)\n",
                  "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community==0.4.1->-r requirements.txt (line 4)) (3.26.2)\n",
                  "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community==0.4.1->-r requirements.txt (line 4)) (0.9.0)\n",
                  "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio->-r requirements.txt (line 16)) (0.0.4)\n",
                  "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain_groq->-r requirements.txt (line 9)) (1.9.0)\n",
                  "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain_groq->-r requirements.txt (line 9)) (1.3.1)\n",
                  "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (1.0.9)\n",
                  "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (0.16.0)\n",
                  "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface==1.2.0->-r requirements.txt (line 6)) (3.20.0)\n",
                  "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface==1.2.0->-r requirements.txt (line 6)) (1.2.0)\n",
                  "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core==1.2.3->-r requirements.txt (line 2)) (3.0.0)\n",
                  "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (2025.9.1)\n",
                  "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (0.37.0)\n",
                  "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (0.30.0)\n",
                  "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (1.17.0)\n",
                  "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (2.9.0.post0)\n",
                  "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (2.43.0)\n",
                  "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (1.9.0)\n",
                  "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (2.0.0)\n",
                  "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (0.10)\n",
                  "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph->-r requirements.txt (line 14)) (1.12.1)\n",
                  "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core==1.2.3->-r requirements.txt (line 2)) (1.0.0)\n",
                  "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core==1.2.3->-r requirements.txt (line 2)) (0.25.0)\n",
                  "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.3.5->-r requirements.txt (line 11)) (15.0.1)\n",
                  "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.3.5->-r requirements.txt (line 11)) (25.9.23)\n",
                  "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.3.5->-r requirements.txt (line 11)) (5.29.5)\n",
                  "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.3.5->-r requirements.txt (line 11)) (1.14.0)\n",
                  "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (8.7.0)\n",
                  "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (1.72.0)\n",
                  "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.1 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (1.39.1)\n",
                  "Requirement already satisfied: opentelemetry-proto==1.39.1 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (1.39.1)\n",
                  "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (0.60b1)\n",
                  "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 16)) (2025.2)\n",
                  "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 16)) (2025.3)\n",
                  "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (2.2.1)\n",
                  "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==1.2.3->-r requirements.txt (line 1)) (0.7.0)\n",
                  "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==1.2.3->-r requirements.txt (line 1)) (2.41.4)\n",
                  "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==1.2.3->-r requirements.txt (line 1)) (0.4.2)\n",
                  "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community==0.4.1->-r requirements.txt (line 4)) (1.2.1)\n",
                  "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (4.0.0)\n",
                  "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (2.19.2)\n",
                  "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community==0.4.1->-r requirements.txt (line 4)) (3.3.0)\n",
                  "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (8.3.1)\n",
                  "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (1.5.4)\n",
                  "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.3.5->-r requirements.txt (line 11)) (0.7.1)\n",
                  "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.3.5->-r requirements.txt (line 11)) (0.22.1)\n",
                  "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.3.5->-r requirements.txt (line 11)) (1.1.1)\n",
                  "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (6.2.4)\n",
                  "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (0.4.2)\n",
                  "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (4.9.1)\n",
                  "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (3.23.0)\n",
                  "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (0.1.2)\n",
                  "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community==0.4.1->-r requirements.txt (line 4)) (1.1.0)\n",
                  "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.3.5->-r requirements.txt (line 11)) (10.0)\n",
                  "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (3.3.1)\n",
                  "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.3.5->-r requirements.txt (line 11)) (1.3.0)\n",
                  "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.3.5->-r requirements.txt (line 11)) (0.6.1)\n",
                  "Ollama est d√©j√† install√©.\n",
                  "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
               ]
            }
         ],
         "source": [
            "import subprocess\n",
            "import time\n",
            "import os\n",
            "\n",
            "def fetch_ollama():\n",
            "    !curl -fsSL https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\n",
            "    !mkdir -p /usr/local/bin\n",
            "    !tar -C /usr/local -xzf ollama-linux-amd64.tgz\n",
            "    !chmod +x /usr/local/bin/ollama\n",
            "\n",
            "def install_ollama():\n",
            "    if not os.path.isfile('/usr/local/bin/ollama'):\n",
            "        fetch_ollama()\n",
            "    process = subprocess.Popen(\n",
            "        ['/usr/local/bin/ollama', 'serve'],\n",
            "        stdout=subprocess.PIPE,\n",
            "        stderr=subprocess.PIPE,\n",
            "        env={**os.environ, 'OLLAMA_HOST': '0.0.0.0:11434'}\n",
            "    )\n",
            "\n",
            "    # Esperar a que el servidor se inicie\n",
            "    time.sleep(5)\n",
            "\n",
            "def check_ollama():\n",
            "    import requests\n",
            "    try:\n",
            "        response = requests.get(\"http://localhost:11434/ping\")\n",
            "        print(\"Ollama est d√©j√† install√©.\")\n",
            "    except requests.ConnectionError:\n",
            "        print(\"Ollama n'est pas install√©. Installation en cours...\")\n",
            "        install_ollama()\n",
            "        print(\"Ollama a √©t√© install√© avec succ√®s.\")\n",
            "\n",
            "# V√©rifie si le code est ex√©cut√© sur Google Colab\n",
            "if 'COLAB_GPU' in os.environ:\n",
            "    # Commandes √† ex√©cuter uniquement sur Google Colab\n",
            "    if os.path.isdir('tp-rag'):\n",
            "        %cd tp-rag\n",
            "    if os.path.isdir('.git'):\n",
            "        # Already in the git repository, just pull\n",
            "        # Pull updates; only check/install if no updates\n",
            "        !git pull | grep -q 'Already up to date.' || pip install -r requirements.txt\n",
            "    else:\n",
            "        # Clone the repository\n",
            "        !git clone https://github.com/Florian-Audouard/tp-rag\n",
            "        %cd tp-rag\n",
            "        !pip install -r requirements.txt\n",
            "    check_ollama()\n",
            "    !/usr/local/bin/ollama pull qwen3:8b\n",
            "\n",
            "else:\n",
            "    # Commandes √† ex√©cuter si ce n'est pas sur Google Colab\n",
            "    print(\"Pas sur Google Colab, ces commandes ne seront pas ex√©cut√©es.\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 78,
         "id": "825b9f24",
         "metadata": {},
         "outputs": [],
         "source": [
            "from langchain_chroma import Chroma\n",
            "from langchain_huggingface import HuggingFaceEmbeddings\n",
            "from langchain_community.document_loaders import DirectoryLoader\n",
            "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
            "from langchain_ollama import ChatOllama\n",
            "from langchain_groq import ChatGroq\n",
            "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
            "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
            "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
            "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
            "\n",
            "\n",
            "EMBESSINGS_MODEL_NAME = \"intfloat/multilingual-e5-base\"\n",
            "DATA_FOLDER = \"data/\"\n",
            "CHUNK_SIZE = 1000\n",
            "CHUNK_OVERLAP = CHUNK_SIZE // 5"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 50,
         "id": "0615ece5",
         "metadata": {},
         "outputs": [],
         "source": [
            "embeddings = HuggingFaceEmbeddings(model_name=EMBESSINGS_MODEL_NAME)\n",
            "vector_store_splits = Chroma(\n",
            "    collection_name=\"split_data_collection\",\n",
            "    embedding_function=embeddings,\n",
            "    persist_directory=\"./chroma_langchain_split_db\",  # Where to save data locally, remove if not necessary\n",
            ")\n",
            "vector_store_full = Chroma(\n",
            "    collection_name=\"full_data_collection\",\n",
            "    embedding_function=embeddings,\n",
            "    persist_directory=\"./chroma_langchain_full_db\",  # Where to save data locally, remove if not necessary\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 51,
         "id": "75f98b66",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Warning: No languages specified, defaulting to English.\n",
                  "Number of documents loaded: 63\n"
               ]
            }
         ],
         "source": [
            "loader = DirectoryLoader(DATA_FOLDER)\n",
            "documents = loader.load()\n",
            "print(f\"Number of documents loaded: {len(documents)}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 52,
         "id": "63857e50",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Number of paragraphs created: 8847\n"
               ]
            }
         ],
         "source": [
            "text_splitter = RecursiveCharacterTextSplitter(\n",
            "    chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, add_start_index=True\n",
            ")\n",
            "all_splits = text_splitter.split_documents(documents)\n",
            "print(f\"Number of paragraphs created: {len(all_splits)}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 53,
         "id": "47f295c0",
         "metadata": {},
         "outputs": [],
         "source": [
            "_ = vector_store_full.add_documents(documents=documents)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 54,
         "id": "9b61e59a",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Added batch 1: 5000 documents\n",
                  "Added batch 2: 3847 documents\n",
                  "All 8847 documents added to the vector store.\n"
               ]
            }
         ],
         "source": [
            "# Add documents in batches to avoid exceeding max batch size\n",
            "BATCH_SIZE = 5000\n",
            "for i in range(0, len(all_splits), BATCH_SIZE):\n",
            "    batch = all_splits[i : i + BATCH_SIZE]\n",
            "    vector_store_splits.add_documents(documents=batch)\n",
            "    print(f\"Added batch {i//BATCH_SIZE + 1}: {len(batch)} documents\")\n",
            "print(f\"All {len(all_splits)} documents added to the vector store.\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2d648ad1",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "[Document(id='5753a59f-f862-4b2a-8eb6-0b2f8796f1b6', metadata={'start_index': 52738, 'source': 'data/autres_articles/2412.18609v1.pdf'}, page_content='F. Broader Impact\\n\\nWe introduce Video-Panda, an encoder-free Video Lan- guage Model for video understanding. Our model addresses key ethical and practical challenges in large-scale AI de- ployment. While many VLMs raise concerns about data bias, privacy, and computational costs, Video-Panda miti- gates these issues through two key design choices: training exclusively on publicly available datasets and eliminating the need for a pretrained encoder. This approach not only reduces ethical concerns but also significantly lowers com- putational requirements and deployment costs, making the model more accessible and environmentally sustainable.')]"
                  ]
               },
               "execution_count": 56,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "def generate_query(vector_store, query, k=3, score=False):\n",
            "    if score:\n",
            "        return vector_store.similarity_search_with_score(query, k=k)\n",
            "    return vector_store.similarity_search(query, k=k)\n",
            "\n",
            "\n",
            "generate_query(vector_store_splits, \"what is Video-Panda ?\", k=1)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "ff8725a2",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Response from grok : Hello, world. It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
                  "Response from ollama : Hello! üòä How can I assist you today? Whether you have questions, need help with something, or just want to chat, I'm here for you! What's on your mind?\n"
               ]
            }
         ],
         "source": [
            "llm = ChatGroq(api_key=API_KEY, model=\"llama-3.1-8b-instant\", temperature=0)\n",
            "\n",
            "llm_ollama = ChatOllama(\n",
            "    model=\"qwen3:8b\",\n",
            "    temperature=0,\n",
            ")\n",
            "\n",
            "res1 = llm.invoke(\"Hello, world!\").content\n",
            "res2 = llm_ollama.invoke(\"Hello, world!\").content\n",
            "\n",
            "print(\"Response from grok :\", res1)\n",
            "print(\"Response from ollama :\", res2)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "6925b7c5",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "application/vnd.google.colaboratory.intrinsic+json": {
                     "type": "string"
                  },
                  "text/plain": [
                     "'Video-Panda is an **encoder-free Video Language Model (VLM)** designed for video understanding. It addresses ethical and practical challenges in AI deployment by training exclusively on publicly available datasets and eliminating the need for a pretrained encoder. This approach reduces computational costs, improves environmental sustainability, and enhances accessibility. Video-Panda demonstrates competitive performance compared to models like VideoChat-GPT and Video-LLaVA, with faster inference speeds (processing videos in ~41ms) and stronger results in correctness, context, and temporal understanding, despite using fewer parameters (45M) and fewer video frames (8 vs. 100).'"
                  ]
               },
               "execution_count": 80,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant that helps people find information. Use the provided DOCUMENTS to answer the question at the end. If you don't know the answer, just say you don't know, don't try to make up an answer.\"\"\"\n",
            "USER_PROMPT = \"\"\"DOCUMENTS:\n",
            "{context}\n",
            "QUESTION: {question}\n",
            "Answer:\"\"\"\n",
            "\n",
            "\n",
            "def generate_answer(\n",
            "    agent, question, get_session_history=lambda x: InMemoryChatMessageHistory()\n",
            "):\n",
            "    results = generate_query(vector_store_splits, question, k=3)\n",
            "    context = \"\"\n",
            "    for i, documents in enumerate(results):\n",
            "        context += f\"DOCUMENT {i}\" + \":\\n\"\n",
            "        context += documents.page_content + \"\\n\\n\"\n",
            "\n",
            "    prompt = ChatPromptTemplate.from_messages(\n",
            "        [\n",
            "            (\"system\", SYSTEM_PROMPT),\n",
            "            MessagesPlaceholder(variable_name=\"history\"),\n",
            "            (\"human\", USER_PROMPT),\n",
            "        ]\n",
            "    )\n",
            "    chain = prompt | agent\n",
            "    chain_with_memory = RunnableWithMessageHistory(\n",
            "        chain,\n",
            "        get_session_history,\n",
            "        input_messages_key=\"question\",\n",
            "        history_messages_key=\"history\",\n",
            "    )\n",
            "\n",
            "    response = chain_with_memory.invoke(\n",
            "        {\"context\": context, \"question\": question},\n",
            "        config={\"configurable\": {\"session_id\": \"user-1\"}},\n",
            "    )\n",
            "    return response.content\n",
            "\n",
            "\n",
            "generate_answer(llm, \"What is Video-Panda?\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 81,
         "id": "3eefa477",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Answer 1: Video-Panda is an **encoder-free Video Language Model (VLM)** designed for video understanding. It addresses ethical and practical challenges in AI deployment by training exclusively on publicly available datasets and eliminating the need for a pretrained encoder. This design reduces computational costs, improves accessibility, and enhances environmental sustainability. Video-Panda demonstrates competitive performance compared to models like VideoChat-GPT and Video-LLaVA, with faster inference speeds (processing videos in ~41ms) and efficient parameter usage (45M parameters for its visual component). It excels in correctness, context, and temporal understanding while using fewer video frames (8 vs. 100) than competing models.\n",
                  "Answer 2: Based on the provided documents, there is no information available about previous discussions or conversation history. The documents focus on technical content related to AI models, dialogue systems, and academic references, but none contain records of prior chat interactions.\n"
               ]
            }
         ],
         "source": [
            "store = {}\n",
            "\n",
            "\n",
            "def get_session_history(session_id: str):\n",
            "    if session_id not in store:\n",
            "        store[session_id] = InMemoryChatMessageHistory()\n",
            "    return store[session_id]\n",
            "\n",
            "\n",
            "ans1 = generate_answer(llm, \"What is Video-Panda?\", get_session_history)\n",
            "ans2 = generate_answer(llm, \"Tell me what we discussed earlier?\", get_session_history)\n",
            "print(\"Answer 1:\", ans1)\n",
            "print(\"Answer 2:\", ans2)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 93,
         "id": "19e65500",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Document to summarize: data/autres_articles/2412.18609v1.pdf\n",
                  "Summary: The document presents **Video-Panda**, an encoder-free video language model (VLM) designed for video understanding, with a focus on balancing performance, efficiency, and ethical considerations. Below is a structured summary of its key components and findings:\n",
                  "\n",
                  "---\n",
                  "\n",
                  "### **1. Model Architecture & Training Strategy**\n",
                  "- **Encoder-Free Design**: Video-Panda eliminates the need for a pre-trained encoder, reducing computational costs and ethical concerns (e.g., data bias, privacy).\n",
                  "- **Staged Training**: \n",
                  "  - **Stage 1**: Uses 702K video-text pairs for initial alignment, with gradual complexity scaling to avoid overfitting.\n",
                  "  - **Stage 2**: Uses half the dataset (351K samples) to refine representations.\n",
                  "  - **Stage 3**: Freezes parameters for fine-tuning, ensuring robustness.\n",
                  "- **Learnable Selective Downsampling (LSD)**: \n",
                  "  - Applied after **Local Spatial-Temporal Encoding (LSTE)** to preserve temporal context.\n",
                  "  - Outperforms alternatives like **average pooling** (6.7% lower) and **Perceiver Resampler** (21.3% lower on MSVD-QA) due to its adaptive compression.\n",
                  "\n",
                  "---\n",
                  "\n",
                  "### **2. Ablation Studies**\n",
                  "- **Data Scale Impact**: \n",
                  "  - Full dataset (702K) slightly underperforms half (351K) in Stage 1, suggesting gradual complexity scaling is critical.\n",
                  "- **Downsampling Position**: \n",
                  "  - LSD after LSTE improves performance across datasets (except ActivityNet-QA), justifying its placement.\n",
                  "- **Teacher Encoders**: \n",
                  "  - **LanguageBind** (Video-Panda‚Äôs teacher) outperforms **CLIP**, **InternVideo**, and **DINOv2** by 2.2‚Äì4% on key benchmarks.\n",
                  "  - Image-based teachers (CLIP/DINOv2) focus on frame-specific predictions, neglecting global context.\n",
                  "\n",
                  "---\n",
                  "\n",
                  "### **3. Datasets & Evaluation**\n",
                  "- **Pre-training Dataset**: **Valley-Pretrain-702K** (702K video-text pairs) is filtered for balance between conceptual diversity and efficiency.\n",
                  "- **Fine-tuning Dataset**: **Video-ChatGPT-100K** (100K video instructions) combines human expertise with semi-automated annotation.\n",
                  "- **Evaluation Framework**: \n",
                  "  - Uses **ActivityNet-200** with **GPT-3.5** to score models on five criteria:\n",
                  "    1. **Correctness**: Alignment with video content.\n",
                  "    2. **Detail Orientation**: Coverage and specificity of responses.\n",
                  "    3. **Contextual Understanding**: Interpretation of broader video context.\n",
                  "    4. **Temporal Understanding**: Chronological event tracking.\n",
                  "    5. **Consistency**: Coherent responses across questions/segments.\n",
                  "\n",
                  "---\n",
                  "\n",
                  "### **4. Qualitative Analysis**\n",
                  "- **Overfitting Risk**: Full dataset in Stage 1 leads to imbalanced training (e.g., dog vs. cat interactions).\n",
                  "- **LSD Placement**: Placing LSD before LSTE harms cliff recognition due to early downsampling.\n",
                  "- **Alternative Methods**: Average pooling and Perceiver Resampler struggle with content recognition (e.g., cucumbers, pandas).\n",
                  "- **Teacher Bias**: Image-based teachers (CLIP/DINOv2) focus on frame-specific predictions, missing global context (e.g., shredded potatoes across frames).\n",
                  "\n",
                  "---\n",
                  "\n",
                  "### **5. EVE Baseline Comparison**\n",
                  "- **EVE* (Retrained on Video Data)**: Treats each frame as an image, using CLIP-ViT-L/14 for distillation. \n",
                  "  - **Limitation**: Ignores temporal relationships.\n",
                  "  - **Video-Panda‚Äôs Advantage**: Uses LSD to preserve temporal context while reducing frames to consistent tokens.\n",
                  "\n",
                  "---\n",
                  "\n",
                  "### **6. Broader Impact**\n",
                  "- **Ethical & Practical Benefits**:\n",
                  "  - **Public Datasets Only**: Mitigates data bias and privacy risks.\n",
                  "  - **No Pretrained Encoder**: Lowers computational costs and environmental impact.\n",
                  "  - **Accessibility**: Makes the model more deployable for real-world applications.\n",
                  "\n",
                  "---\n",
                  "\n",
                  "### **Key Takeaways**\n",
                  "- **Staged Training + LSD** enables efficient, context-aware video understanding.\n",
                  "- **Encoder-Free Design** addresses ethical and practical challenges in large-scale AI.\n",
                  "- **LanguageBind as Teacher** outperforms image-based encoders in capturing global context.\n",
                  "- **Temporal Preservation** via LSD is critical for accurate video QA and reasoning.\n",
                  "\n",
                  "This work advances video understanding by combining efficient architecture, ethical design, and rigorous evaluation, setting a new benchmark for encoder-free VLMs.\n"
               ]
            }
         ],
         "source": [
            "SYSTEM_PROMPT_SUMMARY = \"\"\"You are a helpful AI assistant that helps people summarize documents. Use the provided DOCUMENT to create a concise summary.\"\"\"\n",
            "\n",
            "USER_PROMPT_SUMMARY = \"\"\"DOCUMENT:{document}\"\"\"\n",
            "\n",
            "\n",
            "def create_sumarry(document, debug=False):\n",
            "    document = generate_query(vector_store_full, document, k=1)[0]\n",
            "    if debug:\n",
            "        print(\"Document to summarize:\", document.metadata[\"source\"])\n",
            "    prompt = ChatPromptTemplate.from_messages(\n",
            "        [\n",
            "            (\"system\", SYSTEM_PROMPT_SUMMARY),\n",
            "            (\"human\", USER_PROMPT_SUMMARY),\n",
            "        ]\n",
            "    )\n",
            "    chain = prompt | llm_ollama\n",
            "    summary = chain.invoke({\"document\": document.page_content})\n",
            "    return summary.content\n",
            "\n",
            "\n",
            "summary = create_sumarry(\"Video-Panda\", debug=True)\n",
            "print(\"Summary:\", summary)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "5339545c",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Closing server running on port: 7861\n",
                  "Closed existing Gradio server\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
                  "  self.chatbot = Chatbot(\n",
                  "/usr/local/lib/python3.12/dist-packages/gradio/utils.py:1052: UserWarning: Expected 2 arguments for function <function <lambda> at 0x7959d6d68e00>, received 3.\n",
                  "  warnings.warn(\n",
                  "/usr/local/lib/python3.12/dist-packages/gradio/utils.py:1060: UserWarning: Expected maximum 2 arguments for function <function <lambda> at 0x7959d6d68e00>, received 3.\n",
                  "  warnings.warn(\n",
                  "/usr/local/lib/python3.12/dist-packages/gradio/utils.py:1052: UserWarning: Expected 2 arguments for function <function <lambda> at 0x7959d6d68f40>, received 3.\n",
                  "  warnings.warn(\n",
                  "/usr/local/lib/python3.12/dist-packages/gradio/utils.py:1060: UserWarning: Expected maximum 2 arguments for function <function <lambda> at 0x7959d6d68f40>, received 3.\n",
                  "  warnings.warn(\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
                  "* Running on public URL: https://ddeacaaf5c66f156da.gradio.live\n",
                  "\n",
                  "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
               ]
            },
            {
               "data": {
                  "text/html": [
                     "<div><iframe src=\"https://ddeacaaf5c66f156da.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
                  ],
                  "text/plain": [
                     "<IPython.core.display.HTML object>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "text/plain": []
               },
               "execution_count": 94,
               "metadata": {},
               "output_type": "execute_result"
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Traceback (most recent call last):\n",
                  "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 759, in process_events\n",
                  "    response = await route_utils.call_process_api(\n",
                  "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                  "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
                  "    output = await app.get_blocks().process_api(\n",
                  "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                  "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2191, in process_api\n",
                  "    result = await self.call_function(\n",
                  "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                  "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1696, in call_function\n",
                  "    prediction = await fn(*processed_input)\n",
                  "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                  "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 882, in async_wrapper\n",
                  "    response = await f(*args, **kwargs)\n",
                  "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
                  "  File \"/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py\", line 553, in __wrapper\n",
                  "    return await submit_fn(*args, **kwargs)\n",
                  "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                  "  File \"/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py\", line 943, in _submit_fn\n",
                  "    response = await anyio.to_thread.run_sync(\n",
                  "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                  "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 61, in run_sync\n",
                  "    return await get_async_backend().run_sync_in_worker_thread(\n",
                  "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                  "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2525, in run_sync_in_worker_thread\n",
                  "    return await future\n",
                  "           ^^^^^^^^^^^^\n",
                  "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 986, in run\n",
                  "    result = context.run(func, *args)\n",
                  "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
                  "TypeError: <lambda>() takes 2 positional arguments but 3 were given\n"
               ]
            }
         ],
         "source": [
            "import uuid\n",
            "import gradio as gr\n",
            "\n",
            "\n",
            "def summarize_doc(topic):\n",
            "    return create_sumarry(topic, debug=False)\n",
            "\n",
            "\n",
            "def chat_answer(message, history):\n",
            "    # Just return the answer for the chat interface\n",
            "    answer = generate_answer(llm, message, get_session_history)\n",
            "    return answer\n",
            "\n",
            "\n",
            "# Close existing demo if it exists\n",
            "try:\n",
            "    if \"demo\" in globals() and hasattr(demo, \"close\"):\n",
            "        demo.close()\n",
            "        time.sleep(1)  # Give it a moment to close\n",
            "        print(\"Closed existing Gradio server\")\n",
            "except Exception:\n",
            "    pass\n",
            "\n",
            "with gr.Blocks() as demo:\n",
            "    gr.Markdown(\n",
            "        \"## RAG assistant\\nInteract with the vector store for Q&A or quick summaries.\"\n",
            "    )\n",
            "    with gr.Tab(\"Q&A chat\"):\n",
            "        gr.ChatInterface(\n",
            "            fn=chat_answer,\n",
            "            title=\"Ask the indexed documents\",\n",
            "            description=\"Conversational Q&A using your vector store.\",\n",
            "        )\n",
            "    with gr.Tab(\"Summarize\"):\n",
            "        topic_box = gr.Textbox(label=\"Document/topic\", placeholder=\"e.g., Video-Panda\")\n",
            "        summary_box = gr.Textbox(label=\"Summary\", lines=6)\n",
            "        sum_btn = gr.Button(\"Summarize\")\n",
            "        sum_btn.click(fn=summarize_doc, inputs=topic_box, outputs=summary_box)\n",
            "\n",
            "demo.launch(server_name=\"0.0.0.0\", share=True)"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.12"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
